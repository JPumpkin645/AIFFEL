{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 프로젝트는 지금까지의 실습과 동일한 방법으로 CIFAR-10 데이터셋에 대해 진행해 보겠습니다. 여러분들이 만들어야 할 모델은 CIFAR-10의 10가지 클래스 중 개구리 라벨을 이상 데이터로 처리하는 모델입니다. 혹시 개구리가 출현할 경우 이를 감지하여 이상감지 경고를 발생시키는 개구리 감지 모델이라고 할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "- Skip-GANomaly 모델의 구현\n",
    "- 모델의 학습과 검증\n",
    "- 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_backup = train_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pixel: 255\n",
      "min pixel: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"max pixel:\", train_data.max())\n",
    "print(\"min pixel:\", train_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_data):\n",
    "\n",
    "    normalized_train_data = (train_data - 127.5) / 127.5\n",
    "    \n",
    "    return normalized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pixel: 1.0\n",
      "min pixel: -1.0\n"
     ]
    }
   ],
   "source": [
    "train_data = normalization(train_data)\n",
    "\n",
    "print(\"max pixel:\", train_data.max())\n",
    "print(\"min pixel:\", train_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5BcV5nYf1+/puf90Nsj2ZJl2WvD2jII4wDZGIh3ZVelDKmF2KSApSBaqnBqqdo/oEjVQpJ/vNmFDVvx4hKgxSQEL7XYwaEUvMS1rMOah2wwtiW/ZFlYI8l6zYxmerpn+vXlj24tPTPnu9Nj9fT0lb9f1a2ZPt89556+984355zvcURVcRzHiROJ1e6A4zjOcnHF5ThO7HDF5ThO7HDF5ThO7HDF5ThO7HDF5ThO7HDF5TjOiiEi+0TktIg8a8hFRP5SRA6LyNMi8pZm2nXF5TjOSvINYHeE/DZgR/3YA3ylmUZdcTmOs2Ko6mPAeMQpdwDf1Bo/BYZEZNNS7aZa1cFmSKfT2pXNBmWVStmsJ0Z5yhIAmZStk9MRslTSlomELygSof+NOgDlcsWURcUzJJNJ+3JGJERVq/a1qvbVJBFxkyOoVsPfLarvUd+6GhHhIeYbYssSEc85mYi4vxHPM6qPGtFHs05ke2HGJ6fJ5Wdf30Or83vv7tVz4/a72ciTT88dBGYbivaq6t5lXG4UONbweaxedjKq0kUpLhHZDXwZSAJfU9V7os7vyma54cbwFHZy0lbKXYnwH91Ixn6wV6zpMWXrRnpN2drhPlOWSaaD5amubrMOSfsWj09MmrJi2f5uw0ODpixRKQXL5+bmzDqzs7OmLNsd/kcDUMF+ufOFXLB8cGjArIPa7RXniqYsSfi5gK0o+/vs59zba8vSaft+FCL6qFH/3BLhdyTqO5c1rJv+9Ovfta/TJOfGK/z8kcubOje56aVZVd11EZcLfZEl4xBft+ISkSRwL3ArNS15QEQeVtVDr7dNx3FWHwWq2CP0FjMGbGn4vBk4sVSli1njugk4rKpHVLUIPEBtvuo4ToxRlJJWmjpawMPAR+rWxZuB86oaOU2Ei5sqhuamb194kojsoWYtINPVdRGXcxynXbRqxCUi3wZuAdaKyBjweajN7VX1PmA/cDtwGMgDH2um3YtRXE3NTesLdXsB+vr7PYeO43Q4ilJpUborVb1rCbkCn1puuxejuF7X3NRxnM6nuvT6+KpyMYrrALBDRLYBx4E7gQ9FVZgtFDh46GBQdv7cObPesDHDlDX21HNtpd+Ure9eb8pmqrZ1M1cJP0yVjFknP2tbhvIF29JXqthD9bNJ29qdTYX7WC7b7SUNqxZAV8T0Pj87Y8rK1fD3lsIas06EFwKlCKtod8q29OUMy9x4hPtNT49tdZaEbcEUw+oMQIT7Rb4QtgSXS+FygGQq/FxKswW7D02iQOVSVVyqWhaRu4FHqLlD7FPVsFZyHCdWXMojLlR1P7XFNcdxLhEUKHV4Sve2es47jtP5KHrpThUdx7lEUTCWczsGV1yO48yj5jnf2bjichxnAULldQSFt5O2Ki4RodtK6WB7FHCF4fawdYMdbLxh3Ygp644yd0dE/xfmwsHIsyXbVK8R7WW6I4KzI4KstWpfb3AkHFxeLtntZdJ2PyoRUR3JjO0qMVcM36tS2b4fPRHtpfrsPmYj6pWnwy4biYhsGeWIP9oITxT6+uzA/lwub8pK5bDbQ1Rijump88HyatQDa5La4rwrLsdxYkTNj8sVl+M4MaPqIy7HceKEj7gcx4kdilDp8Kzurrgcx1mETxUbSKBkJRzc2t9vR9hePTocLF/bbddJV+10xLlxO/C5UrX/0xTy4b4nIiyiA0N2GuBUhDVs8vy0XS/iqY0MhC1b0+ftgOhiRLB0YdYO9I3Ko97XG7bclop2EHCiYn+xdNq+VxUjXTVAyjADzs3ZdTJp+4EmqnZw9tz0hCmL8ujsMl7jctW2fE7OhC3LUXWaRRGKGrU3wOrjIy7HceZRc0D1qaLjODHDF+cdx4kVqkJFfcTlOE7MqPqIy3GcOFFbnO9s1dDZvXMcp+344vwCkglhuCt8ye6I3OZDveEA23UDdo7virEFPBCx/zIkUxFm4ET4Yc5VI8zxEb4LqYhA38qc7TagEfnLT58K745dKdnfejpvBwDnK7brSF93xK7Uc+HrJSMSpiTEdhlIdkXsIJ2zXV96MuE+piIyfM5G7BNQKNnuEFHpjicj+jiRD78/OcP9BmC2FH4HShF7CyyHivtxOY4TJ9xz3nGcWFJ1q6LjOHGiFmTtistxnBihCCUP+XEcJ06o4g6ojuPEDXEH1EbSSWHdUNis3Z+2h6bZbFiWSNrm5+6IfO6lsu0aEPXAVMNm8mJEfvhK0XaVqGpE5oUINwRN2dkLpovhTA+Vin1/8xXbhF6KkE3P2P0fGw/3I52w2xvM2fe+9NpZU5Y/b7tzXLH2qmD5+g2bzTqSCedzB5gbP2fKcjk7y8bktO0OcfZ82PXl6LEps04lGX6ec8XW5Jy/pEdcInIUmKbmGlVW1V2t6JTjOKvLG2Fx/t2qav87dBwnVijiiQQdx4kXte3JOls1XOx4UIG/E5EnRWRP6AQR2SMiT4jIE8UWhSM4jrOS1DaEbeZYLS5Wrb5TVU+IyHrghyLyvKo+1niCqu4F9gIM9mTsVWzHcToCpfM95y+qd6p6ov7zNPAQcFMrOuU4zupyyY64RKQXSKjqdP333wX+U1SddCrB6LrwJgoDGTsSvq8nbP6XCHcCIiL1JSIrw1zBNq0njAe1pn/QrNPba2c1mDpv2zQGB+zMC9MRG1gcPR5uMzdnu0NkImbwm3sisluk7QwWr5wLZ6mYi/DITkdkhxgc6Ddl77zONmZPnQy7B+hMxLXW2VlH5ozsJgC5nD0O6ErbbV6+Mfzd1q/fYNY5NRV2rxh/6TWzTrOoSktHXCKyG/gykAS+pqr3LJAPAv8DuJyaTvpzVf3rqDYvZqq4AXhIRC608z9V9QcX0Z7jOB1AbXG+NSE/IpIE7gVuBcaAAyLysKoeajjtU8AhVf1XIrIOeEFEvqWW4yQXobhU9Qhww+ut7zhOp9LSnPM3AYfr+gIReQC4A2hUXAr0S20U1AeMA/YUDHeHcBxnAbXF+abXr9aKyBMNn/fWDXIXGAWONXweA96+oI3/BjwMnAD6gX+jGrGegysux3ECLMNz/uwSETMhDbhwgfH3gKeA9wDbqXko/D9VNWOeOtvm6ThO27ngOd/M0QRjwJaGz5upjawa+RjwoNY4DLwC/FZUo664HMdZRJVEU0cTHAB2iMg2EckAd1KbFjbyKvBeABHZAFwDHIlqtK1TxVQywUh/OGtDqhg2nwN0pcPd7Mn2mHXmjA0IAEpVe91vaGjYlKmxwUKxYj/AUiliI4e+PlN24sycKTv8azt7wZnp8HeL2HeBK7ptC9L7fmenKdu8ye7/3z4Rfu8eP2yb68tVOyNGKmG7L0xPnjFl+Vz4Pvb32+4JVOyRRDZr18sYWUwAesWuV66EH87lWy4z6wyMTwfLnzl68WHDqlCqtmZMo6plEbkbeISaO8Q+VT0oIp+sy+8D/jPwDRF5htrU8jNLxT/7GpfjOPOoTRVbNxlT1f3A/gVl9zX8foKaH2jTuOJyHGcRq+kV3wyuuBzHmccy3SFWBVdcjuMsoLVTxZXAFZfjOIvwnPONF0ulWL9mTVBWOGdb3xIS7mYuIud5oWib0VISkX89Yqt6639QoWRbw4ZG7GDpqFz1L48tdHX5DeNTdh+tfPTJpP0fdCBrt7c+FbZeAWTHbcvnjoGNwfKTI3Y/Xps8bcrm8vY9/sWLL5qyhJEDrtRrPxeG7OBmkvafzOCgbeXur9rPetbYl0CLds75rUaygq70xY+UalZF357McZwY4ambHceJJT5VdBwnVrhV0XGcWOJWRcdxYoWqUHbF5ThO3PCpYuPFUimG164Lyob7wsHXAIlEOEB1cmrCrFPK5ez2Krb5v4qdv0yNYO++fjuvfAlbduiIbcafmbO3c89mu2xZJtzH7l7bVD+StF1HnnzplCkrF+3XZ24w7A6xbti+H4LtolAq2+4yM0U79/1M3giML9vfWYq260XUmnU6YQs1EZFrPxW+j+U5291EK+HvZSUCWA6+xuU4TixxxeU4TqxwPy7HcWKJ+3E5jhMrVKHcokSCK4UrLsdxFuFTRcdxYoWvcS1CwHBtkIgtyi26IvJ/9xCOngdIRST5TyQi8scbrhJd3YNmnbMn7ewK+bO2O8f2EdttYNb2DDDdHq7ZPmrWSczZDZaT9j2einBHSSXDefH7M/ZzWTO83ZRtv/oKU/bKqz83Zc+/cDxYnklFuBqo7UpTLtt/MgkjMwdAusu+j9VK+L2KWmcSCb+n0qK1Ke1wxbXkRFZE9onIaRF5tqFsRER+KCIv1X/aO0w4jhM7qkhTx2rRzArcN4DdC8o+CzyqqjuAR+ufHce5BFCllfsqrghLKi5VfQwYX1B8B3B//ff7gfe1uF+O46waQqWaaOpYLV7vGtcGVT0JoKonRWS9daKI7AH2AIwM2OsbjuN0DrFf47pYVHWvqu5S1V39PXaMneM4ncGFWMVYTxUNTonIJoD6TztZuOM48UJr61zNHKvF650qPgx8FLin/vN7zVSqqlKYDW8MICU7wh/CkfwzM/ZmAsWirZPLCdvVIJe33RemDNno5fZt1LLd3ta19n+s7ZfZ5vP8rF1v9JobguUZtV0eJs7bm450D4U3NwHgnJ3xYMvGTcHyyRk768WVv3W1KRsYtrNbDAxfa8omTofv/8T5sLsGQLrLXtJIVO1ZQ6kakXXEFlEphd/viGQTZhaIVumS2If8iMi3gVuAtSIyBnyemsL6joh8HHgV+MBKdtJxnPah9cX5TmZJxaWqdxmi97a4L47jdAirOQ1sBg/5cRxnEZ1uVXTF5TjOPGoL7664HMeJGR5k7ThO7PA1rgYUqEjYLqwVe/MCy/TbnbU32Ojrt83nJ07brhdHxs6YslQ63I/MayfMOrOn7PZ2rLddHt777h2m7OXjCyOwfkP/aDiIYe2a8OYVAKfP2BtiDA1FuQbY/c8Ym0OcPjNm1kllJ03ZmUn7Hh8/aWdzSKfD78HQgL0pSsHYYANAU7a1TSL8F6oR/hAJCdeTiEwlxl4ZLUERqnG3KjqO88ajwwdcKx/y4zhOzKgvzjdzNIOI7BaRF0TksIgEM8mIyC0i8pSIHBSRf1iqTR9xOY6zmBYNuUQkCdwL3AqMAQdE5GFVPdRwzhDwV8BuVX01KmnDBXzE5TjOIlo44roJOKyqR1S1CDxALS1WIx8CHlTVV2vX1iVjn11xOY4zDwWqVWnqoBYK+ETDsWdBc6PAsYbPY/WyRq4GhkXkRyLypIh8ZKk++lTRcZz5KNC8H9dZVd0VIQ81tHAimgLeSi2MsBv4iYj8VFVftBptq+JKJhMMDfUHZeWU7Q6Ry4UzG2jJNjGfn7Kj/4++apv/cznbtN6dDQ9QT7xiZ6nYmLU3UBjdbG8AMXTZNlOWnrZN+RgbiGy+4Sa7ymvhDSUAusu2O0cFO+PEzExYtqnHXr4oGptGAEhvnynb3GtvBNI/FM5SMX3upFnn9GvnTFnJ2OwFYHbO3oCDhL1o1JsNZysp5iPcPIzNN8RwrVguLfTjGgO2NHzeDCz0bRmjpgBngBkReQy4ATAVl08VHcdZjDZ5LM0BYIeIbBORDHAntbRYjXwP+OcikhKRHuDtwHNRjfpU0XGcBTTv6rAUqloWkbuBR4AksE9VD4rIJ+vy+1T1ORH5AfA0UAW+pqrP2q264nIcJ0QLPVBVdT+wf0HZfQs+/xnwZ8226YrLcZz5KGjVg6wdx4kdrrj+iWqlzPTk2XBHinZu9rSx3Th2ynNSSVuYz9kWx+F+O6h4uDds/SlM2FbF9ZfZOdtHr7/FlD07VjRlLx62Ze/YNBIsn5y062zYHs5TD5Agb8qKc7bFcUjDFsKp07bFrrto577fNBL+XgCTFTsPfPr68CbrhUnbqviP++0tFMZetb9zMmNbHKMUQcGYlpUibGcJ415ZCQmWTYcHK/qIy3GcxbjichwnVizPAXVVcMXlOM4iPJGg4zjxw62KjuPEDfERl+M4saL5cJ5Vo+2KK2mMQCsFO6BUDVNyAjswu2LkPAeYsK3uTE1F5BufC7sUXDZou1C87T3vMWWbr7nZlD247+umbGOfHXCcLIbz6R8/8rLd3pXXmbLsmqtMWa/aLiz58XBKpe5q2D0BoFiwXS/OTtuyoXV2QPqajVuD5YXcgFknYYuoZOzAcrFebqBUtN1RpBxOFiBqJxEol8N/uq1xh5COX5xfMshaRPaJyGkRebah7AsicryeavUpEbl9ZbvpOE5baV2Q9YrQTHaIbwC7A+V/oao768f+gNxxnLhSbfJYJZacKqrqYyKydeW74jhORxADP66Lycd1t4g8XZ9KmgsXIrLnQlrXXN6e5zuO0zmINnesFq9XcX0F2A7sBE4CX7ROVNW9qrpLVXf19djZQB3H6SAugTWuRajqKVWtqGoV+Cq1nTwcx3HawutyhxCRTap6Ibz+/UBktsJ/qoc9vKxEZAaQZFi/RuyGjuYj2otYVBxZE96yHWBTT9j94i1vu9qsc+07bJeHidO2C0hX2c5gceXmzaasany5jevXmXXKs7ZbST4iq0SxbNcrFcKvVgXblePl42Om7JlnnzBl77jZ7uOajeHsHFPT9g5YafsVYO022/WlmrBfyEoxwrXBcLM5f3rSrDM3He5kNcKFYjnE3gFVRL4N3EJtG6Ix4PPALSKyk9pg8SjwhyvYR8dx2okS/5AfVb0rUGx7RzqOE3/iPuJyHOeNR+ynio7jvAFxxeU4TuxwxeU4TpxYbefSZmir4lKFqhEJX5izfRQyRjaEVMrenCCZsE3kOzbaGQqy3bZJe+vWLcHyG95lZ4DYdM31puypn/y1Kbv8cruPG99kt5lZtz1YnuoZNOvkZ223jMKUnQHi1IljpmziVNi1oVKyszx094c3IwFYu9Z+1sdO/NKUbdg0GiwvR2xvr4U5UyYzE6asouHMHAAaoQm6u8LfLbPJ/s5TWSNjSqpF1sC4WxUdx3nj4SMux3Hihysux3Fiha9xOY4TS1xxOY4TN6LieTuBi8nH5TiOsyq0dcQlAulk+JITEZshVGbDptnunm6zTjJhj3XXR2SAePWkHZG//S23Bcs3/3Yos/UFbLeG0vSMKRvst90X1l2905TNpEaC5Qd/ecCsM1ew+zE1Zd+Ps8d/bcqSlbA7SjZrv3Kj2+ysF9dfbW/aUU7aGRvSyaFwecbOHpKatTfEyB89bsosVx+AcsQQIZcMb+zSs9b+XhsuC2e9SKdbNBbxqaLjOLHCF+cdx4klrrgcx4kdrrgcx4kTglsVHceJG03u8NPsOpiI7BaRF0TksIh8NuK8t4lIRUR+f6k22xtkXVXmCmGLTU+X3RXJhq0u6YSd81wrtqy7L9wewB133mHK3nHbe4PlA2s3mHVOHXnOlCUj+j85beecP3P0BVN2Yjps2frRQw+Zdfp67GDe2YgA7I0bbcvnQH/YIvbKmB2YXYy4HyOXbTVlV//2W00Zla5g8fiknd8+b1ixASYKdh9F7Xd4tmAPYXIa1gCas62b14aNpVRbNcVrUTsikgTuBW4FxoADIvKwqh4KnPenwCPNtOsjLsdxFtO67cluAg6r6hFVLQIPAKHRwb8HvgvYu5g04IrLcZxFLGOquPbChs/1Y8+CpkaBxmH2WL3sN9cSGaW2W9h9zfbPF+cdx1lM81PFs6q6K0IemncvbP2/Ap9R1YpIc3nAXHE5jjMfbalVcQxozMC5GTix4JxdwAN1pbUWuF1Eyqr6v6xGXXE5jrOY1vlxHQB2iMg24DhwJ/CheZdS3XbhdxH5BvD9KKUFrrgcxwnQqpAfVS2LyN3UrIVJYJ+qHhSRT9blTa9rNdLMTtZbgG8CG4EqsFdVvywiI8DfAFup7Wb9QVW1E3IDilJVIxd81Q5QlXJ43FpWO1BWIu58Njtgyna+1Tatd6XDbgOHnrJznk+ceNmUzc3ZOcqnJ8ZN2bHDh0xZTsOB5+mKbVrvS9nuIQNDdqDvumHbHeLkqdeC5eWi/czy07brxbFXXjVlcNCU5HLhnPnZlP1+lLvWm7JzZfvd6e62c+b39NsJAbpTYZeN6fyUWadcDbtlaKuGSi30nFfV/cD+BWVBhaWqf9BMm81YFcvAH6vqtcDNwKdE5Drgs8CjqroDeLT+2XGcuNOsK8QqhgUtqbhU9aSq/qL++zTwHDVz5h3A/fXT7gfet1KddBynfQit9ZxfCZa1xiUiW4EbgZ8BG1T1JNSUm4jY42vHcWLFJZPWRkT6qHm2flpVp5r1t6g7pO0BGO4Lz+Udx+kwOlxxNeU5LyJpakrrW6r6YL34lIhsqss3Ybjqq+peVd2lqrt6uzOt6LPjOCtN3Ne4pDa0+jrwnKp+qUH0MPDR+u8fBb7X+u45jtN2WpwdYiVoZqr4TuDDwDMi8lS97HPAPcB3ROTjwKvAB5ZuSql5VCymWjbcJIBUOpwjvhKR47uIHcW/YdDOA//Iw//blI1sCJvd12/aEiwHKObtLA/ptG0+7+u1ze6phO2+0Gu4bGzcEM5RDlCYsr1Yunvt6f25M2dNWakYfjb93bZbQDFnu0O8FJEz/+TzdraMubLhcpK272El6v5usd1D6LXf4USX7Y6SNVwbhrHv1bVv2hYs786+YtZZFh0+VVxScanqjwnHGwGE87w4jhNrOj2RoHvOO46ziEvGqug4zhuEVV54bwZXXI7jLMYVl+M4ceKC53wn44rLcZxFSMuS168M7VVcCtVq2ECZichQkE0ZJo6E7b2vEduyVyMyFJw9E85qAJAzZN2lN9vXwv5eI8O2i8LQZetMWbkyZ8qOnzgZLI/KGpBI2q9BsWy7lSTF3mSjNxt2YTESfdTaixJGDAEqRdvlJGG8b1N52wWk2GVn7ei/zL73M92Tpmy6artKzM6E3SnXDFxp1lm7fm2wPJVuwZ+0r3E5jhNHfKroOE78cMXlOE7c8BGX4zjxwxWX4zixorW7/KwIrrgcx5mH+3EtIkFCwtkGsl12JLwamR56e8Imd4De/rC5GCBfsiP11wzYOcNSRj+K520XimrCbi+ftv+tbdgQjv4HqBZt0/o1128Olj/+9//XrFPUvClLRySMLEzb9QYGwtktMin7lUtG/JvPzdrP7JUTtmvDxGT4mc3JjFln/TV2tqfRoYjsFmo/64mz9r3KzIbdSnpHIzJ65MPZN6qtGilpZ2suH3E5jrMIH3E5jhMv3AHVcZw44ovzjuPEDldcjuPEC8UX5xtJCGRSYYtNfs4OXk1mwwHT1aSdDz1fsgNlk2n7oXRlbKtROh3uR6ZnyKwzOGAHe78WEdCdHw1bBwHWb7nKlB0/Hc4D/6a3vcuskztzwpQdefFZUzaTs4OKU6nw/R8ctHPpi7EfAcDJMbuPv/51RJB1V/j+D2ywLdLrRiL6GGHdlHH7WQ9P2H9qo+tHguWbh+x34PChcDD9XMG2OC8HX5x3HCd+uOJyHCdOuAOq4zjxQ9UTCTqOE0M6W2+54nIcZzE+VXQcJ14oEPepoohsAb4JbASqwF5V/bKIfAH4d8CZ+qmfU9X9kRdLCRvWhd0hSmfPmfUKlbCZfMbesR1NhINQa/2wv/bAoB3YmjG2ty/M2Ob47qgc4EVb9sTj/2jKrrzmlCkbGwubyRMR+fl7uuzc8ckIl5Pubtv8PzMddoco5G03lXLZNuX3ddv9eOeNV5uyrBHsXU7aufQrJTsgunDMdodITGVN2freflN249VvCtcZ2mjWefLkK8Hycsl+75dFZ+st7DD431AG/lhVrwVuBj4lItfVZX+hqjvrR6TSchwnPog2dzTVlshuEXlBRA6LyGcD8n8rIk/Xj8dF5Ial2lxyxKWqJ4GT9d+nReQ5YLS5LjuOE0daZVUUkSRwL3ArMAYcEJGHVfVQw2mvAP9CVSdE5DZgL/D2qHabGXE1dmIrcCPws3rR3XUtuU9EhpfTluM4HYou41iam4DDqnpEVYvAA8Ad8y6n+riqXkiq9lPADhmo07TiEpE+4LvAp1V1CvgKsB3YSW1E9kWj3h4ReUJEnpjKtyYcwXGclaPmgKpNHcDaC3/f9WPPguZGgWMNn8eInrF9HPg/S/WxKauiiKSpKa1vqeqDAKp6qkH+VeD7obqqupfa0I/tlw11+JKf4zgAEWGjCzmrqrsi5CGrUFAPiMi7qSkuO7C2TjNWRQG+Djynql9qKN9UX/8CeD9gR+M6jhMrpHXZIcaALQ2fNwOLIuZF5Hrga8Btqmq7GNRpZsT1TuDDwDMi8lS97HPAXSKyk5r2PAr84VINZTLC5VvCebkHxTYlHz4WNk+fOm3f3GLFNp/39dlfeyZvuzZUqmH/i2TEjHv8jP0Mpqdtk/xsye5HUm1Zf194qfHUa+NmnbGcbeKvqu1GsWG97Toi1VKwfGLCzg/f1Ws/s6Eh250gk7Tv/1zRcA9I2S4gM3N2e8Vpu15v1a531RbbteGyjeH9EY6N2dlDzp0O/02Uyy1IpNXaDKgHgB0isg04DtwJfKjxBBG5HHgQ+LCqvthMo81YFX9MeLjn7g+Oc0nSulhFVS2LyN3AI0AS2KeqB0Xkk3X5fcCfAGuAv6pN8CgvMf10z3nHcQK0MJFg3cdz/4Ky+xp+/wTwieW06YrLcZz5+IawjuPEEk/d7DhO7OhsveWKy3GcxUjLtsReGdqquJIpYWDYyLBwxo7IH16fDAt67Q0Pzr5mb74xG7GFfSpjb5RQNKzdVcvkDpQqdj/OF2zXgN6IbAizedt9oTAb3iyjGNHHSkRGAVXj3gO58/YzGxgMbzoyMDho1ink7fbOnrXvVV+/naVCEuGHJmV7SJFJ2RumdNkiMhn7Xm29apspK+TDfXnsHw4FywF+9eLpcFuztotN0yjLcUBdFXzE5TjOPARtpQPqiuCKy3GcxbjichwndrjichwnVvgal+M4ccStio7jxAz1qZESxSYAAAfrSURBVGIjIkIqG75kdiCcNQJgpC9s0k4VbFeDdLf9H2NqIuJrV+wI/+7s+nCVtH2tytykKcv02P1Ip+z7kUzabiBzGu5LsWS7gGhEBoiovOJatN0yKoYoHZGVgS7bBWQyIqtEoRjORAEwOBx2b0kZbhIAiYh7n8d2Nzh1ZtqUTeTsetMz4Xfkhz963r6W4TkyW2yRO4QrLsdxYkdnzxRdcTmOsxj343IcJ3644nIcJ1aogrEJc6fgistxnMX4iMtxnNjhius3VKtCLmeYmpN9Zr2+3rBtPd1t39zeLnvzjcFBexicmypEyE6Fy2ciskPM2rL+jL3ZRDZjuw2UZ203kFQ6bObPROygme6ysxqI2BV7+u3XJ2GIyhXbXJ/pttsbGLJdQMbHbTeEacM9ZGCNfe/zZdt15MVX7M1Pnn/mmCnbsMbOOrJhs/HdEvZ7unYwvHnI6ZztGtI0CrQo5/xK4SMux3EWoGAo/E7BFZfjOPNRfHHecZwY4mtcjuPEDldcjuPEi0sgyFpEssBjQFf9/L9V1c+LyAjwN8BW4CjwQVW1I2GBYhHGfh2WzU3aVsD+dWFLVLY7IrjWNlIyMmJ/7dyMnfd8cjIsmzhnB+VO2EYoklXbmleNeHEqFdtSSTUsizAqIgk7yDqZsu9VISIgXQ3jYbpqP7PyzLgpqxTs51KJCNyezIXrRaTgZzzCsnz0sP1AJ8/ZfSzm7AtuHNwYLL/uilGzznmji4dP2RbWplGgw9PaRL3PF5gD3qOqNwA7gd0icjPwWeBRVd0BPFr/7DjOpYBqc8cqsaTi0hq5+sd0/VDgDuD+evn9wPtWpIeO47SZeshPM8cq0cyICxFJishTwGngh6r6M2CDqp4EqP8MJ6tyHCdeKKhWmzpWi6YW51W1AuwUkSHgIRF5c7MXEJE9wB6AdUP2/neO43QQHe4539SI6wKqOgn8CNgNnBKRTQD1n8EdKlV1r6ruUtVdg332ArzjOB1E3Ne4RGRdfaSFiHQD/xJ4HngY+Gj9tI8C31upTjqO00ZUa1bFZo5Vopmp4ibgfhFJUlN031HV74vIT4DviMjHgVeBDyzVkEqKSnptUFbK7DLrzVXDQcWJcni7eYDsoG3iH1pnj/yGE3YQ8Eg+/KAmx+192SfP2i4PhRn79lfKtosFav+/qZbDfZwt2PnhM5mI/PYpu//Ts/aLW8gZgfFqBzD3d4cDhwGqiSlTVirZ97GrNzwqyGbs/PZDGbuPVzJkyq6/wV4KueaGnaZs61VXBctv+me2e8XY8Vyw/PEjEf43yyHuflyq+jRwY6D8HPDeleiU4ziriaJRvoIdgHvOO44zH09r4zhOLOnwtDbLsio6jnPpo4BWtamjGURkt4i8ICKHRWRRhI3U+Mu6/GkRectSbbrichxnPlpPJNjMsQR1o969wG3AdcBdInLdgtNuA3bUjz3AV5Zq1xWX4ziL0EqlqaMJbgIOq+oRVS0CD1ALF2zkDuCb9fDCnwJDF3xELUTbaPYUkTPAhfwQawHbn6F9eD/m4/2YT9z6cYWqrruYC4nID+rXa4Ys0Oj7sldV9za09fvAblX9RP3zh4G3q+rdDed8H7hHVX9c//wo8BlVfcK6aFsX5xtvqIg8oaq281ab8H54P7wf81HV3S1sLuRQuXC01Mw58/CpouM4K8kYsKXh82bgxOs4Zx6uuBzHWUkOADtEZJuIZIA7qYULNvIw8JG6dfFm4PyFzDMWq+nHtXfpU9qC92M+3o/5eD8uAlUti8jdwCNAEtinqgdF5JN1+X3AfuB24DCQBz62VLttXZx3HMdpBT5VdBwndrjichwndqyK4loqBKCN/TgqIs+IyFMiYvqMrMB194nIaRF5tqFsRER+KCIv1X8Or1I/viAix+v35CkRub0N/dgiIn8vIs+JyEER+aN6eVvvSUQ/2npPRCQrIj8XkV/V+/Ef6+Vtf0c6lbavcdVDAF4EbqVmBj0A3KWqh9rakVpfjgK7VLWtDoYi8jtAjpq38JvrZf8FGFfVe+rKfFhVP7MK/fgCkFPVP1/Jay/oxyZgk6r+QkT6gSepbb7yB7TxnkT044O08Z6IiAC9qpoTkTTwY+CPgH9Nm9+RTmU1RlzNhABc0qjqY8DCTQTbvmuS0Y+2o6onVfUX9d+ngeeAUdp8TyL60VZ8Z62lWQ3FNQoca/g8xiq8HHUU+DsRebK+qcdq0km7Jt1dj9Lf1+7piIhspZa4clV3klrQD2jzPRHfWSuS1VBcy3bvX0HeqapvoRad/qn61OmNzleA7dQ2/z0JfLFdFxaRPuC7wKdV1c7V3P5+tP2eqGpFVXdS8yK/SZaxs9YbgdVQXMt2718pVPVE/edp4CFq09jVoqldk1YaVT1V/6OpAl+lTfekvpbzXeBbqvpgvbjt9yTUj9W6J/VrL3tnrTcCq6G4mgkBWHFEpLe+AIuI9AK/CzwbXWtF6YhdkxakE3k/bbgn9cXorwPPqeqXGkRtvSdWP9p9T8R31loaVW37Qc29/0XgZeA/rFIfrgR+VT8OtrMfwLepTTlK1EagHwfWAI8CL9V/jqxSP/478AzwNLU/lE1t6Me7qC0XPA08VT9ub/c9iehHW+8JcD3wy/r1ngX+pF7e9nekUw8P+XEcJ3a457zjOLHDFZfjOLHDFZfjOLHDFZfjOLHDFZfjOLHDFZfjOLHDFZfjOLHj/wOS5JLwlMi7LwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((train_data[0].reshape(32, 32, 3)+1)/2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow((train_data[i].reshape(32, 32, 3)+1)/2)\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   # frog: 6\n",
    "            new_t_labels.append([0])  # Frog 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator가 32X32X3 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 17.221202850341797, \t Total Dis Loss : 0.6876643300056458\n",
      "Steps : 200, \t Total Gen Loss : 14.107324600219727, \t Total Dis Loss : 0.5261241793632507\n",
      "Steps : 300, \t Total Gen Loss : 13.918658256530762, \t Total Dis Loss : 0.43970656394958496\n",
      "Steps : 400, \t Total Gen Loss : 15.863085746765137, \t Total Dis Loss : 0.11770561337471008\n",
      "Steps : 500, \t Total Gen Loss : 18.69468116760254, \t Total Dis Loss : 0.09753168374300003\n",
      "Steps : 600, \t Total Gen Loss : 16.91179084777832, \t Total Dis Loss : 0.2538461685180664\n",
      "Steps : 700, \t Total Gen Loss : 15.974733352661133, \t Total Dis Loss : 0.06814403086900711\n",
      "Steps : 800, \t Total Gen Loss : 18.196239471435547, \t Total Dis Loss : 0.16496649384498596\n",
      "Steps : 900, \t Total Gen Loss : 16.33879852294922, \t Total Dis Loss : 0.20363202691078186\n",
      "Steps : 1000, \t Total Gen Loss : 20.265602111816406, \t Total Dis Loss : 0.1821017563343048\n",
      "Steps : 1100, \t Total Gen Loss : 20.07366371154785, \t Total Dis Loss : 0.06295118480920792\n",
      "Steps : 1200, \t Total Gen Loss : 20.51274871826172, \t Total Dis Loss : 0.057617124170064926\n",
      "Steps : 1300, \t Total Gen Loss : 16.961030960083008, \t Total Dis Loss : 0.14816416800022125\n",
      "Steps : 1400, \t Total Gen Loss : 16.35630989074707, \t Total Dis Loss : 0.32106301188468933\n",
      "Steps : 1500, \t Total Gen Loss : 18.190858840942383, \t Total Dis Loss : 0.09093974530696869\n",
      "Steps : 1600, \t Total Gen Loss : 17.71011734008789, \t Total Dis Loss : 0.06520599126815796\n",
      "Steps : 1700, \t Total Gen Loss : 21.570083618164062, \t Total Dis Loss : 0.10885309427976608\n",
      "Steps : 1800, \t Total Gen Loss : 18.668170928955078, \t Total Dis Loss : 0.02061065100133419\n",
      "Steps : 1900, \t Total Gen Loss : 18.174842834472656, \t Total Dis Loss : 0.025430817157030106\n",
      "Steps : 2000, \t Total Gen Loss : 19.005084991455078, \t Total Dis Loss : 0.06792733073234558\n",
      "Steps : 2100, \t Total Gen Loss : 21.713991165161133, \t Total Dis Loss : 0.014377593994140625\n",
      "Steps : 2200, \t Total Gen Loss : 16.67176055908203, \t Total Dis Loss : 0.05562250688672066\n",
      "Steps : 2300, \t Total Gen Loss : 21.24698829650879, \t Total Dis Loss : 0.01169967744499445\n",
      "Steps : 2400, \t Total Gen Loss : 20.119169235229492, \t Total Dis Loss : 0.021559350192546844\n",
      "Steps : 2500, \t Total Gen Loss : 20.33147430419922, \t Total Dis Loss : 0.05442270636558533\n",
      "Steps : 2600, \t Total Gen Loss : 20.829906463623047, \t Total Dis Loss : 0.030245337635278702\n",
      "Steps : 2700, \t Total Gen Loss : 17.944849014282227, \t Total Dis Loss : 0.029220309108495712\n",
      "Steps : 2800, \t Total Gen Loss : 22.296846389770508, \t Total Dis Loss : 0.018637914210557938\n",
      "Steps : 2900, \t Total Gen Loss : 21.636592864990234, \t Total Dis Loss : 0.02471495047211647\n",
      "Steps : 3000, \t Total Gen Loss : 23.36347198486328, \t Total Dis Loss : 0.014680844731628895\n",
      "Steps : 3100, \t Total Gen Loss : 21.159435272216797, \t Total Dis Loss : 0.04998402297496796\n",
      "Steps : 3200, \t Total Gen Loss : 21.48480224609375, \t Total Dis Loss : 0.013568440452218056\n",
      "Steps : 3300, \t Total Gen Loss : 19.619348526000977, \t Total Dis Loss : 0.033253807574510574\n",
      "Steps : 3400, \t Total Gen Loss : 19.008441925048828, \t Total Dis Loss : 0.0673559159040451\n",
      "Steps : 3500, \t Total Gen Loss : 20.252132415771484, \t Total Dis Loss : 0.009828044101595879\n",
      "Steps : 3600, \t Total Gen Loss : 21.858375549316406, \t Total Dis Loss : 0.009734069928526878\n",
      "Steps : 3700, \t Total Gen Loss : 23.481292724609375, \t Total Dis Loss : 0.008497933857142925\n",
      "Steps : 3800, \t Total Gen Loss : 20.87417984008789, \t Total Dis Loss : 0.006359059363603592\n",
      "Steps : 3900, \t Total Gen Loss : 20.723339080810547, \t Total Dis Loss : 0.009570007212460041\n",
      "Steps : 4000, \t Total Gen Loss : 22.23270034790039, \t Total Dis Loss : 0.010909687727689743\n",
      "Steps : 4100, \t Total Gen Loss : 23.899354934692383, \t Total Dis Loss : 0.0056567564606666565\n",
      "Steps : 4200, \t Total Gen Loss : 20.868270874023438, \t Total Dis Loss : 0.02983732521533966\n",
      "Steps : 4300, \t Total Gen Loss : 20.792789459228516, \t Total Dis Loss : 0.019306480884552002\n",
      "Steps : 4400, \t Total Gen Loss : 23.15961456298828, \t Total Dis Loss : 0.007472830358892679\n",
      "Steps : 4500, \t Total Gen Loss : 21.076208114624023, \t Total Dis Loss : 0.034720346331596375\n",
      "Steps : 4600, \t Total Gen Loss : 24.41379165649414, \t Total Dis Loss : 0.0025709676556289196\n",
      "Steps : 4700, \t Total Gen Loss : 22.889514923095703, \t Total Dis Loss : 0.0019667495507746935\n",
      "Steps : 4800, \t Total Gen Loss : 21.969100952148438, \t Total Dis Loss : 0.008664857596158981\n",
      "Steps : 4900, \t Total Gen Loss : 20.591922760009766, \t Total Dis Loss : 0.01076104212552309\n",
      "Steps : 5000, \t Total Gen Loss : 24.17996597290039, \t Total Dis Loss : 0.003965945914387703\n",
      "Steps : 5100, \t Total Gen Loss : 21.862619400024414, \t Total Dis Loss : 0.006631322670727968\n",
      "Steps : 5200, \t Total Gen Loss : 22.016700744628906, \t Total Dis Loss : 0.02019614353775978\n",
      "Steps : 5300, \t Total Gen Loss : 20.251842498779297, \t Total Dis Loss : 0.009178048931062222\n",
      "Steps : 5400, \t Total Gen Loss : 23.813241958618164, \t Total Dis Loss : 0.0524306520819664\n",
      "Steps : 5500, \t Total Gen Loss : 21.877948760986328, \t Total Dis Loss : 0.018286461010575294\n",
      "Steps : 5600, \t Total Gen Loss : 22.51073455810547, \t Total Dis Loss : 0.0054854415357112885\n",
      "Time for epoch 1 is 72.8499755859375 sec\n",
      "Steps : 5700, \t Total Gen Loss : 21.04815673828125, \t Total Dis Loss : 0.009792969562113285\n",
      "Steps : 5800, \t Total Gen Loss : 18.991043090820312, \t Total Dis Loss : 0.04622536897659302\n",
      "Steps : 5900, \t Total Gen Loss : 21.59808349609375, \t Total Dis Loss : 0.030307482928037643\n",
      "Steps : 6000, \t Total Gen Loss : 20.67108726501465, \t Total Dis Loss : 0.03590933233499527\n",
      "Steps : 6100, \t Total Gen Loss : 24.703689575195312, \t Total Dis Loss : 0.025247782468795776\n",
      "Steps : 6200, \t Total Gen Loss : 21.952495574951172, \t Total Dis Loss : 0.005444279871881008\n",
      "Steps : 6300, \t Total Gen Loss : 20.771541595458984, \t Total Dis Loss : 0.011916687712073326\n",
      "Steps : 6400, \t Total Gen Loss : 24.017873764038086, \t Total Dis Loss : 0.018172701820731163\n",
      "Steps : 6500, \t Total Gen Loss : 20.47956085205078, \t Total Dis Loss : 0.0276857428252697\n",
      "Steps : 6600, \t Total Gen Loss : 23.801755905151367, \t Total Dis Loss : 0.016547739505767822\n",
      "Steps : 6700, \t Total Gen Loss : 20.23189926147461, \t Total Dis Loss : 0.01156515721231699\n",
      "Steps : 6800, \t Total Gen Loss : 22.960512161254883, \t Total Dis Loss : 0.001430307049304247\n",
      "Steps : 6900, \t Total Gen Loss : 21.263397216796875, \t Total Dis Loss : 0.014543845318257809\n",
      "Steps : 7000, \t Total Gen Loss : 23.386056900024414, \t Total Dis Loss : 0.0033740797080099583\n",
      "Steps : 7100, \t Total Gen Loss : 20.112707138061523, \t Total Dis Loss : 0.006592875812202692\n",
      "Steps : 7200, \t Total Gen Loss : 21.582752227783203, \t Total Dis Loss : 0.006350325886160135\n",
      "Steps : 7300, \t Total Gen Loss : 20.59415054321289, \t Total Dis Loss : 0.002593263052403927\n",
      "Steps : 7400, \t Total Gen Loss : 21.99060821533203, \t Total Dis Loss : 0.00374310789629817\n",
      "Steps : 7500, \t Total Gen Loss : 21.38822364807129, \t Total Dis Loss : 0.002702282974496484\n",
      "Steps : 7600, \t Total Gen Loss : 22.024093627929688, \t Total Dis Loss : 0.0015572499250993133\n",
      "Steps : 7700, \t Total Gen Loss : 22.51810073852539, \t Total Dis Loss : 0.0012383908033370972\n",
      "Steps : 7800, \t Total Gen Loss : 20.271711349487305, \t Total Dis Loss : 0.002509215148165822\n",
      "Steps : 7900, \t Total Gen Loss : 22.515365600585938, \t Total Dis Loss : 0.0011261170729994774\n",
      "Steps : 8000, \t Total Gen Loss : 20.099355697631836, \t Total Dis Loss : 0.05385546013712883\n",
      "Steps : 8100, \t Total Gen Loss : 22.100000381469727, \t Total Dis Loss : 0.0018330737948417664\n",
      "Steps : 8200, \t Total Gen Loss : 26.99489974975586, \t Total Dis Loss : 0.0010997616918757558\n",
      "Steps : 8300, \t Total Gen Loss : 20.009292602539062, \t Total Dis Loss : 0.0024169397074729204\n",
      "Steps : 8400, \t Total Gen Loss : 20.537437438964844, \t Total Dis Loss : 0.0100332573056221\n",
      "Steps : 8500, \t Total Gen Loss : 22.02823829650879, \t Total Dis Loss : 0.0016280426643788815\n",
      "Steps : 8600, \t Total Gen Loss : 21.18083953857422, \t Total Dis Loss : 0.0015203235670924187\n",
      "Steps : 8700, \t Total Gen Loss : 24.489402770996094, \t Total Dis Loss : 0.00116689782589674\n",
      "Steps : 8800, \t Total Gen Loss : 21.375507354736328, \t Total Dis Loss : 0.0013375002890825272\n",
      "Steps : 8900, \t Total Gen Loss : 19.333881378173828, \t Total Dis Loss : 0.02064528688788414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9000, \t Total Gen Loss : 24.15283966064453, \t Total Dis Loss : 0.003947913646697998\n",
      "Steps : 9100, \t Total Gen Loss : 21.407514572143555, \t Total Dis Loss : 0.0022330074571073055\n",
      "Steps : 9200, \t Total Gen Loss : 24.265722274780273, \t Total Dis Loss : 0.0006838716799393296\n",
      "Steps : 9300, \t Total Gen Loss : 28.25703239440918, \t Total Dis Loss : 0.0024464288726449013\n",
      "Steps : 9400, \t Total Gen Loss : 27.92479133605957, \t Total Dis Loss : 0.0018356989603489637\n",
      "Steps : 9500, \t Total Gen Loss : 23.403928756713867, \t Total Dis Loss : 0.002721863565966487\n",
      "Steps : 9600, \t Total Gen Loss : 25.1699275970459, \t Total Dis Loss : 0.0013967279810458422\n",
      "Steps : 9700, \t Total Gen Loss : 21.456758499145508, \t Total Dis Loss : 0.0013111945008859038\n",
      "Steps : 9800, \t Total Gen Loss : 23.739227294921875, \t Total Dis Loss : 0.0016619410598650575\n",
      "Steps : 9900, \t Total Gen Loss : 22.951465606689453, \t Total Dis Loss : 0.0010852718260139227\n",
      "Steps : 10000, \t Total Gen Loss : 24.833843231201172, \t Total Dis Loss : 0.0026338729076087475\n",
      "Steps : 10100, \t Total Gen Loss : 23.36693572998047, \t Total Dis Loss : 0.014320812188088894\n",
      "Steps : 10200, \t Total Gen Loss : 20.648326873779297, \t Total Dis Loss : 0.016535168513655663\n",
      "Steps : 10300, \t Total Gen Loss : 23.938615798950195, \t Total Dis Loss : 0.0021773960907012224\n",
      "Steps : 10400, \t Total Gen Loss : 21.306489944458008, \t Total Dis Loss : 0.010295266285538673\n",
      "Steps : 10500, \t Total Gen Loss : 21.980180740356445, \t Total Dis Loss : 0.0033156215213239193\n",
      "Steps : 10600, \t Total Gen Loss : 23.744617462158203, \t Total Dis Loss : 0.0017797171603888273\n",
      "Steps : 10700, \t Total Gen Loss : 25.35097885131836, \t Total Dis Loss : 0.0013637063093483448\n",
      "Steps : 10800, \t Total Gen Loss : 23.730098724365234, \t Total Dis Loss : 0.0009929263032972813\n",
      "Steps : 10900, \t Total Gen Loss : 21.63351821899414, \t Total Dis Loss : 0.0008735661976970732\n",
      "Steps : 11000, \t Total Gen Loss : 25.52012062072754, \t Total Dis Loss : 0.0009906820487231016\n",
      "Steps : 11100, \t Total Gen Loss : 23.477298736572266, \t Total Dis Loss : 0.0011694522108882666\n",
      "Steps : 11200, \t Total Gen Loss : 24.726884841918945, \t Total Dis Loss : 0.0007932994049042463\n",
      "Time for epoch 2 is 69.738685131073 sec\n",
      "Steps : 11300, \t Total Gen Loss : 23.97542381286621, \t Total Dis Loss : 0.0010181142715737224\n",
      "Steps : 11400, \t Total Gen Loss : 24.547897338867188, \t Total Dis Loss : 0.003832645248621702\n",
      "Steps : 11500, \t Total Gen Loss : 21.81264877319336, \t Total Dis Loss : 0.0013471012935042381\n",
      "Steps : 11600, \t Total Gen Loss : 24.022586822509766, \t Total Dis Loss : 0.0012601753696799278\n",
      "Steps : 11700, \t Total Gen Loss : 21.405101776123047, \t Total Dis Loss : 0.0016568319406360388\n",
      "Steps : 11800, \t Total Gen Loss : 22.308700561523438, \t Total Dis Loss : 0.01296193152666092\n",
      "Steps : 11900, \t Total Gen Loss : 22.585063934326172, \t Total Dis Loss : 0.005049899220466614\n",
      "Steps : 12000, \t Total Gen Loss : 24.250871658325195, \t Total Dis Loss : 0.0027884391602128744\n",
      "Steps : 12100, \t Total Gen Loss : 25.895936965942383, \t Total Dis Loss : 0.021842436864972115\n",
      "Steps : 12200, \t Total Gen Loss : 23.802215576171875, \t Total Dis Loss : 0.004363880958408117\n",
      "Steps : 12300, \t Total Gen Loss : 21.01611328125, \t Total Dis Loss : 0.0009602293721400201\n",
      "Steps : 12400, \t Total Gen Loss : 26.485551834106445, \t Total Dis Loss : 0.001568144652992487\n",
      "Steps : 12500, \t Total Gen Loss : 25.57940101623535, \t Total Dis Loss : 0.002455463632941246\n",
      "Steps : 12600, \t Total Gen Loss : 23.338394165039062, \t Total Dis Loss : 0.0205082930624485\n",
      "Steps : 12700, \t Total Gen Loss : 28.39992904663086, \t Total Dis Loss : 0.004932570271193981\n",
      "Steps : 12800, \t Total Gen Loss : 23.064504623413086, \t Total Dis Loss : 0.012505912221968174\n",
      "Steps : 12900, \t Total Gen Loss : 23.55077362060547, \t Total Dis Loss : 0.0031923751812428236\n",
      "Steps : 13000, \t Total Gen Loss : 27.96196174621582, \t Total Dis Loss : 0.014112823642790318\n",
      "Steps : 13100, \t Total Gen Loss : 28.851470947265625, \t Total Dis Loss : 0.0019486864330247045\n",
      "Steps : 13200, \t Total Gen Loss : 24.916332244873047, \t Total Dis Loss : 0.001788631547242403\n",
      "Steps : 13300, \t Total Gen Loss : 24.216079711914062, \t Total Dis Loss : 0.0012207148829475045\n",
      "Steps : 13400, \t Total Gen Loss : 25.326671600341797, \t Total Dis Loss : 0.0007557094213552773\n",
      "Steps : 13500, \t Total Gen Loss : 26.08511734008789, \t Total Dis Loss : 0.0028093515429645777\n",
      "Steps : 13600, \t Total Gen Loss : 24.361921310424805, \t Total Dis Loss : 0.00142391724511981\n",
      "Steps : 13700, \t Total Gen Loss : 27.376588821411133, \t Total Dis Loss : 0.002793830819427967\n",
      "Steps : 13800, \t Total Gen Loss : 22.642486572265625, \t Total Dis Loss : 0.0018617259338498116\n",
      "Steps : 13900, \t Total Gen Loss : 25.348146438598633, \t Total Dis Loss : 0.0024286266416311264\n",
      "Steps : 14000, \t Total Gen Loss : 26.80632209777832, \t Total Dis Loss : 0.02651060000061989\n",
      "Steps : 14100, \t Total Gen Loss : 26.322071075439453, \t Total Dis Loss : 0.001792985713109374\n",
      "Steps : 14200, \t Total Gen Loss : 25.255367279052734, \t Total Dis Loss : 0.006891390308737755\n",
      "Steps : 14300, \t Total Gen Loss : 25.537094116210938, \t Total Dis Loss : 0.00152457389049232\n",
      "Steps : 14400, \t Total Gen Loss : 23.68602752685547, \t Total Dis Loss : 0.003046792931854725\n",
      "Steps : 14500, \t Total Gen Loss : 24.410892486572266, \t Total Dis Loss : 0.0019801189191639423\n",
      "Steps : 14600, \t Total Gen Loss : 24.426292419433594, \t Total Dis Loss : 0.0013362206518650055\n",
      "Steps : 14700, \t Total Gen Loss : 24.971111297607422, \t Total Dis Loss : 0.0010099055944010615\n",
      "Steps : 14800, \t Total Gen Loss : 26.690994262695312, \t Total Dis Loss : 0.0011045793071389198\n",
      "Steps : 14900, \t Total Gen Loss : 25.373863220214844, \t Total Dis Loss : 0.0018104806076735258\n",
      "Steps : 15000, \t Total Gen Loss : 23.868505477905273, \t Total Dis Loss : 0.001035839319229126\n",
      "Steps : 15100, \t Total Gen Loss : 23.46666717529297, \t Total Dis Loss : 0.002849932760000229\n",
      "Steps : 15200, \t Total Gen Loss : 23.459585189819336, \t Total Dis Loss : 0.0015603657811880112\n",
      "Steps : 15300, \t Total Gen Loss : 22.81676483154297, \t Total Dis Loss : 0.0009611836867406964\n",
      "Steps : 15400, \t Total Gen Loss : 21.979259490966797, \t Total Dis Loss : 0.0031648026779294014\n",
      "Steps : 15500, \t Total Gen Loss : 24.86109733581543, \t Total Dis Loss : 0.0009855563985183835\n",
      "Steps : 15600, \t Total Gen Loss : 23.707923889160156, \t Total Dis Loss : 0.0013302054721862078\n",
      "Steps : 15700, \t Total Gen Loss : 26.197193145751953, \t Total Dis Loss : 0.00033489803900010884\n",
      "Steps : 15800, \t Total Gen Loss : 24.756711959838867, \t Total Dis Loss : 0.0002648396766744554\n",
      "Steps : 15900, \t Total Gen Loss : 20.244524002075195, \t Total Dis Loss : 0.022568395361304283\n",
      "Steps : 16000, \t Total Gen Loss : 29.516246795654297, \t Total Dis Loss : 0.0007509079878218472\n",
      "Steps : 16100, \t Total Gen Loss : 22.52710723876953, \t Total Dis Loss : 0.000753869186155498\n",
      "Steps : 16200, \t Total Gen Loss : 25.99409294128418, \t Total Dis Loss : 0.0006845585303381085\n",
      "Steps : 16300, \t Total Gen Loss : 26.459056854248047, \t Total Dis Loss : 0.00122809037566185\n",
      "Steps : 16400, \t Total Gen Loss : 24.020998001098633, \t Total Dis Loss : 0.000692937639541924\n",
      "Steps : 16500, \t Total Gen Loss : 26.5423583984375, \t Total Dis Loss : 0.0003565400547813624\n",
      "Steps : 16600, \t Total Gen Loss : 24.41204071044922, \t Total Dis Loss : 0.0034081987105309963\n",
      "Steps : 16700, \t Total Gen Loss : 24.327205657958984, \t Total Dis Loss : 0.000591730116866529\n",
      "Steps : 16800, \t Total Gen Loss : 23.424083709716797, \t Total Dis Loss : 0.0009695435292087495\n",
      "Time for epoch 3 is 69.16389799118042 sec\n",
      "Steps : 16900, \t Total Gen Loss : 24.932825088500977, \t Total Dis Loss : 0.0005332482396624982\n",
      "Steps : 17000, \t Total Gen Loss : 27.830490112304688, \t Total Dis Loss : 0.00038635378587059677\n",
      "Steps : 17100, \t Total Gen Loss : 27.087730407714844, \t Total Dis Loss : 0.0002961451536975801\n",
      "Steps : 17200, \t Total Gen Loss : 28.068443298339844, \t Total Dis Loss : 0.0030479501001536846\n",
      "Steps : 17300, \t Total Gen Loss : 29.71628189086914, \t Total Dis Loss : 0.036603935062885284\n",
      "Steps : 17400, \t Total Gen Loss : 25.63040542602539, \t Total Dis Loss : 0.006860434077680111\n",
      "Steps : 17500, \t Total Gen Loss : 27.82602310180664, \t Total Dis Loss : 0.004117262549698353\n",
      "Steps : 17600, \t Total Gen Loss : 25.795612335205078, \t Total Dis Loss : 0.0010500464122742414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17700, \t Total Gen Loss : 26.450645446777344, \t Total Dis Loss : 0.0017892573960125446\n",
      "Steps : 17800, \t Total Gen Loss : 26.183679580688477, \t Total Dis Loss : 0.029391707852482796\n",
      "Steps : 17900, \t Total Gen Loss : 26.6033992767334, \t Total Dis Loss : 0.004501688294112682\n",
      "Steps : 18000, \t Total Gen Loss : 25.691368103027344, \t Total Dis Loss : 0.009449385106563568\n",
      "Steps : 18100, \t Total Gen Loss : 21.722253799438477, \t Total Dis Loss : 0.0012457312550395727\n",
      "Steps : 18200, \t Total Gen Loss : 23.85072898864746, \t Total Dis Loss : 0.12988826632499695\n",
      "Steps : 18300, \t Total Gen Loss : 25.26854133605957, \t Total Dis Loss : 0.0006623233202844858\n",
      "Steps : 18400, \t Total Gen Loss : 24.33901596069336, \t Total Dis Loss : 0.0008553708903491497\n",
      "Steps : 18500, \t Total Gen Loss : 25.115272521972656, \t Total Dis Loss : 0.0005288789398036897\n",
      "Steps : 18600, \t Total Gen Loss : 20.57918357849121, \t Total Dis Loss : 0.003491668961942196\n",
      "Steps : 18700, \t Total Gen Loss : 21.96249008178711, \t Total Dis Loss : 0.002329019596800208\n",
      "Steps : 18800, \t Total Gen Loss : 24.340591430664062, \t Total Dis Loss : 0.0010246426099911332\n",
      "Steps : 18900, \t Total Gen Loss : 21.553386688232422, \t Total Dis Loss : 0.0014571170322597027\n",
      "Steps : 19000, \t Total Gen Loss : 23.79084014892578, \t Total Dis Loss : 0.0007850767578929663\n",
      "Steps : 19100, \t Total Gen Loss : 19.80007553100586, \t Total Dis Loss : 0.18509253859519958\n",
      "Steps : 19200, \t Total Gen Loss : 24.938018798828125, \t Total Dis Loss : 0.0007009173859842122\n",
      "Steps : 19300, \t Total Gen Loss : 21.245128631591797, \t Total Dis Loss : 0.0008877678774297237\n",
      "Steps : 19400, \t Total Gen Loss : 22.638675689697266, \t Total Dis Loss : 0.0006981503101997077\n",
      "Steps : 19500, \t Total Gen Loss : 23.327180862426758, \t Total Dis Loss : 0.0003035356930922717\n",
      "Steps : 19600, \t Total Gen Loss : 25.51116943359375, \t Total Dis Loss : 0.0004711920046247542\n",
      "Steps : 19700, \t Total Gen Loss : 24.549877166748047, \t Total Dis Loss : 0.002577375853434205\n",
      "Steps : 19800, \t Total Gen Loss : 23.301616668701172, \t Total Dis Loss : 0.00145946245174855\n",
      "Steps : 19900, \t Total Gen Loss : 23.152013778686523, \t Total Dis Loss : 0.00038824332295916975\n",
      "Steps : 20000, \t Total Gen Loss : 23.50322723388672, \t Total Dis Loss : 0.0003314403002150357\n",
      "Steps : 20100, \t Total Gen Loss : 23.63314437866211, \t Total Dis Loss : 0.0005219472222961485\n",
      "Steps : 20200, \t Total Gen Loss : 22.214584350585938, \t Total Dis Loss : 0.0005774555029347539\n",
      "Steps : 20300, \t Total Gen Loss : 24.053077697753906, \t Total Dis Loss : 0.0022765928879380226\n",
      "Steps : 20400, \t Total Gen Loss : 24.847869873046875, \t Total Dis Loss : 0.0003672459861263633\n",
      "Steps : 20500, \t Total Gen Loss : 24.128341674804688, \t Total Dis Loss : 0.0020956548396497965\n",
      "Steps : 20600, \t Total Gen Loss : 20.41579818725586, \t Total Dis Loss : 0.0017399832140654325\n",
      "Steps : 20700, \t Total Gen Loss : 24.09085464477539, \t Total Dis Loss : 0.0005609063664451241\n",
      "Steps : 20800, \t Total Gen Loss : 25.972946166992188, \t Total Dis Loss : 0.0007535871700383723\n",
      "Steps : 20900, \t Total Gen Loss : 21.92997169494629, \t Total Dis Loss : 0.005927487276494503\n",
      "Steps : 21000, \t Total Gen Loss : 23.54961585998535, \t Total Dis Loss : 0.00356759550049901\n",
      "Steps : 21100, \t Total Gen Loss : 21.596900939941406, \t Total Dis Loss : 0.0007853449205867946\n",
      "Steps : 21200, \t Total Gen Loss : 20.445171356201172, \t Total Dis Loss : 0.00952828861773014\n",
      "Steps : 21300, \t Total Gen Loss : 22.928138732910156, \t Total Dis Loss : 0.0018806305015459657\n",
      "Steps : 21400, \t Total Gen Loss : 24.98592758178711, \t Total Dis Loss : 0.21502824127674103\n",
      "Steps : 21500, \t Total Gen Loss : 28.908103942871094, \t Total Dis Loss : 0.001481391373090446\n",
      "Steps : 21600, \t Total Gen Loss : 29.296035766601562, \t Total Dis Loss : 0.00022123707458376884\n",
      "Steps : 21700, \t Total Gen Loss : 27.499267578125, \t Total Dis Loss : 0.0003930804959964007\n",
      "Steps : 21800, \t Total Gen Loss : 25.322555541992188, \t Total Dis Loss : 0.0009718000655993819\n",
      "Steps : 21900, \t Total Gen Loss : 24.83507537841797, \t Total Dis Loss : 0.0021471360232681036\n",
      "Steps : 22000, \t Total Gen Loss : 27.287086486816406, \t Total Dis Loss : 0.000711663335096091\n",
      "Steps : 22100, \t Total Gen Loss : 26.283248901367188, \t Total Dis Loss : 0.0009483647882007062\n",
      "Steps : 22200, \t Total Gen Loss : 25.261512756347656, \t Total Dis Loss : 0.025563683360815048\n",
      "Steps : 22300, \t Total Gen Loss : 24.76578712463379, \t Total Dis Loss : 0.003829352557659149\n",
      "Steps : 22400, \t Total Gen Loss : 23.84347915649414, \t Total Dis Loss : 0.0005906174192205071\n",
      "Steps : 22500, \t Total Gen Loss : 21.587291717529297, \t Total Dis Loss : 0.0020416374318301678\n",
      "Time for epoch 4 is 69.15066742897034 sec\n",
      "Steps : 22600, \t Total Gen Loss : 24.73821258544922, \t Total Dis Loss : 0.004776472225785255\n",
      "Steps : 22700, \t Total Gen Loss : 25.99974822998047, \t Total Dis Loss : 0.00097829126752913\n",
      "Steps : 22800, \t Total Gen Loss : 22.379459381103516, \t Total Dis Loss : 0.0011557458201423287\n",
      "Steps : 22900, \t Total Gen Loss : 26.798959732055664, \t Total Dis Loss : 0.004691197536885738\n",
      "Steps : 23000, \t Total Gen Loss : 24.376251220703125, \t Total Dis Loss : 0.0005371099105104804\n",
      "Steps : 23100, \t Total Gen Loss : 23.008895874023438, \t Total Dis Loss : 0.0019660417456179857\n",
      "Steps : 23200, \t Total Gen Loss : 24.224918365478516, \t Total Dis Loss : 0.0003972424892708659\n",
      "Steps : 23300, \t Total Gen Loss : 23.54130744934082, \t Total Dis Loss : 0.0008092564530670643\n",
      "Steps : 23400, \t Total Gen Loss : 24.06256103515625, \t Total Dis Loss : 0.006039364729076624\n",
      "Steps : 23500, \t Total Gen Loss : 26.300697326660156, \t Total Dis Loss : 0.0013705018209293485\n",
      "Steps : 23600, \t Total Gen Loss : 21.482616424560547, \t Total Dis Loss : 0.0016035649459809065\n",
      "Steps : 23700, \t Total Gen Loss : 24.188282012939453, \t Total Dis Loss : 0.0007469574920833111\n",
      "Steps : 23800, \t Total Gen Loss : 25.368244171142578, \t Total Dis Loss : 0.0006886664777994156\n",
      "Steps : 23900, \t Total Gen Loss : 23.615921020507812, \t Total Dis Loss : 0.0019755829125642776\n",
      "Steps : 24000, \t Total Gen Loss : 25.1566162109375, \t Total Dis Loss : 0.00031241675605997443\n",
      "Steps : 24100, \t Total Gen Loss : 28.673358917236328, \t Total Dis Loss : 0.01114677544683218\n",
      "Steps : 24200, \t Total Gen Loss : 28.58883285522461, \t Total Dis Loss : 0.002382624428719282\n",
      "Steps : 24300, \t Total Gen Loss : 26.183429718017578, \t Total Dis Loss : 0.0019611960742622614\n",
      "Steps : 24400, \t Total Gen Loss : 25.811595916748047, \t Total Dis Loss : 0.0003477053251117468\n",
      "Steps : 24500, \t Total Gen Loss : 28.60365104675293, \t Total Dis Loss : 0.0012012963416054845\n",
      "Steps : 24600, \t Total Gen Loss : 23.784374237060547, \t Total Dis Loss : 0.0007979054353199899\n",
      "Steps : 24700, \t Total Gen Loss : 26.672653198242188, \t Total Dis Loss : 0.002204523654654622\n",
      "Steps : 24800, \t Total Gen Loss : 23.873432159423828, \t Total Dis Loss : 0.0005410818266682327\n",
      "Steps : 24900, \t Total Gen Loss : 22.2969970703125, \t Total Dis Loss : 0.0013021007180213928\n",
      "Steps : 25000, \t Total Gen Loss : 26.03109359741211, \t Total Dis Loss : 0.0016798857832327485\n",
      "Steps : 25100, \t Total Gen Loss : 22.055103302001953, \t Total Dis Loss : 0.0006601421628147364\n",
      "Steps : 25200, \t Total Gen Loss : 25.092243194580078, \t Total Dis Loss : 0.00033556384732946754\n",
      "Steps : 25300, \t Total Gen Loss : 27.127628326416016, \t Total Dis Loss : 0.0003145923255942762\n",
      "Steps : 25400, \t Total Gen Loss : 23.34363555908203, \t Total Dis Loss : 0.00021475579706020653\n",
      "Steps : 25500, \t Total Gen Loss : 24.21323585510254, \t Total Dis Loss : 0.0005751214921474457\n",
      "Steps : 25600, \t Total Gen Loss : 27.90639877319336, \t Total Dis Loss : 0.00013094618043396622\n",
      "Steps : 25700, \t Total Gen Loss : 25.31058692932129, \t Total Dis Loss : 0.00016045253141783178\n",
      "Steps : 25800, \t Total Gen Loss : 22.60540008544922, \t Total Dis Loss : 0.1271749585866928\n",
      "Steps : 25900, \t Total Gen Loss : 22.152069091796875, \t Total Dis Loss : 0.003730770666152239\n",
      "Steps : 26000, \t Total Gen Loss : 24.923118591308594, \t Total Dis Loss : 0.0006734468624927104\n",
      "Steps : 26100, \t Total Gen Loss : 24.64421272277832, \t Total Dis Loss : 0.0006812723586335778\n",
      "Steps : 26200, \t Total Gen Loss : 23.23663330078125, \t Total Dis Loss : 0.0008168315980583429\n",
      "Steps : 26300, \t Total Gen Loss : 26.39969253540039, \t Total Dis Loss : 0.0008683549240231514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26400, \t Total Gen Loss : 23.352590560913086, \t Total Dis Loss : 0.0005842629470862448\n",
      "Steps : 26500, \t Total Gen Loss : 23.618667602539062, \t Total Dis Loss : 0.000383949518436566\n",
      "Steps : 26600, \t Total Gen Loss : 26.00638771057129, \t Total Dis Loss : 0.0006052502430975437\n",
      "Steps : 26700, \t Total Gen Loss : 25.210039138793945, \t Total Dis Loss : 0.00036206666845828295\n",
      "Steps : 26800, \t Total Gen Loss : 22.721649169921875, \t Total Dis Loss : 0.00038393508293665946\n",
      "Steps : 26900, \t Total Gen Loss : 24.18467140197754, \t Total Dis Loss : 0.0002052805502898991\n",
      "Steps : 27000, \t Total Gen Loss : 29.69091033935547, \t Total Dis Loss : 0.00018896741676144302\n",
      "Steps : 27100, \t Total Gen Loss : 24.654197692871094, \t Total Dis Loss : 0.006058487109839916\n",
      "Steps : 27200, \t Total Gen Loss : 25.3968563079834, \t Total Dis Loss : 0.0010078834602609277\n",
      "Steps : 27300, \t Total Gen Loss : 24.335186004638672, \t Total Dis Loss : 0.0011854812037199736\n",
      "Steps : 27400, \t Total Gen Loss : 24.29674530029297, \t Total Dis Loss : 0.0006981510668992996\n",
      "Steps : 27500, \t Total Gen Loss : 20.992202758789062, \t Total Dis Loss : 0.0019051150884479284\n",
      "Steps : 27600, \t Total Gen Loss : 25.330381393432617, \t Total Dis Loss : 0.0010028444230556488\n",
      "Steps : 27700, \t Total Gen Loss : 22.996524810791016, \t Total Dis Loss : 0.00038344573113135993\n",
      "Steps : 27800, \t Total Gen Loss : 22.06493377685547, \t Total Dis Loss : 0.0010047985706478357\n",
      "Steps : 27900, \t Total Gen Loss : 22.241561889648438, \t Total Dis Loss : 0.00028236678917892277\n",
      "Steps : 28000, \t Total Gen Loss : 23.8718204498291, \t Total Dis Loss : 0.0005799235077574849\n",
      "Steps : 28100, \t Total Gen Loss : 23.811031341552734, \t Total Dis Loss : 0.0002589621290098876\n",
      "Time for epoch 5 is 69.75232648849487 sec\n",
      "Steps : 28200, \t Total Gen Loss : 20.746326446533203, \t Total Dis Loss : 0.0015647370601072907\n",
      "Steps : 28300, \t Total Gen Loss : 24.612279891967773, \t Total Dis Loss : 0.0006357831880450249\n",
      "Steps : 28400, \t Total Gen Loss : 27.13426971435547, \t Total Dis Loss : 0.000515563995577395\n",
      "Steps : 28500, \t Total Gen Loss : 24.462688446044922, \t Total Dis Loss : 0.000332130235619843\n",
      "Steps : 28600, \t Total Gen Loss : 23.67827033996582, \t Total Dis Loss : 0.0010488678235560656\n",
      "Steps : 28700, \t Total Gen Loss : 25.87152862548828, \t Total Dis Loss : 0.0002072207280434668\n",
      "Steps : 28800, \t Total Gen Loss : 26.413700103759766, \t Total Dis Loss : 0.0002457154914736748\n",
      "Steps : 28900, \t Total Gen Loss : 26.871932983398438, \t Total Dis Loss : 0.0007530558505095541\n",
      "Steps : 29000, \t Total Gen Loss : 27.83919906616211, \t Total Dis Loss : 9.178816253552213e-05\n",
      "Steps : 29100, \t Total Gen Loss : 24.840225219726562, \t Total Dis Loss : 0.00029293971601873636\n",
      "Steps : 29200, \t Total Gen Loss : 28.418109893798828, \t Total Dis Loss : 0.00016370594676118344\n",
      "Steps : 29300, \t Total Gen Loss : 24.356689453125, \t Total Dis Loss : 0.00040079440805129707\n",
      "Steps : 29400, \t Total Gen Loss : 27.576435089111328, \t Total Dis Loss : 0.0001109093936975114\n",
      "Steps : 29500, \t Total Gen Loss : 22.99416160583496, \t Total Dis Loss : 0.8542221784591675\n",
      "Steps : 29600, \t Total Gen Loss : 22.047447204589844, \t Total Dis Loss : 0.0019302332075312734\n",
      "Steps : 29700, \t Total Gen Loss : 22.147987365722656, \t Total Dis Loss : 0.00585225410759449\n",
      "Steps : 29800, \t Total Gen Loss : 22.33089828491211, \t Total Dis Loss : 0.0012715323828160763\n",
      "Steps : 29900, \t Total Gen Loss : 26.058612823486328, \t Total Dis Loss : 0.0004708335909526795\n",
      "Steps : 30000, \t Total Gen Loss : 28.993667602539062, \t Total Dis Loss : 0.23187699913978577\n",
      "Steps : 30100, \t Total Gen Loss : 25.54134750366211, \t Total Dis Loss : 0.004473767708986998\n",
      "Steps : 30200, \t Total Gen Loss : 24.617530822753906, \t Total Dis Loss : 0.0025327512994408607\n",
      "Steps : 30300, \t Total Gen Loss : 24.14969253540039, \t Total Dis Loss : 0.0003640423819888383\n",
      "Steps : 30400, \t Total Gen Loss : 22.455158233642578, \t Total Dis Loss : 0.000820119574200362\n",
      "Steps : 30500, \t Total Gen Loss : 22.61170196533203, \t Total Dis Loss : 0.0011522779241204262\n",
      "Steps : 30600, \t Total Gen Loss : 19.59269905090332, \t Total Dis Loss : 0.00248219002969563\n",
      "Steps : 30700, \t Total Gen Loss : 22.419635772705078, \t Total Dis Loss : 0.0005510255577974021\n",
      "Steps : 30800, \t Total Gen Loss : 21.619857788085938, \t Total Dis Loss : 0.007765506394207478\n",
      "Steps : 30900, \t Total Gen Loss : 24.63939666748047, \t Total Dis Loss : 0.0010597563814371824\n",
      "Steps : 31000, \t Total Gen Loss : 19.589048385620117, \t Total Dis Loss : 0.00638910848647356\n",
      "Steps : 31100, \t Total Gen Loss : 20.446430206298828, \t Total Dis Loss : 0.0026972186751663685\n",
      "Steps : 31200, \t Total Gen Loss : 24.882431030273438, \t Total Dis Loss : 0.0006643907399848104\n",
      "Steps : 31300, \t Total Gen Loss : 23.764968872070312, \t Total Dis Loss : 0.0008893718477338552\n",
      "Steps : 31400, \t Total Gen Loss : 21.8359317779541, \t Total Dis Loss : 0.04819774627685547\n",
      "Steps : 31500, \t Total Gen Loss : 24.33771324157715, \t Total Dis Loss : 0.0005237022996880114\n",
      "Steps : 31600, \t Total Gen Loss : 22.475126266479492, \t Total Dis Loss : 0.001709470641799271\n",
      "Steps : 31700, \t Total Gen Loss : 23.008167266845703, \t Total Dis Loss : 0.0006072246469557285\n",
      "Steps : 31800, \t Total Gen Loss : 24.13996124267578, \t Total Dis Loss : 0.0006500727031379938\n",
      "Steps : 31900, \t Total Gen Loss : 23.67325210571289, \t Total Dis Loss : 0.0005402363603934646\n",
      "Steps : 32000, \t Total Gen Loss : 27.02292251586914, \t Total Dis Loss : 0.00028045158251188695\n",
      "Steps : 32100, \t Total Gen Loss : 24.71416473388672, \t Total Dis Loss : 0.005703376140445471\n",
      "Steps : 32200, \t Total Gen Loss : 22.969364166259766, \t Total Dis Loss : 0.0016711722128093243\n",
      "Steps : 32300, \t Total Gen Loss : 18.789085388183594, \t Total Dis Loss : 0.0076299672946333885\n",
      "Steps : 32400, \t Total Gen Loss : 23.427703857421875, \t Total Dis Loss : 0.0006369201000779867\n",
      "Steps : 32500, \t Total Gen Loss : 23.46663475036621, \t Total Dis Loss : 0.0013250020565465093\n",
      "Steps : 32600, \t Total Gen Loss : 22.603736877441406, \t Total Dis Loss : 0.0010761839803308249\n",
      "Steps : 32700, \t Total Gen Loss : 25.82638168334961, \t Total Dis Loss : 0.00887657143175602\n",
      "Steps : 32800, \t Total Gen Loss : 22.600614547729492, \t Total Dis Loss : 0.0006611249409615993\n",
      "Steps : 32900, \t Total Gen Loss : 21.37244987487793, \t Total Dis Loss : 0.005278810393065214\n",
      "Steps : 33000, \t Total Gen Loss : 20.958969116210938, \t Total Dis Loss : 0.004142876714468002\n",
      "Steps : 33100, \t Total Gen Loss : 21.716747283935547, \t Total Dis Loss : 0.0006790902116335928\n",
      "Steps : 33200, \t Total Gen Loss : 21.992387771606445, \t Total Dis Loss : 0.0003259498334955424\n",
      "Steps : 33300, \t Total Gen Loss : 23.073089599609375, \t Total Dis Loss : 0.00027423587744124234\n",
      "Steps : 33400, \t Total Gen Loss : 23.221450805664062, \t Total Dis Loss : 0.0005283441860228777\n",
      "Steps : 33500, \t Total Gen Loss : 25.69713592529297, \t Total Dis Loss : 0.0002578983549028635\n",
      "Steps : 33600, \t Total Gen Loss : 21.016355514526367, \t Total Dis Loss : 0.0006884007598273456\n",
      "Steps : 33700, \t Total Gen Loss : 20.90631103515625, \t Total Dis Loss : 0.0010086209513247013\n",
      "Time for epoch 6 is 68.94639468193054 sec\n",
      "Steps : 33800, \t Total Gen Loss : 25.922752380371094, \t Total Dis Loss : 0.0003664002288132906\n",
      "Steps : 33900, \t Total Gen Loss : 27.042587280273438, \t Total Dis Loss : 0.000443513214122504\n",
      "Steps : 34000, \t Total Gen Loss : 24.26190185546875, \t Total Dis Loss : 0.00020249554654583335\n",
      "Steps : 34100, \t Total Gen Loss : 28.7908935546875, \t Total Dis Loss : 0.0001773441763361916\n",
      "Steps : 34200, \t Total Gen Loss : 25.53035545349121, \t Total Dis Loss : 0.0002859334053937346\n",
      "Steps : 34300, \t Total Gen Loss : 23.58460235595703, \t Total Dis Loss : 0.0007006482337601483\n",
      "Steps : 34400, \t Total Gen Loss : 24.211898803710938, \t Total Dis Loss : 0.00015767986769787967\n",
      "Steps : 34500, \t Total Gen Loss : 23.935462951660156, \t Total Dis Loss : 0.0003626595716923475\n",
      "Steps : 34600, \t Total Gen Loss : 22.593421936035156, \t Total Dis Loss : 0.00045040788245387375\n",
      "Steps : 34700, \t Total Gen Loss : 21.048336029052734, \t Total Dis Loss : 0.0016387210926041007\n",
      "Steps : 34800, \t Total Gen Loss : 24.14380645751953, \t Total Dis Loss : 0.0006276873173192143\n",
      "Steps : 34900, \t Total Gen Loss : 24.244482040405273, \t Total Dis Loss : 0.0021472338121384382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35000, \t Total Gen Loss : 24.799808502197266, \t Total Dis Loss : 0.0027087824419140816\n",
      "Steps : 35100, \t Total Gen Loss : 27.36859130859375, \t Total Dis Loss : 0.00040733569767326117\n",
      "Steps : 35200, \t Total Gen Loss : 27.330171585083008, \t Total Dis Loss : 0.00026001123478636146\n",
      "Steps : 35300, \t Total Gen Loss : 24.631778717041016, \t Total Dis Loss : 0.00046615995233878493\n",
      "Steps : 35400, \t Total Gen Loss : 23.750324249267578, \t Total Dis Loss : 0.0016940662171691656\n",
      "Steps : 35500, \t Total Gen Loss : 23.381420135498047, \t Total Dis Loss : 0.001331103267148137\n",
      "Steps : 35600, \t Total Gen Loss : 20.548086166381836, \t Total Dis Loss : 0.0007146018906496465\n",
      "Steps : 35700, \t Total Gen Loss : 20.652931213378906, \t Total Dis Loss : 0.0008237104048021138\n",
      "Steps : 35800, \t Total Gen Loss : 25.780323028564453, \t Total Dis Loss : 0.0006210795254446566\n",
      "Steps : 35900, \t Total Gen Loss : 18.40245819091797, \t Total Dis Loss : 0.0025647771544754505\n",
      "Steps : 36000, \t Total Gen Loss : 22.972461700439453, \t Total Dis Loss : 0.0010250790510326624\n",
      "Steps : 36100, \t Total Gen Loss : 25.042346954345703, \t Total Dis Loss : 0.0005871173925697803\n",
      "Steps : 36200, \t Total Gen Loss : 23.398006439208984, \t Total Dis Loss : 0.000542849360499531\n",
      "Steps : 36300, \t Total Gen Loss : 22.296171188354492, \t Total Dis Loss : 0.0008561952854506671\n",
      "Steps : 36400, \t Total Gen Loss : 23.387155532836914, \t Total Dis Loss : 0.0010792298708111048\n",
      "Steps : 36500, \t Total Gen Loss : 29.58263397216797, \t Total Dis Loss : 0.002317235805094242\n",
      "Steps : 36600, \t Total Gen Loss : 26.870548248291016, \t Total Dis Loss : 0.00022203278786037117\n",
      "Steps : 36700, \t Total Gen Loss : 28.820575714111328, \t Total Dis Loss : 0.00045472150668501854\n",
      "Steps : 36800, \t Total Gen Loss : 25.079710006713867, \t Total Dis Loss : 0.000267951691057533\n",
      "Steps : 36900, \t Total Gen Loss : 23.214475631713867, \t Total Dis Loss : 0.0005211835377849638\n",
      "Steps : 37000, \t Total Gen Loss : 30.06252670288086, \t Total Dis Loss : 0.0003007690829690546\n",
      "Steps : 37100, \t Total Gen Loss : 26.77916145324707, \t Total Dis Loss : 0.0009843234438449144\n",
      "Steps : 37200, \t Total Gen Loss : 30.132949829101562, \t Total Dis Loss : 0.0013193374034017324\n",
      "Steps : 37300, \t Total Gen Loss : 23.894840240478516, \t Total Dis Loss : 0.0009867342887446284\n",
      "Steps : 37400, \t Total Gen Loss : 30.122255325317383, \t Total Dis Loss : 0.0005732981953769922\n",
      "Steps : 37500, \t Total Gen Loss : 29.561729431152344, \t Total Dis Loss : 0.0016508555272594094\n",
      "Steps : 37600, \t Total Gen Loss : 30.30008888244629, \t Total Dis Loss : 0.0011643200414255261\n",
      "Steps : 37700, \t Total Gen Loss : 27.61131477355957, \t Total Dis Loss : 0.0006172666326165199\n",
      "Steps : 37800, \t Total Gen Loss : 26.881099700927734, \t Total Dis Loss : 0.0007658707909286022\n",
      "Steps : 37900, \t Total Gen Loss : 27.572525024414062, \t Total Dis Loss : 0.0034314210060983896\n",
      "Steps : 38000, \t Total Gen Loss : 28.133499145507812, \t Total Dis Loss : 0.0007622145349159837\n",
      "Steps : 38100, \t Total Gen Loss : 26.18721580505371, \t Total Dis Loss : 0.003875530092045665\n",
      "Steps : 38200, \t Total Gen Loss : 24.16314697265625, \t Total Dis Loss : 0.000681924750097096\n",
      "Steps : 38300, \t Total Gen Loss : 28.541166305541992, \t Total Dis Loss : 0.0008287650416605175\n",
      "Steps : 38400, \t Total Gen Loss : 24.80043601989746, \t Total Dis Loss : 0.00026579038240015507\n",
      "Steps : 38500, \t Total Gen Loss : 25.259342193603516, \t Total Dis Loss : 0.0005216011195443571\n",
      "Steps : 38600, \t Total Gen Loss : 24.244873046875, \t Total Dis Loss : 0.00040390383219346404\n",
      "Steps : 38700, \t Total Gen Loss : 26.032291412353516, \t Total Dis Loss : 0.0006335717043839395\n",
      "Steps : 38800, \t Total Gen Loss : 23.532379150390625, \t Total Dis Loss : 0.0006151015404611826\n",
      "Steps : 38900, \t Total Gen Loss : 27.786174774169922, \t Total Dis Loss : 0.0004407422093208879\n",
      "Steps : 39000, \t Total Gen Loss : 23.68189239501953, \t Total Dis Loss : 0.00043997899047099054\n",
      "Steps : 39100, \t Total Gen Loss : 22.772972106933594, \t Total Dis Loss : 0.0015141931362450123\n",
      "Steps : 39200, \t Total Gen Loss : 25.078853607177734, \t Total Dis Loss : 0.0007575671188533306\n",
      "Steps : 39300, \t Total Gen Loss : 24.324939727783203, \t Total Dis Loss : 0.00042238773312419653\n",
      "Time for epoch 7 is 68.96224808692932 sec\n",
      "Steps : 39400, \t Total Gen Loss : 25.99605941772461, \t Total Dis Loss : 0.0021686533000320196\n",
      "Steps : 39500, \t Total Gen Loss : 20.277851104736328, \t Total Dis Loss : 0.0035256429109722376\n",
      "Steps : 39600, \t Total Gen Loss : 22.609783172607422, \t Total Dis Loss : 0.0006407927721738815\n",
      "Steps : 39700, \t Total Gen Loss : 20.52005386352539, \t Total Dis Loss : 0.0006975627038627863\n",
      "Steps : 39800, \t Total Gen Loss : 24.433940887451172, \t Total Dis Loss : 0.0004628280585166067\n",
      "Steps : 39900, \t Total Gen Loss : 23.902973175048828, \t Total Dis Loss : 0.0004993904731236398\n",
      "Steps : 40000, \t Total Gen Loss : 28.411848068237305, \t Total Dis Loss : 0.00032230617944151163\n",
      "Steps : 40100, \t Total Gen Loss : 23.33391571044922, \t Total Dis Loss : 0.0004249079793225974\n",
      "Steps : 40200, \t Total Gen Loss : 24.756446838378906, \t Total Dis Loss : 0.00021384109277278185\n",
      "Steps : 40300, \t Total Gen Loss : 24.578609466552734, \t Total Dis Loss : 0.0005071516497991979\n",
      "Steps : 40400, \t Total Gen Loss : 24.970476150512695, \t Total Dis Loss : 0.00028947496321052313\n",
      "Steps : 40500, \t Total Gen Loss : 23.8841552734375, \t Total Dis Loss : 0.00013900076737627387\n",
      "Steps : 40600, \t Total Gen Loss : 25.216907501220703, \t Total Dis Loss : 0.0001832574198488146\n",
      "Steps : 40700, \t Total Gen Loss : 25.202163696289062, \t Total Dis Loss : 0.00011288397217867896\n",
      "Steps : 40800, \t Total Gen Loss : 25.417705535888672, \t Total Dis Loss : 9.040070290211588e-05\n",
      "Steps : 40900, \t Total Gen Loss : 25.937742233276367, \t Total Dis Loss : 0.0001347501529380679\n",
      "Steps : 41000, \t Total Gen Loss : 24.931594848632812, \t Total Dis Loss : 9.881710138870403e-05\n",
      "Steps : 41100, \t Total Gen Loss : 25.066518783569336, \t Total Dis Loss : 0.00011911248293472454\n",
      "Steps : 41200, \t Total Gen Loss : 29.312435150146484, \t Total Dis Loss : 0.0002325749082956463\n",
      "Steps : 41300, \t Total Gen Loss : 27.355003356933594, \t Total Dis Loss : 7.442075730068609e-05\n",
      "Steps : 41400, \t Total Gen Loss : 26.262378692626953, \t Total Dis Loss : 6.634434248553589e-05\n",
      "Steps : 41500, \t Total Gen Loss : 29.23967170715332, \t Total Dis Loss : 7.397008448606357e-05\n",
      "Steps : 41600, \t Total Gen Loss : 26.869709014892578, \t Total Dis Loss : 9.316900104749948e-05\n",
      "Steps : 41700, \t Total Gen Loss : 25.13092613220215, \t Total Dis Loss : 0.00015940642333589494\n",
      "Steps : 41800, \t Total Gen Loss : 28.250747680664062, \t Total Dis Loss : 0.040140777826309204\n",
      "Steps : 41900, \t Total Gen Loss : 25.85107421875, \t Total Dis Loss : 0.0003261423553340137\n",
      "Steps : 42000, \t Total Gen Loss : 25.338956832885742, \t Total Dis Loss : 0.00019601982785388827\n",
      "Steps : 42100, \t Total Gen Loss : 25.23979377746582, \t Total Dis Loss : 0.00039843644481152296\n",
      "Steps : 42200, \t Total Gen Loss : 24.855791091918945, \t Total Dis Loss : 0.00022565275139641017\n",
      "Steps : 42300, \t Total Gen Loss : 24.677413940429688, \t Total Dis Loss : 0.00018072492093779147\n",
      "Steps : 42400, \t Total Gen Loss : 22.55463218688965, \t Total Dis Loss : 0.001436809659935534\n",
      "Steps : 42500, \t Total Gen Loss : 24.04473114013672, \t Total Dis Loss : 0.0002728019026108086\n",
      "Steps : 42600, \t Total Gen Loss : 25.159751892089844, \t Total Dis Loss : 0.00016050352132879198\n",
      "Steps : 42700, \t Total Gen Loss : 24.248207092285156, \t Total Dis Loss : 0.0004086690314579755\n",
      "Steps : 42800, \t Total Gen Loss : 25.690073013305664, \t Total Dis Loss : 0.00021229355479590595\n",
      "Steps : 42900, \t Total Gen Loss : 25.259666442871094, \t Total Dis Loss : 0.00020060523820575327\n",
      "Steps : 43000, \t Total Gen Loss : 22.649131774902344, \t Total Dis Loss : 0.00036519416607916355\n",
      "Steps : 43100, \t Total Gen Loss : 24.78832244873047, \t Total Dis Loss : 0.00041785769280977547\n",
      "Steps : 43200, \t Total Gen Loss : 24.893049240112305, \t Total Dis Loss : 0.007457515690475702\n",
      "Steps : 43300, \t Total Gen Loss : 23.47161293029785, \t Total Dis Loss : 0.001404962153173983\n",
      "Steps : 43400, \t Total Gen Loss : 23.680397033691406, \t Total Dis Loss : 0.00026333320420235395\n",
      "Steps : 43500, \t Total Gen Loss : 21.603485107421875, \t Total Dis Loss : 0.00026090542087331414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43600, \t Total Gen Loss : 25.79257583618164, \t Total Dis Loss : 8.082261774688959e-05\n",
      "Steps : 43700, \t Total Gen Loss : 23.542339324951172, \t Total Dis Loss : 0.00024082321033347398\n",
      "Steps : 43800, \t Total Gen Loss : 21.555315017700195, \t Total Dis Loss : 0.00043093872955068946\n",
      "Steps : 43900, \t Total Gen Loss : 28.156862258911133, \t Total Dis Loss : 0.00017031459719873965\n",
      "Steps : 44000, \t Total Gen Loss : 25.420541763305664, \t Total Dis Loss : 0.0001536016061436385\n",
      "Steps : 44100, \t Total Gen Loss : 27.09189796447754, \t Total Dis Loss : 9.833676449488848e-05\n",
      "Steps : 44200, \t Total Gen Loss : 25.383277893066406, \t Total Dis Loss : 0.00015597883611917496\n",
      "Steps : 44300, \t Total Gen Loss : 25.180959701538086, \t Total Dis Loss : 0.00012869543570559472\n",
      "Steps : 44400, \t Total Gen Loss : 24.981863021850586, \t Total Dis Loss : 0.0005701914196833968\n",
      "Steps : 44500, \t Total Gen Loss : 26.834636688232422, \t Total Dis Loss : 0.0004013697907794267\n",
      "Steps : 44600, \t Total Gen Loss : 24.889659881591797, \t Total Dis Loss : 0.00016316484834533185\n",
      "Steps : 44700, \t Total Gen Loss : 21.653053283691406, \t Total Dis Loss : 0.0001534699840703979\n",
      "Steps : 44800, \t Total Gen Loss : 24.536264419555664, \t Total Dis Loss : 0.00010817634029081091\n",
      "Steps : 44900, \t Total Gen Loss : 27.236583709716797, \t Total Dis Loss : 0.00015354582865256816\n",
      "Steps : 45000, \t Total Gen Loss : 22.72440528869629, \t Total Dis Loss : 0.0013114818139001727\n",
      "Time for epoch 8 is 69.43853831291199 sec\n",
      "Steps : 45100, \t Total Gen Loss : 23.476713180541992, \t Total Dis Loss : 0.0057051233015954494\n",
      "Steps : 45200, \t Total Gen Loss : 27.604507446289062, \t Total Dis Loss : 9.588342800270766e-05\n",
      "Steps : 45300, \t Total Gen Loss : 27.762893676757812, \t Total Dis Loss : 4.918920967611484e-05\n",
      "Steps : 45400, \t Total Gen Loss : 27.721895217895508, \t Total Dis Loss : 3.700606248457916e-05\n",
      "Steps : 45500, \t Total Gen Loss : 25.610855102539062, \t Total Dis Loss : 5.479357059812173e-05\n",
      "Steps : 45600, \t Total Gen Loss : 23.920337677001953, \t Total Dis Loss : 0.00024001122801564634\n",
      "Steps : 45700, \t Total Gen Loss : 27.50796127319336, \t Total Dis Loss : 0.00010352126992074773\n",
      "Steps : 45800, \t Total Gen Loss : 29.454307556152344, \t Total Dis Loss : 0.0002281868946738541\n",
      "Steps : 45900, \t Total Gen Loss : 27.900480270385742, \t Total Dis Loss : 0.0016616986831650138\n",
      "Steps : 46000, \t Total Gen Loss : 26.261280059814453, \t Total Dis Loss : 0.0008220691233873367\n",
      "Steps : 46100, \t Total Gen Loss : 26.006380081176758, \t Total Dis Loss : 0.0007900322671048343\n",
      "Steps : 46200, \t Total Gen Loss : 26.452922821044922, \t Total Dis Loss : 0.00019271313794888556\n",
      "Steps : 46300, \t Total Gen Loss : 29.39264678955078, \t Total Dis Loss : 0.000105709106719587\n",
      "Steps : 46400, \t Total Gen Loss : 28.169776916503906, \t Total Dis Loss : 9.642522491049021e-05\n",
      "Steps : 46500, \t Total Gen Loss : 23.662551879882812, \t Total Dis Loss : 0.0002021603286266327\n",
      "Steps : 46600, \t Total Gen Loss : 24.59503746032715, \t Total Dis Loss : 0.0004978904034942389\n",
      "Steps : 46700, \t Total Gen Loss : 26.30022621154785, \t Total Dis Loss : 0.00021292576275300235\n",
      "Steps : 46800, \t Total Gen Loss : 25.313886642456055, \t Total Dis Loss : 0.00022984683164395392\n",
      "Steps : 46900, \t Total Gen Loss : 25.031221389770508, \t Total Dis Loss : 0.00016745663015171885\n",
      "Steps : 47000, \t Total Gen Loss : 23.005603790283203, \t Total Dis Loss : 0.00025872373953461647\n",
      "Steps : 47100, \t Total Gen Loss : 25.789806365966797, \t Total Dis Loss : 0.0002502895949874073\n",
      "Steps : 47200, \t Total Gen Loss : 24.79254150390625, \t Total Dis Loss : 0.00023145857267081738\n",
      "Steps : 47300, \t Total Gen Loss : 26.42910385131836, \t Total Dis Loss : 0.00012772652553394437\n",
      "Steps : 47400, \t Total Gen Loss : 25.075023651123047, \t Total Dis Loss : 0.0001449789706384763\n",
      "Steps : 47500, \t Total Gen Loss : 23.40224266052246, \t Total Dis Loss : 0.00010043213842436671\n",
      "Steps : 47600, \t Total Gen Loss : 27.428977966308594, \t Total Dis Loss : 0.00010067234688904136\n",
      "Steps : 47700, \t Total Gen Loss : 25.04228973388672, \t Total Dis Loss : 0.0002832279133144766\n",
      "Steps : 47800, \t Total Gen Loss : 26.855485916137695, \t Total Dis Loss : 7.848948007449508e-05\n",
      "Steps : 47900, \t Total Gen Loss : 24.79140853881836, \t Total Dis Loss : 0.0001245989406015724\n",
      "Steps : 48000, \t Total Gen Loss : 25.337299346923828, \t Total Dis Loss : 0.00018421909771859646\n",
      "Steps : 48100, \t Total Gen Loss : 24.70246124267578, \t Total Dis Loss : 0.000369860150385648\n",
      "Steps : 48200, \t Total Gen Loss : 24.077529907226562, \t Total Dis Loss : 8.956086094258353e-05\n",
      "Steps : 48300, \t Total Gen Loss : 23.845687866210938, \t Total Dis Loss : 0.00022248757886700332\n",
      "Steps : 48400, \t Total Gen Loss : 23.19338607788086, \t Total Dis Loss : 0.002359003759920597\n",
      "Steps : 48500, \t Total Gen Loss : 22.784095764160156, \t Total Dis Loss : 0.00027277498156763613\n",
      "Steps : 48600, \t Total Gen Loss : 22.74350929260254, \t Total Dis Loss : 0.002587700728327036\n",
      "Steps : 48700, \t Total Gen Loss : 26.653823852539062, \t Total Dis Loss : 0.02019093930721283\n",
      "Steps : 48800, \t Total Gen Loss : 24.055988311767578, \t Total Dis Loss : 0.023584242910146713\n",
      "Steps : 48900, \t Total Gen Loss : 24.978126525878906, \t Total Dis Loss : 0.002004732144996524\n",
      "Steps : 49000, \t Total Gen Loss : 24.599258422851562, \t Total Dis Loss : 0.0006345580914057791\n",
      "Steps : 49100, \t Total Gen Loss : 23.216516494750977, \t Total Dis Loss : 0.0035899223294109106\n",
      "Steps : 49200, \t Total Gen Loss : 22.22136116027832, \t Total Dis Loss : 0.001385215320624411\n",
      "Steps : 49300, \t Total Gen Loss : 22.333086013793945, \t Total Dis Loss : 0.48229143023490906\n",
      "Steps : 49400, \t Total Gen Loss : 22.24093246459961, \t Total Dis Loss : 0.0009412727085873485\n",
      "Steps : 49500, \t Total Gen Loss : 26.135635375976562, \t Total Dis Loss : 0.0005685393698513508\n",
      "Steps : 49600, \t Total Gen Loss : 22.646434783935547, \t Total Dis Loss : 0.0010995038319379091\n",
      "Steps : 49700, \t Total Gen Loss : 21.11880111694336, \t Total Dis Loss : 0.0015859795967116952\n",
      "Steps : 49800, \t Total Gen Loss : 23.902374267578125, \t Total Dis Loss : 0.0008929336327128112\n",
      "Steps : 49900, \t Total Gen Loss : 23.916080474853516, \t Total Dis Loss : 0.00023378789774142206\n",
      "Steps : 50000, \t Total Gen Loss : 24.324419021606445, \t Total Dis Loss : 0.00014468995505012572\n",
      "Steps : 50100, \t Total Gen Loss : 26.519134521484375, \t Total Dis Loss : 0.00019267341122031212\n",
      "Steps : 50200, \t Total Gen Loss : 21.579498291015625, \t Total Dis Loss : 0.0011811506701633334\n",
      "Steps : 50300, \t Total Gen Loss : 21.539344787597656, \t Total Dis Loss : 0.000644221727270633\n",
      "Steps : 50400, \t Total Gen Loss : 21.654674530029297, \t Total Dis Loss : 0.0007825656211934984\n",
      "Steps : 50500, \t Total Gen Loss : 29.177005767822266, \t Total Dis Loss : 0.0006531253457069397\n",
      "Steps : 50600, \t Total Gen Loss : 23.438358306884766, \t Total Dis Loss : 0.0005972011131234467\n",
      "Time for epoch 9 is 68.98020386695862 sec\n",
      "Steps : 50700, \t Total Gen Loss : 27.33670997619629, \t Total Dis Loss : 0.0003399337292648852\n",
      "Steps : 50800, \t Total Gen Loss : 21.495784759521484, \t Total Dis Loss : 0.0007570823654532433\n",
      "Steps : 50900, \t Total Gen Loss : 24.68109893798828, \t Total Dis Loss : 0.0009501006570644677\n",
      "Steps : 51000, \t Total Gen Loss : 24.296035766601562, \t Total Dis Loss : 0.0008662139880470932\n",
      "Steps : 51100, \t Total Gen Loss : 26.26654052734375, \t Total Dis Loss : 0.0009243596578016877\n",
      "Steps : 51200, \t Total Gen Loss : 22.277921676635742, \t Total Dis Loss : 0.0016868642996996641\n",
      "Steps : 51300, \t Total Gen Loss : 27.51508331298828, \t Total Dis Loss : 0.0004935071337968111\n",
      "Steps : 51400, \t Total Gen Loss : 25.052051544189453, \t Total Dis Loss : 0.0002491655759513378\n",
      "Steps : 51500, \t Total Gen Loss : 26.411235809326172, \t Total Dis Loss : 0.0002779181522782892\n",
      "Steps : 51600, \t Total Gen Loss : 23.84961700439453, \t Total Dis Loss : 0.00038095752825029194\n",
      "Steps : 51700, \t Total Gen Loss : 21.0390625, \t Total Dis Loss : 0.0033951906953006983\n",
      "Steps : 51800, \t Total Gen Loss : 21.97365951538086, \t Total Dis Loss : 0.0020074457861483097\n",
      "Steps : 51900, \t Total Gen Loss : 22.878158569335938, \t Total Dis Loss : 0.0007236763485707343\n",
      "Steps : 52000, \t Total Gen Loss : 24.52835464477539, \t Total Dis Loss : 0.00018414118676446378\n",
      "Steps : 52100, \t Total Gen Loss : 24.004838943481445, \t Total Dis Loss : 0.0005124170565977693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52200, \t Total Gen Loss : 23.45004653930664, \t Total Dis Loss : 0.0017936236690729856\n",
      "Steps : 52300, \t Total Gen Loss : 24.556236267089844, \t Total Dis Loss : 0.0002952822542283684\n",
      "Steps : 52400, \t Total Gen Loss : 25.95597267150879, \t Total Dis Loss : 0.0010531081352382898\n",
      "Steps : 52500, \t Total Gen Loss : 23.683639526367188, \t Total Dis Loss : 0.0007825231296010315\n",
      "Steps : 52600, \t Total Gen Loss : 27.7427921295166, \t Total Dis Loss : 0.00024020041746553034\n",
      "Steps : 52700, \t Total Gen Loss : 23.44312286376953, \t Total Dis Loss : 0.0013536206679418683\n",
      "Steps : 52800, \t Total Gen Loss : 24.933977127075195, \t Total Dis Loss : 0.0002524067531339824\n",
      "Steps : 52900, \t Total Gen Loss : 23.627147674560547, \t Total Dis Loss : 0.00030611446709372103\n",
      "Steps : 53000, \t Total Gen Loss : 25.91587257385254, \t Total Dis Loss : 0.00040553859435021877\n",
      "Steps : 53100, \t Total Gen Loss : 25.085918426513672, \t Total Dis Loss : 0.0008336673490703106\n",
      "Steps : 53200, \t Total Gen Loss : 23.824604034423828, \t Total Dis Loss : 0.00048189592780545354\n",
      "Steps : 53300, \t Total Gen Loss : 30.090587615966797, \t Total Dis Loss : 0.00012493749090936035\n",
      "Steps : 53400, \t Total Gen Loss : 28.526891708374023, \t Total Dis Loss : 8.742835780140013e-05\n",
      "Steps : 53500, \t Total Gen Loss : 25.244199752807617, \t Total Dis Loss : 0.0002492378989700228\n",
      "Steps : 53600, \t Total Gen Loss : 21.68693733215332, \t Total Dis Loss : 0.0005406007985584438\n",
      "Steps : 53700, \t Total Gen Loss : 25.769386291503906, \t Total Dis Loss : 0.0001238360273418948\n",
      "Steps : 53800, \t Total Gen Loss : 24.161314010620117, \t Total Dis Loss : 0.00018002346041612327\n",
      "Steps : 53900, \t Total Gen Loss : 25.28308868408203, \t Total Dis Loss : 0.0002992286754306406\n",
      "Steps : 54000, \t Total Gen Loss : 24.234689712524414, \t Total Dis Loss : 0.00014568436017725617\n",
      "Steps : 54100, \t Total Gen Loss : 23.457355499267578, \t Total Dis Loss : 0.00018210354028269649\n",
      "Steps : 54200, \t Total Gen Loss : 23.679706573486328, \t Total Dis Loss : 0.00017414023750461638\n",
      "Steps : 54300, \t Total Gen Loss : 24.3670597076416, \t Total Dis Loss : 0.0005962878931313753\n",
      "Steps : 54400, \t Total Gen Loss : 25.816104888916016, \t Total Dis Loss : 0.0006191864376887679\n",
      "Steps : 54500, \t Total Gen Loss : 25.684898376464844, \t Total Dis Loss : 0.0003004520258400589\n",
      "Steps : 54600, \t Total Gen Loss : 25.658058166503906, \t Total Dis Loss : 0.00027500465512275696\n",
      "Steps : 54700, \t Total Gen Loss : 26.11111831665039, \t Total Dis Loss : 6.0149035562062636e-05\n",
      "Steps : 54800, \t Total Gen Loss : 28.445396423339844, \t Total Dis Loss : 6.906762428116053e-05\n",
      "Steps : 54900, \t Total Gen Loss : 25.176132202148438, \t Total Dis Loss : 0.00010204607679042965\n",
      "Steps : 55000, \t Total Gen Loss : 24.251222610473633, \t Total Dis Loss : 0.00020467567082960159\n",
      "Steps : 55100, \t Total Gen Loss : 23.59880828857422, \t Total Dis Loss : 5.6366538046859205e-05\n",
      "Steps : 55200, \t Total Gen Loss : 27.185291290283203, \t Total Dis Loss : 0.001753309741616249\n",
      "Steps : 55300, \t Total Gen Loss : 27.336254119873047, \t Total Dis Loss : 9.363668505102396e-05\n",
      "Steps : 55400, \t Total Gen Loss : 24.159584045410156, \t Total Dis Loss : 0.0002518545661587268\n",
      "Steps : 55500, \t Total Gen Loss : 30.23873519897461, \t Total Dis Loss : 0.00011096488742623478\n",
      "Steps : 55600, \t Total Gen Loss : 25.50768280029297, \t Total Dis Loss : 0.00013154208136256784\n",
      "Steps : 55700, \t Total Gen Loss : 25.925189971923828, \t Total Dis Loss : 0.00011642035678960383\n",
      "Steps : 55800, \t Total Gen Loss : 26.736459732055664, \t Total Dis Loss : 0.00020125093578826636\n",
      "Steps : 55900, \t Total Gen Loss : 24.88719367980957, \t Total Dis Loss : 0.018490849062800407\n",
      "Steps : 56000, \t Total Gen Loss : 26.0406494140625, \t Total Dis Loss : 0.0003578489413484931\n",
      "Steps : 56100, \t Total Gen Loss : 27.679641723632812, \t Total Dis Loss : 0.00018312247993890196\n",
      "Steps : 56200, \t Total Gen Loss : 22.91461181640625, \t Total Dis Loss : 0.002026049653068185\n",
      "Time for epoch 10 is 69.0806713104248 sec\n",
      "Steps : 56300, \t Total Gen Loss : 26.0825252532959, \t Total Dis Loss : 0.000173996711964719\n",
      "Steps : 56400, \t Total Gen Loss : 24.26469612121582, \t Total Dis Loss : 0.0001034580054692924\n",
      "Steps : 56500, \t Total Gen Loss : 25.981311798095703, \t Total Dis Loss : 5.3288247727323323e-05\n",
      "Steps : 56600, \t Total Gen Loss : 24.515487670898438, \t Total Dis Loss : 0.0006299284868873656\n",
      "Steps : 56700, \t Total Gen Loss : 24.86315155029297, \t Total Dis Loss : 0.0003073851694352925\n",
      "Steps : 56800, \t Total Gen Loss : 26.95130157470703, \t Total Dis Loss : 4.066786277689971e-05\n",
      "Steps : 56900, \t Total Gen Loss : 27.297950744628906, \t Total Dis Loss : 0.008908510208129883\n",
      "Steps : 57000, \t Total Gen Loss : 33.811683654785156, \t Total Dis Loss : 0.0015077772550284863\n",
      "Steps : 57100, \t Total Gen Loss : 29.268890380859375, \t Total Dis Loss : 0.0005681717884726822\n",
      "Steps : 57200, \t Total Gen Loss : 26.592174530029297, \t Total Dis Loss : 0.00022605754202231765\n",
      "Steps : 57300, \t Total Gen Loss : 28.19901466369629, \t Total Dis Loss : 0.0003665831172838807\n",
      "Steps : 57400, \t Total Gen Loss : 24.872087478637695, \t Total Dis Loss : 0.00024254442541860044\n",
      "Steps : 57500, \t Total Gen Loss : 27.86285400390625, \t Total Dis Loss : 0.0004025661910418421\n",
      "Steps : 57600, \t Total Gen Loss : 24.180265426635742, \t Total Dis Loss : 0.00015605820226483047\n",
      "Steps : 57700, \t Total Gen Loss : 26.797975540161133, \t Total Dis Loss : 0.00012434810923878103\n",
      "Steps : 57800, \t Total Gen Loss : 27.37731170654297, \t Total Dis Loss : 9.157121530734003e-05\n",
      "Steps : 57900, \t Total Gen Loss : 26.751083374023438, \t Total Dis Loss : 0.001143209869042039\n",
      "Steps : 58000, \t Total Gen Loss : 23.171451568603516, \t Total Dis Loss : 0.0007799314917065203\n",
      "Steps : 58100, \t Total Gen Loss : 28.340778350830078, \t Total Dis Loss : 0.0003196906764060259\n",
      "Steps : 58200, \t Total Gen Loss : 21.991390228271484, \t Total Dis Loss : 0.0007743992027826607\n",
      "Steps : 58300, \t Total Gen Loss : 23.100997924804688, \t Total Dis Loss : 0.0008607545751146972\n",
      "Steps : 58400, \t Total Gen Loss : 24.82799530029297, \t Total Dis Loss : 0.0004143731785006821\n",
      "Steps : 58500, \t Total Gen Loss : 26.74895477294922, \t Total Dis Loss : 0.00023746753868181258\n",
      "Steps : 58600, \t Total Gen Loss : 21.996143341064453, \t Total Dis Loss : 0.0008312860154546797\n",
      "Steps : 58700, \t Total Gen Loss : 21.868255615234375, \t Total Dis Loss : 0.0005857183714397252\n",
      "Steps : 58800, \t Total Gen Loss : 24.35396385192871, \t Total Dis Loss : 0.00015288882423192263\n",
      "Steps : 58900, \t Total Gen Loss : 26.018295288085938, \t Total Dis Loss : 0.0007216341327875853\n",
      "Steps : 59000, \t Total Gen Loss : 25.830856323242188, \t Total Dis Loss : 0.0012541342293843627\n",
      "Steps : 59100, \t Total Gen Loss : 23.859249114990234, \t Total Dis Loss : 0.00013124998076818883\n",
      "Steps : 59200, \t Total Gen Loss : 23.606735229492188, \t Total Dis Loss : 0.0006311548058874905\n",
      "Steps : 59300, \t Total Gen Loss : 21.733570098876953, \t Total Dis Loss : 0.0012638014741241932\n",
      "Steps : 59400, \t Total Gen Loss : 25.193614959716797, \t Total Dis Loss : 0.0001270318462047726\n",
      "Steps : 59500, \t Total Gen Loss : 25.82781982421875, \t Total Dis Loss : 0.00018192929564975202\n",
      "Steps : 59600, \t Total Gen Loss : 23.98784637451172, \t Total Dis Loss : 0.00044963572872802615\n",
      "Steps : 59700, \t Total Gen Loss : 24.361125946044922, \t Total Dis Loss : 0.00034630607115104795\n",
      "Steps : 59800, \t Total Gen Loss : 23.116924285888672, \t Total Dis Loss : 0.00019707523460965604\n",
      "Steps : 59900, \t Total Gen Loss : 24.813756942749023, \t Total Dis Loss : 0.00028696696972474456\n",
      "Steps : 60000, \t Total Gen Loss : 25.545209884643555, \t Total Dis Loss : 0.00014328780525829643\n",
      "Steps : 60100, \t Total Gen Loss : 25.870208740234375, \t Total Dis Loss : 0.00012420225539244711\n",
      "Steps : 60200, \t Total Gen Loss : 26.71163558959961, \t Total Dis Loss : 0.00044864771189168096\n",
      "Steps : 60300, \t Total Gen Loss : 25.066818237304688, \t Total Dis Loss : 6.84246770106256e-05\n",
      "Steps : 60400, \t Total Gen Loss : 23.509241104125977, \t Total Dis Loss : 0.00020328524988144636\n",
      "Steps : 60500, \t Total Gen Loss : 25.12242889404297, \t Total Dis Loss : 0.0001345734781352803\n",
      "Steps : 60600, \t Total Gen Loss : 25.165681838989258, \t Total Dis Loss : 0.005835445132106543\n",
      "Steps : 60700, \t Total Gen Loss : 25.354442596435547, \t Total Dis Loss : 0.00067679159110412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 60800, \t Total Gen Loss : 22.237974166870117, \t Total Dis Loss : 0.010370917618274689\n",
      "Steps : 60900, \t Total Gen Loss : 24.544410705566406, \t Total Dis Loss : 0.00033535188413225114\n",
      "Steps : 61000, \t Total Gen Loss : 23.790388107299805, \t Total Dis Loss : 0.0006505047203972936\n",
      "Steps : 61100, \t Total Gen Loss : 22.88066291809082, \t Total Dis Loss : 0.0003725477436091751\n",
      "Steps : 61200, \t Total Gen Loss : 24.766010284423828, \t Total Dis Loss : 0.0004192097985651344\n",
      "Steps : 61300, \t Total Gen Loss : 26.763547897338867, \t Total Dis Loss : 0.0001428571413271129\n",
      "Steps : 61400, \t Total Gen Loss : 27.224483489990234, \t Total Dis Loss : 0.00015292223542928696\n",
      "Steps : 61500, \t Total Gen Loss : 24.01025390625, \t Total Dis Loss : 0.0002114741801051423\n",
      "Steps : 61600, \t Total Gen Loss : 23.118764877319336, \t Total Dis Loss : 0.0012941784225404263\n",
      "Steps : 61700, \t Total Gen Loss : 24.415578842163086, \t Total Dis Loss : 0.0004097276250831783\n",
      "Steps : 61800, \t Total Gen Loss : 22.489898681640625, \t Total Dis Loss : 0.0007846042863093317\n",
      "Time for epoch 11 is 69.32569527626038 sec\n",
      "Steps : 61900, \t Total Gen Loss : 26.493206024169922, \t Total Dis Loss : 0.00012570300896186382\n",
      "Steps : 62000, \t Total Gen Loss : 24.87342071533203, \t Total Dis Loss : 0.00019830200471915305\n",
      "Steps : 62100, \t Total Gen Loss : 25.634193420410156, \t Total Dis Loss : 0.09302107244729996\n",
      "Steps : 62200, \t Total Gen Loss : 21.59537696838379, \t Total Dis Loss : 0.004118476063013077\n",
      "Steps : 62300, \t Total Gen Loss : 22.529388427734375, \t Total Dis Loss : 0.025315243750810623\n",
      "Steps : 62400, \t Total Gen Loss : 22.769508361816406, \t Total Dis Loss : 0.0004687538603320718\n",
      "Steps : 62500, \t Total Gen Loss : 23.151611328125, \t Total Dis Loss : 0.0003470268566161394\n",
      "Steps : 62600, \t Total Gen Loss : 22.98323631286621, \t Total Dis Loss : 0.004114770796149969\n",
      "Steps : 62700, \t Total Gen Loss : 24.09903907775879, \t Total Dis Loss : 0.021297791972756386\n",
      "Steps : 62800, \t Total Gen Loss : 24.16775894165039, \t Total Dis Loss : 0.0009158643078990281\n",
      "Steps : 62900, \t Total Gen Loss : 22.695035934448242, \t Total Dis Loss : 0.0002212550607509911\n",
      "Steps : 63000, \t Total Gen Loss : 25.453813552856445, \t Total Dis Loss : 0.0004032883734907955\n",
      "Steps : 63100, \t Total Gen Loss : 24.969284057617188, \t Total Dis Loss : 0.0002183938049711287\n",
      "Steps : 63200, \t Total Gen Loss : 23.416645050048828, \t Total Dis Loss : 0.004987570457160473\n",
      "Steps : 63300, \t Total Gen Loss : 24.65370750427246, \t Total Dis Loss : 0.0004484810051508248\n",
      "Steps : 63400, \t Total Gen Loss : 25.513317108154297, \t Total Dis Loss : 0.0003480474988464266\n",
      "Steps : 63500, \t Total Gen Loss : 28.4226131439209, \t Total Dis Loss : 8.082696876954287e-05\n",
      "Steps : 63600, \t Total Gen Loss : 24.077388763427734, \t Total Dis Loss : 0.0002108992193825543\n",
      "Steps : 63700, \t Total Gen Loss : 23.33561897277832, \t Total Dis Loss : 0.000523603695910424\n",
      "Steps : 63800, \t Total Gen Loss : 25.075462341308594, \t Total Dis Loss : 0.00018910561630036682\n",
      "Steps : 63900, \t Total Gen Loss : 23.91654396057129, \t Total Dis Loss : 0.0006150758708827198\n",
      "Steps : 64000, \t Total Gen Loss : 26.488636016845703, \t Total Dis Loss : 0.00019222659466322511\n",
      "Steps : 64100, \t Total Gen Loss : 25.893653869628906, \t Total Dis Loss : 0.00017301994375884533\n",
      "Steps : 64200, \t Total Gen Loss : 26.40033721923828, \t Total Dis Loss : 0.00023494925699196756\n",
      "Steps : 64300, \t Total Gen Loss : 23.64451789855957, \t Total Dis Loss : 0.0003819344856310636\n",
      "Steps : 64400, \t Total Gen Loss : 24.888105392456055, \t Total Dis Loss : 0.0002230091777164489\n",
      "Steps : 64500, \t Total Gen Loss : 25.72734832763672, \t Total Dis Loss : 0.00040363139123655856\n",
      "Steps : 64600, \t Total Gen Loss : 25.943119049072266, \t Total Dis Loss : 6.725035200361162e-05\n",
      "Steps : 64700, \t Total Gen Loss : 21.92706298828125, \t Total Dis Loss : 0.0005943753640167415\n",
      "Steps : 64800, \t Total Gen Loss : 24.069255828857422, \t Total Dis Loss : 8.16826504888013e-05\n",
      "Steps : 64900, \t Total Gen Loss : 26.206985473632812, \t Total Dis Loss : 0.00011053442722186446\n",
      "Steps : 65000, \t Total Gen Loss : 23.726810455322266, \t Total Dis Loss : 0.00012454614625312388\n",
      "Steps : 65100, \t Total Gen Loss : 26.63707733154297, \t Total Dis Loss : 9.075091656995937e-05\n",
      "Steps : 65200, \t Total Gen Loss : 23.551860809326172, \t Total Dis Loss : 0.00011543790606083348\n",
      "Steps : 65300, \t Total Gen Loss : 24.299850463867188, \t Total Dis Loss : 5.8502489991951734e-05\n",
      "Steps : 65400, \t Total Gen Loss : 22.903881072998047, \t Total Dis Loss : 0.00011761371570173651\n",
      "Steps : 65500, \t Total Gen Loss : 25.814271926879883, \t Total Dis Loss : 6.670166476396844e-05\n",
      "Steps : 65600, \t Total Gen Loss : 23.77832794189453, \t Total Dis Loss : 0.00010674387158360332\n",
      "Steps : 65700, \t Total Gen Loss : 24.027610778808594, \t Total Dis Loss : 0.0007992441533133388\n",
      "Steps : 65800, \t Total Gen Loss : 28.554092407226562, \t Total Dis Loss : 0.0001373609120491892\n",
      "Steps : 65900, \t Total Gen Loss : 28.81829261779785, \t Total Dis Loss : 4.153649933869019e-05\n",
      "Steps : 66000, \t Total Gen Loss : 23.898170471191406, \t Total Dis Loss : 0.0006046083872206509\n",
      "Steps : 66100, \t Total Gen Loss : 26.622047424316406, \t Total Dis Loss : 0.00014983394066803157\n",
      "Steps : 66200, \t Total Gen Loss : 26.841838836669922, \t Total Dis Loss : 0.0010455684969201684\n",
      "Steps : 66300, \t Total Gen Loss : 25.824726104736328, \t Total Dis Loss : 0.004036277998238802\n",
      "Steps : 66400, \t Total Gen Loss : 23.17642593383789, \t Total Dis Loss : 0.004793952684849501\n",
      "Steps : 66500, \t Total Gen Loss : 22.415138244628906, \t Total Dis Loss : 0.0003121916379313916\n",
      "Steps : 66600, \t Total Gen Loss : 27.341209411621094, \t Total Dis Loss : 0.00013300770660862327\n",
      "Steps : 66700, \t Total Gen Loss : 22.409679412841797, \t Total Dis Loss : 0.005642001982778311\n",
      "Steps : 66800, \t Total Gen Loss : 26.307960510253906, \t Total Dis Loss : 0.00010426636436022818\n",
      "Steps : 66900, \t Total Gen Loss : 27.84650993347168, \t Total Dis Loss : 0.0002511967904865742\n",
      "Steps : 67000, \t Total Gen Loss : 28.09487533569336, \t Total Dis Loss : 0.0001213105206261389\n",
      "Steps : 67100, \t Total Gen Loss : 22.033817291259766, \t Total Dis Loss : 0.00303194229491055\n",
      "Steps : 67200, \t Total Gen Loss : 22.031301498413086, \t Total Dis Loss : 0.02897735685110092\n",
      "Steps : 67300, \t Total Gen Loss : 25.538768768310547, \t Total Dis Loss : 0.0007034504087641835\n",
      "Steps : 67400, \t Total Gen Loss : 22.282238006591797, \t Total Dis Loss : 0.006148908287286758\n",
      "Steps : 67500, \t Total Gen Loss : 23.299232482910156, \t Total Dis Loss : 0.0035413000732660294\n",
      "Time for epoch 12 is 69.0478196144104 sec\n",
      "Steps : 67600, \t Total Gen Loss : 27.890853881835938, \t Total Dis Loss : 0.0015161273768171668\n",
      "Steps : 67700, \t Total Gen Loss : 25.346572875976562, \t Total Dis Loss : 0.000169035280123353\n",
      "Steps : 67800, \t Total Gen Loss : 24.176010131835938, \t Total Dis Loss : 0.0013009493704885244\n",
      "Steps : 67900, \t Total Gen Loss : 24.47287940979004, \t Total Dis Loss : 0.0045866831205785275\n",
      "Steps : 68000, \t Total Gen Loss : 26.250934600830078, \t Total Dis Loss : 0.0002837554202415049\n",
      "Steps : 68100, \t Total Gen Loss : 25.7586669921875, \t Total Dis Loss : 0.00018618410103954375\n",
      "Steps : 68200, \t Total Gen Loss : 23.681720733642578, \t Total Dis Loss : 0.0006089064409025013\n",
      "Steps : 68300, \t Total Gen Loss : 27.55681800842285, \t Total Dis Loss : 0.0004067549016326666\n",
      "Steps : 68400, \t Total Gen Loss : 26.389549255371094, \t Total Dis Loss : 0.0005218191654421389\n",
      "Steps : 68500, \t Total Gen Loss : 26.02129554748535, \t Total Dis Loss : 0.0009148034732788801\n",
      "Steps : 68600, \t Total Gen Loss : 23.547853469848633, \t Total Dis Loss : 0.00744289718568325\n",
      "Steps : 68700, \t Total Gen Loss : 25.2602596282959, \t Total Dis Loss : 0.0008285809308290482\n",
      "Steps : 68800, \t Total Gen Loss : 25.05849838256836, \t Total Dis Loss : 0.0010561434319242835\n",
      "Steps : 68900, \t Total Gen Loss : 27.14437484741211, \t Total Dis Loss : 0.00021579698659479618\n",
      "Steps : 69000, \t Total Gen Loss : 24.261394500732422, \t Total Dis Loss : 0.0008237061556428671\n",
      "Steps : 69100, \t Total Gen Loss : 23.508953094482422, \t Total Dis Loss : 0.0006188740953803062\n",
      "Steps : 69200, \t Total Gen Loss : 26.89053726196289, \t Total Dis Loss : 0.00015563939814455807\n",
      "Steps : 69300, \t Total Gen Loss : 28.006908416748047, \t Total Dis Loss : 0.00035872909938916564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 69400, \t Total Gen Loss : 26.361732482910156, \t Total Dis Loss : 0.00023561058333143592\n",
      "Steps : 69500, \t Total Gen Loss : 25.56139373779297, \t Total Dis Loss : 0.0001331182720605284\n",
      "Steps : 69600, \t Total Gen Loss : 26.474634170532227, \t Total Dis Loss : 0.0007820759201422334\n",
      "Steps : 69700, \t Total Gen Loss : 24.023942947387695, \t Total Dis Loss : 0.0011013650801032782\n",
      "Steps : 69800, \t Total Gen Loss : 26.430431365966797, \t Total Dis Loss : 0.0007194318459369242\n",
      "Steps : 69900, \t Total Gen Loss : 20.162612915039062, \t Total Dis Loss : 0.0015642691869288683\n",
      "Steps : 70000, \t Total Gen Loss : 23.694599151611328, \t Total Dis Loss : 0.0005635170382447541\n",
      "Steps : 70100, \t Total Gen Loss : 25.570419311523438, \t Total Dis Loss : 0.00016308041813317686\n",
      "Steps : 70200, \t Total Gen Loss : 24.80716323852539, \t Total Dis Loss : 0.00010261996067129076\n",
      "Steps : 70300, \t Total Gen Loss : 24.715240478515625, \t Total Dis Loss : 0.00032617885153740644\n",
      "Steps : 70400, \t Total Gen Loss : 25.85321807861328, \t Total Dis Loss : 0.00020551458874251693\n",
      "Steps : 70500, \t Total Gen Loss : 24.19444465637207, \t Total Dis Loss : 0.00034269338357262313\n",
      "Steps : 70600, \t Total Gen Loss : 24.425907135009766, \t Total Dis Loss : 0.00016279722331091762\n",
      "Steps : 70700, \t Total Gen Loss : 26.065597534179688, \t Total Dis Loss : 8.06002426543273e-05\n",
      "Steps : 70800, \t Total Gen Loss : 28.651052474975586, \t Total Dis Loss : 0.003286735387519002\n",
      "Steps : 70900, \t Total Gen Loss : 23.69087028503418, \t Total Dis Loss : 0.00018049846403300762\n",
      "Steps : 71000, \t Total Gen Loss : 24.151805877685547, \t Total Dis Loss : 9.215797763317823e-05\n",
      "Steps : 71100, \t Total Gen Loss : 27.775386810302734, \t Total Dis Loss : 4.2827487050089985e-05\n",
      "Steps : 71200, \t Total Gen Loss : 28.1754207611084, \t Total Dis Loss : 0.019584087654948235\n",
      "Steps : 71300, \t Total Gen Loss : 24.921878814697266, \t Total Dis Loss : 0.00011349531268933788\n",
      "Steps : 71400, \t Total Gen Loss : 25.31622314453125, \t Total Dis Loss : 0.00011645875929389149\n",
      "Steps : 71500, \t Total Gen Loss : 22.762001037597656, \t Total Dis Loss : 0.0035133911296725273\n",
      "Steps : 71600, \t Total Gen Loss : 25.421302795410156, \t Total Dis Loss : 8.373480523005128e-05\n",
      "Steps : 71700, \t Total Gen Loss : 29.354700088500977, \t Total Dis Loss : 2.8762722649844363e-05\n",
      "Steps : 71800, \t Total Gen Loss : 23.119342803955078, \t Total Dis Loss : 0.00031402744934894145\n",
      "Steps : 71900, \t Total Gen Loss : 26.51025390625, \t Total Dis Loss : 3.0550996598321944e-05\n",
      "Steps : 72000, \t Total Gen Loss : 24.265419006347656, \t Total Dis Loss : 0.0007430710247717798\n",
      "Steps : 72100, \t Total Gen Loss : 23.24457550048828, \t Total Dis Loss : 0.00018199440091848373\n",
      "Steps : 72200, \t Total Gen Loss : 24.60607147216797, \t Total Dis Loss : 5.573895396082662e-05\n",
      "Steps : 72300, \t Total Gen Loss : 30.6711368560791, \t Total Dis Loss : 0.24408183991909027\n",
      "Steps : 72400, \t Total Gen Loss : 35.45458984375, \t Total Dis Loss : 0.0008747994434088469\n",
      "Steps : 72500, \t Total Gen Loss : 27.502967834472656, \t Total Dis Loss : 0.00030108398641459644\n",
      "Steps : 72600, \t Total Gen Loss : 30.380844116210938, \t Total Dis Loss : 0.00016131935990415514\n",
      "Steps : 72700, \t Total Gen Loss : 32.67827606201172, \t Total Dis Loss : 0.00015068725042510778\n",
      "Steps : 72800, \t Total Gen Loss : 29.830190658569336, \t Total Dis Loss : 0.0001388153905281797\n",
      "Steps : 72900, \t Total Gen Loss : 29.770248413085938, \t Total Dis Loss : 0.005814882926642895\n",
      "Steps : 73000, \t Total Gen Loss : 29.8643856048584, \t Total Dis Loss : 0.00020090569159947336\n",
      "Steps : 73100, \t Total Gen Loss : 27.739267349243164, \t Total Dis Loss : 0.00018894075765274465\n",
      "Time for epoch 13 is 68.99118089675903 sec\n",
      "Steps : 73200, \t Total Gen Loss : 28.65412139892578, \t Total Dis Loss : 0.0002624422195367515\n",
      "Steps : 73300, \t Total Gen Loss : 22.477264404296875, \t Total Dis Loss : 0.000120528471597936\n",
      "Steps : 73400, \t Total Gen Loss : 26.79991340637207, \t Total Dis Loss : 0.0017579993000254035\n",
      "Steps : 73500, \t Total Gen Loss : 26.280750274658203, \t Total Dis Loss : 0.00431162491440773\n",
      "Steps : 73600, \t Total Gen Loss : 23.489688873291016, \t Total Dis Loss : 0.00175708148162812\n",
      "Steps : 73700, \t Total Gen Loss : 23.92836570739746, \t Total Dis Loss : 0.0008874483173713088\n",
      "Steps : 73800, \t Total Gen Loss : 24.058319091796875, \t Total Dis Loss : 0.004375250078737736\n",
      "Steps : 73900, \t Total Gen Loss : 28.99099349975586, \t Total Dis Loss : 0.0006899966392666101\n",
      "Steps : 74000, \t Total Gen Loss : 25.171249389648438, \t Total Dis Loss : 0.0018802419072017074\n",
      "Steps : 74100, \t Total Gen Loss : 26.02661895751953, \t Total Dis Loss : 0.00026107742451131344\n",
      "Steps : 74200, \t Total Gen Loss : 23.101844787597656, \t Total Dis Loss : 0.0012553550768643618\n",
      "Steps : 74300, \t Total Gen Loss : 26.507591247558594, \t Total Dis Loss : 0.000949560315348208\n",
      "Steps : 74400, \t Total Gen Loss : 23.412921905517578, \t Total Dis Loss : 0.0010279824491590261\n",
      "Steps : 74500, \t Total Gen Loss : 29.363651275634766, \t Total Dis Loss : 0.0002504021394997835\n",
      "Steps : 74600, \t Total Gen Loss : 22.09261131286621, \t Total Dis Loss : 0.0005911579355597496\n",
      "Steps : 74700, \t Total Gen Loss : 27.42191505432129, \t Total Dis Loss : 0.0008896366343833506\n",
      "Steps : 74800, \t Total Gen Loss : 23.640926361083984, \t Total Dis Loss : 0.0005597274866886437\n",
      "Steps : 74900, \t Total Gen Loss : 29.784292221069336, \t Total Dis Loss : 0.0003046027850359678\n",
      "Steps : 75000, \t Total Gen Loss : 23.50856590270996, \t Total Dis Loss : 0.0005913650966249406\n",
      "Steps : 75100, \t Total Gen Loss : 34.098392486572266, \t Total Dis Loss : 0.00018105926574207842\n",
      "Steps : 75200, \t Total Gen Loss : 26.205127716064453, \t Total Dis Loss : 0.0018351154867559671\n",
      "Steps : 75300, \t Total Gen Loss : 28.637706756591797, \t Total Dis Loss : 0.00026393766165710986\n",
      "Steps : 75400, \t Total Gen Loss : 27.01776123046875, \t Total Dis Loss : 0.0008550548809580505\n",
      "Steps : 75500, \t Total Gen Loss : 31.04839515686035, \t Total Dis Loss : 7.035955059109256e-05\n",
      "Steps : 75600, \t Total Gen Loss : 27.67589569091797, \t Total Dis Loss : 0.00012805737787857652\n",
      "Steps : 75700, \t Total Gen Loss : 29.053136825561523, \t Total Dis Loss : 0.0005041540134698153\n",
      "Steps : 75800, \t Total Gen Loss : 28.828243255615234, \t Total Dis Loss : 0.00011185758194187656\n",
      "Steps : 75900, \t Total Gen Loss : 29.367050170898438, \t Total Dis Loss : 0.0004285746836103499\n",
      "Steps : 76000, \t Total Gen Loss : 26.47112274169922, \t Total Dis Loss : 0.0011799809290096164\n",
      "Steps : 76100, \t Total Gen Loss : 29.094615936279297, \t Total Dis Loss : 0.00012174500443506986\n",
      "Steps : 76200, \t Total Gen Loss : 26.75465202331543, \t Total Dis Loss : 0.006916204001754522\n",
      "Steps : 76300, \t Total Gen Loss : 30.274337768554688, \t Total Dis Loss : 0.00027934368699789047\n",
      "Steps : 76400, \t Total Gen Loss : 29.2127628326416, \t Total Dis Loss : 0.0018286540871486068\n",
      "Steps : 76500, \t Total Gen Loss : 32.73567199707031, \t Total Dis Loss : 0.0033212529961019754\n",
      "Steps : 76600, \t Total Gen Loss : 29.548648834228516, \t Total Dis Loss : 0.00017909565940499306\n",
      "Steps : 76700, \t Total Gen Loss : 29.189083099365234, \t Total Dis Loss : 0.000610574905294925\n",
      "Steps : 76800, \t Total Gen Loss : 32.397308349609375, \t Total Dis Loss : 0.00010543258395045996\n",
      "Steps : 76900, \t Total Gen Loss : 28.510263442993164, \t Total Dis Loss : 9.092008986044675e-05\n",
      "Steps : 77000, \t Total Gen Loss : 30.765544891357422, \t Total Dis Loss : 7.644001743756235e-05\n",
      "Steps : 77100, \t Total Gen Loss : 28.772518157958984, \t Total Dis Loss : 0.00026288372464478016\n",
      "Steps : 77200, \t Total Gen Loss : 28.88595199584961, \t Total Dis Loss : 0.00015608270769007504\n",
      "Steps : 77300, \t Total Gen Loss : 30.139999389648438, \t Total Dis Loss : 0.0003210644063074142\n",
      "Steps : 77400, \t Total Gen Loss : 31.07276153564453, \t Total Dis Loss : 0.0016411659307777882\n",
      "Steps : 77500, \t Total Gen Loss : 27.876842498779297, \t Total Dis Loss : 0.0005323196528479457\n",
      "Steps : 77600, \t Total Gen Loss : 28.233840942382812, \t Total Dis Loss : 0.00011896777868969366\n",
      "Steps : 77700, \t Total Gen Loss : 28.602249145507812, \t Total Dis Loss : 0.0025110067799687386\n",
      "Steps : 77800, \t Total Gen Loss : 25.59695053100586, \t Total Dis Loss : 0.0002593885874375701\n",
      "Steps : 77900, \t Total Gen Loss : 28.82423973083496, \t Total Dis Loss : 0.00021382019622251391\n",
      "Steps : 78000, \t Total Gen Loss : 29.60985565185547, \t Total Dis Loss : 0.0002553079684730619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 78100, \t Total Gen Loss : 22.681241989135742, \t Total Dis Loss : 0.008476444520056248\n",
      "Steps : 78200, \t Total Gen Loss : 24.230310440063477, \t Total Dis Loss : 0.01071784645318985\n",
      "Steps : 78300, \t Total Gen Loss : 32.38615036010742, \t Total Dis Loss : 0.0003661156806629151\n",
      "Steps : 78400, \t Total Gen Loss : 32.55577087402344, \t Total Dis Loss : 0.0005568276974372566\n",
      "Steps : 78500, \t Total Gen Loss : 31.015853881835938, \t Total Dis Loss : 0.0003663120442070067\n",
      "Steps : 78600, \t Total Gen Loss : 28.9790096282959, \t Total Dis Loss : 0.0003287300933152437\n",
      "Steps : 78700, \t Total Gen Loss : 28.68724822998047, \t Total Dis Loss : 0.003849519183859229\n",
      "Time for epoch 14 is 69.01912117004395 sec\n",
      "Steps : 78800, \t Total Gen Loss : 34.16685104370117, \t Total Dis Loss : 0.0047273035161197186\n",
      "Steps : 78900, \t Total Gen Loss : 30.505447387695312, \t Total Dis Loss : 0.00010980638762703165\n",
      "Steps : 79000, \t Total Gen Loss : 30.079097747802734, \t Total Dis Loss : 0.004695529583841562\n",
      "Steps : 79100, \t Total Gen Loss : 26.402069091796875, \t Total Dis Loss : 9.810024494072422e-05\n",
      "Steps : 79200, \t Total Gen Loss : 30.330686569213867, \t Total Dis Loss : 8.299217734020203e-05\n",
      "Steps : 79300, \t Total Gen Loss : 28.56161880493164, \t Total Dis Loss : 7.677763642277569e-05\n",
      "Steps : 79400, \t Total Gen Loss : 32.34172058105469, \t Total Dis Loss : 0.00011093795183114707\n",
      "Steps : 79500, \t Total Gen Loss : 28.467464447021484, \t Total Dis Loss : 4.0384566091233864e-05\n",
      "Steps : 79600, \t Total Gen Loss : 27.537355422973633, \t Total Dis Loss : 0.0009480899898335338\n",
      "Steps : 79700, \t Total Gen Loss : 27.661928176879883, \t Total Dis Loss : 0.0001612092019058764\n",
      "Steps : 79800, \t Total Gen Loss : 30.7319278717041, \t Total Dis Loss : 0.00010939307685475796\n",
      "Steps : 79900, \t Total Gen Loss : 28.782569885253906, \t Total Dis Loss : 0.0002139593125320971\n",
      "Steps : 80000, \t Total Gen Loss : 30.14023208618164, \t Total Dis Loss : 6.879107240820304e-05\n",
      "Steps : 80100, \t Total Gen Loss : 27.608478546142578, \t Total Dis Loss : 0.0001427957322448492\n",
      "Steps : 80200, \t Total Gen Loss : 24.279199600219727, \t Total Dis Loss : 0.003038944210857153\n",
      "Steps : 80300, \t Total Gen Loss : 30.930286407470703, \t Total Dis Loss : 7.93704530224204e-05\n",
      "Steps : 80400, \t Total Gen Loss : 25.325654983520508, \t Total Dis Loss : 0.00011372387234587222\n",
      "Steps : 80500, \t Total Gen Loss : 28.14881134033203, \t Total Dis Loss : 0.00024689047131687403\n",
      "Steps : 80600, \t Total Gen Loss : 26.33246612548828, \t Total Dis Loss : 0.0001829598768381402\n",
      "Steps : 80700, \t Total Gen Loss : 26.157245635986328, \t Total Dis Loss : 0.00014832681335974485\n",
      "Steps : 80800, \t Total Gen Loss : 28.325918197631836, \t Total Dis Loss : 0.00011505439033498988\n",
      "Steps : 80900, \t Total Gen Loss : 27.849227905273438, \t Total Dis Loss : 9.144320210907608e-05\n",
      "Steps : 81000, \t Total Gen Loss : 28.252017974853516, \t Total Dis Loss : 7.782648026477545e-05\n",
      "Steps : 81100, \t Total Gen Loss : 28.292644500732422, \t Total Dis Loss : 4.608746894518845e-05\n",
      "Steps : 81200, \t Total Gen Loss : 27.23543930053711, \t Total Dis Loss : 0.00047844520304352045\n",
      "Steps : 81300, \t Total Gen Loss : 26.436687469482422, \t Total Dis Loss : 0.00015189901751000434\n",
      "Steps : 81400, \t Total Gen Loss : 27.441190719604492, \t Total Dis Loss : 8.358568447874859e-05\n",
      "Steps : 81500, \t Total Gen Loss : 28.940296173095703, \t Total Dis Loss : 0.0002020077663473785\n",
      "Steps : 81600, \t Total Gen Loss : 26.29543685913086, \t Total Dis Loss : 0.0006421121070161462\n",
      "Steps : 81700, \t Total Gen Loss : 25.360666275024414, \t Total Dis Loss : 0.00019205163698643446\n",
      "Steps : 81800, \t Total Gen Loss : 27.243003845214844, \t Total Dis Loss : 0.00012080988381057978\n",
      "Steps : 81900, \t Total Gen Loss : 28.506187438964844, \t Total Dis Loss : 0.00014673086116090417\n",
      "Steps : 82000, \t Total Gen Loss : 29.001689910888672, \t Total Dis Loss : 0.00018119749438483268\n",
      "Steps : 82100, \t Total Gen Loss : 27.77484893798828, \t Total Dis Loss : 0.0002733590081334114\n",
      "Steps : 82200, \t Total Gen Loss : 28.33572769165039, \t Total Dis Loss : 8.405208791373298e-05\n",
      "Steps : 82300, \t Total Gen Loss : 26.126270294189453, \t Total Dis Loss : 9.62129415711388e-05\n",
      "Steps : 82400, \t Total Gen Loss : 28.070419311523438, \t Total Dis Loss : 0.005730545613914728\n",
      "Steps : 82500, \t Total Gen Loss : 29.78278160095215, \t Total Dis Loss : 0.0002798055938910693\n",
      "Steps : 82600, \t Total Gen Loss : 30.03678321838379, \t Total Dis Loss : 7.650535553693771e-05\n",
      "Steps : 82700, \t Total Gen Loss : 27.27276611328125, \t Total Dis Loss : 0.000116711511509493\n",
      "Steps : 82800, \t Total Gen Loss : 31.386280059814453, \t Total Dis Loss : 9.143684292212129e-05\n",
      "Steps : 82900, \t Total Gen Loss : 27.07346534729004, \t Total Dis Loss : 0.04548513889312744\n",
      "Steps : 83000, \t Total Gen Loss : 33.23191833496094, \t Total Dis Loss : 7.197300146799535e-05\n",
      "Steps : 83100, \t Total Gen Loss : 25.730026245117188, \t Total Dis Loss : 0.0002745480160228908\n",
      "Steps : 83200, \t Total Gen Loss : 30.634918212890625, \t Total Dis Loss : 0.0002822093665599823\n",
      "Steps : 83300, \t Total Gen Loss : 26.015899658203125, \t Total Dis Loss : 8.176921983249485e-05\n",
      "Steps : 83400, \t Total Gen Loss : 29.13530158996582, \t Total Dis Loss : 0.0007149068405851722\n",
      "Steps : 83500, \t Total Gen Loss : 25.833499908447266, \t Total Dis Loss : 9.848950867308304e-05\n",
      "Steps : 83600, \t Total Gen Loss : 32.167423248291016, \t Total Dis Loss : 8.329429692821577e-05\n",
      "Steps : 83700, \t Total Gen Loss : 26.731590270996094, \t Total Dis Loss : 0.004242403898388147\n",
      "Steps : 83800, \t Total Gen Loss : 30.38901710510254, \t Total Dis Loss : 0.00023867208801675588\n",
      "Steps : 83900, \t Total Gen Loss : 29.68437957763672, \t Total Dis Loss : 0.0001042936637531966\n",
      "Steps : 84000, \t Total Gen Loss : 29.682025909423828, \t Total Dis Loss : 0.00013512665464077145\n",
      "Steps : 84100, \t Total Gen Loss : 30.12775993347168, \t Total Dis Loss : 0.0010523294331505895\n",
      "Steps : 84200, \t Total Gen Loss : 30.110149383544922, \t Total Dis Loss : 0.006004457827657461\n",
      "Steps : 84300, \t Total Gen Loss : 26.447980880737305, \t Total Dis Loss : 0.0008958768448792398\n",
      "Time for epoch 15 is 69.62266182899475 sec\n",
      "Steps : 84400, \t Total Gen Loss : 28.10369873046875, \t Total Dis Loss : 0.000745099619962275\n",
      "Steps : 84500, \t Total Gen Loss : 31.030467987060547, \t Total Dis Loss : 0.0011665992205962539\n",
      "Steps : 84600, \t Total Gen Loss : 28.03898811340332, \t Total Dis Loss : 0.00034918071469292045\n",
      "Steps : 84700, \t Total Gen Loss : 26.615285873413086, \t Total Dis Loss : 0.0077895354479551315\n",
      "Steps : 84800, \t Total Gen Loss : 30.839662551879883, \t Total Dis Loss : 0.00011933752830373123\n",
      "Steps : 84900, \t Total Gen Loss : 27.66791534423828, \t Total Dis Loss : 4.6630339056719095e-05\n",
      "Steps : 85000, \t Total Gen Loss : 32.2137451171875, \t Total Dis Loss : 8.138066914398223e-05\n",
      "Steps : 85100, \t Total Gen Loss : 32.993316650390625, \t Total Dis Loss : 0.0017068339511752129\n",
      "Steps : 85200, \t Total Gen Loss : 31.325241088867188, \t Total Dis Loss : 0.0035966262221336365\n",
      "Steps : 85300, \t Total Gen Loss : 29.93120574951172, \t Total Dis Loss : 0.0027396574150770903\n",
      "Steps : 85400, \t Total Gen Loss : 27.155513763427734, \t Total Dis Loss : 0.00026180874556303024\n",
      "Steps : 85500, \t Total Gen Loss : 26.86180877685547, \t Total Dis Loss : 0.007143666967749596\n",
      "Steps : 85600, \t Total Gen Loss : 31.752017974853516, \t Total Dis Loss : 0.00015820135013200343\n",
      "Steps : 85700, \t Total Gen Loss : 30.883682250976562, \t Total Dis Loss : 0.0003036626731045544\n",
      "Steps : 85800, \t Total Gen Loss : 28.273967742919922, \t Total Dis Loss : 0.00011730343976523727\n",
      "Steps : 85900, \t Total Gen Loss : 30.959888458251953, \t Total Dis Loss : 0.00017702183686196804\n",
      "Steps : 86000, \t Total Gen Loss : 29.332294464111328, \t Total Dis Loss : 6.522964395117015e-05\n",
      "Steps : 86100, \t Total Gen Loss : 30.700916290283203, \t Total Dis Loss : 0.00011571701907087117\n",
      "Steps : 86200, \t Total Gen Loss : 26.829795837402344, \t Total Dis Loss : 0.00036026068846695125\n",
      "Steps : 86300, \t Total Gen Loss : 26.751445770263672, \t Total Dis Loss : 0.0003486339992377907\n",
      "Steps : 86400, \t Total Gen Loss : 29.319400787353516, \t Total Dis Loss : 0.00012355661601759493\n",
      "Steps : 86500, \t Total Gen Loss : 29.49215316772461, \t Total Dis Loss : 0.00047151936450973153\n",
      "Steps : 86600, \t Total Gen Loss : 33.22990417480469, \t Total Dis Loss : 0.0010106025729328394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 86700, \t Total Gen Loss : 29.280521392822266, \t Total Dis Loss : 0.00040498076123185456\n",
      "Steps : 86800, \t Total Gen Loss : 32.012001037597656, \t Total Dis Loss : 0.0002783803502097726\n",
      "Steps : 86900, \t Total Gen Loss : 29.640594482421875, \t Total Dis Loss : 0.0013114401372149587\n",
      "Steps : 87000, \t Total Gen Loss : 29.704097747802734, \t Total Dis Loss : 3.160467167617753e-05\n",
      "Steps : 87100, \t Total Gen Loss : 27.271625518798828, \t Total Dis Loss : 0.00017662244499661028\n",
      "Steps : 87200, \t Total Gen Loss : 28.84469223022461, \t Total Dis Loss : 3.679699148051441e-05\n",
      "Steps : 87300, \t Total Gen Loss : 30.348886489868164, \t Total Dis Loss : 0.0006254701875150204\n",
      "Steps : 87400, \t Total Gen Loss : 38.044700622558594, \t Total Dis Loss : 0.0024850494228303432\n",
      "Steps : 87500, \t Total Gen Loss : 26.350387573242188, \t Total Dis Loss : 0.00038449245039373636\n",
      "Steps : 87600, \t Total Gen Loss : 31.007251739501953, \t Total Dis Loss : 0.00014344278315547854\n",
      "Steps : 87700, \t Total Gen Loss : 28.558792114257812, \t Total Dis Loss : 8.992975199362263e-05\n",
      "Steps : 87800, \t Total Gen Loss : 28.159847259521484, \t Total Dis Loss : 0.00022355405963025987\n",
      "Steps : 87900, \t Total Gen Loss : 26.95229148864746, \t Total Dis Loss : 7.021486817393452e-05\n",
      "Steps : 88000, \t Total Gen Loss : 30.961048126220703, \t Total Dis Loss : 0.0001217888348037377\n",
      "Steps : 88100, \t Total Gen Loss : 24.279560089111328, \t Total Dis Loss : 0.00018889211060013622\n",
      "Steps : 88200, \t Total Gen Loss : 28.376005172729492, \t Total Dis Loss : 0.00018527355859987438\n",
      "Steps : 88300, \t Total Gen Loss : 28.62547492980957, \t Total Dis Loss : 5.9232890635030344e-05\n",
      "Steps : 88400, \t Total Gen Loss : 31.237550735473633, \t Total Dis Loss : 0.00010547219426371157\n",
      "Steps : 88500, \t Total Gen Loss : 30.30254364013672, \t Total Dis Loss : 0.0021762396208941936\n",
      "Steps : 88600, \t Total Gen Loss : 31.169584274291992, \t Total Dis Loss : 0.0005823893006891012\n",
      "Steps : 88700, \t Total Gen Loss : 33.817161560058594, \t Total Dis Loss : 0.005994821432977915\n",
      "Steps : 88800, \t Total Gen Loss : 32.035560607910156, \t Total Dis Loss : 5.507611422217451e-05\n",
      "Steps : 88900, \t Total Gen Loss : 32.54800033569336, \t Total Dis Loss : 7.871461275499314e-05\n",
      "Steps : 89000, \t Total Gen Loss : 28.03878402709961, \t Total Dis Loss : 0.0005711691337637603\n",
      "Steps : 89100, \t Total Gen Loss : 26.97836685180664, \t Total Dis Loss : 0.0002408562577329576\n",
      "Steps : 89200, \t Total Gen Loss : 29.809417724609375, \t Total Dis Loss : 0.00023087512818165123\n",
      "Steps : 89300, \t Total Gen Loss : 30.884944915771484, \t Total Dis Loss : 0.0002921292616520077\n",
      "Steps : 89400, \t Total Gen Loss : 33.01329803466797, \t Total Dis Loss : 0.0005328828701749444\n",
      "Steps : 89500, \t Total Gen Loss : 33.48917007446289, \t Total Dis Loss : 0.0001613250351510942\n",
      "Steps : 89600, \t Total Gen Loss : 29.51861572265625, \t Total Dis Loss : 0.003012662520632148\n",
      "Steps : 89700, \t Total Gen Loss : 31.875534057617188, \t Total Dis Loss : 0.0002667990338522941\n",
      "Steps : 89800, \t Total Gen Loss : 27.495567321777344, \t Total Dis Loss : 0.00030138224246911705\n",
      "Steps : 89900, \t Total Gen Loss : 28.56011962890625, \t Total Dis Loss : 0.00040169665589928627\n",
      "Steps : 90000, \t Total Gen Loss : 32.704833984375, \t Total Dis Loss : 7.999353692866862e-05\n",
      "Time for epoch 16 is 69.03037285804749 sec\n",
      "Steps : 90100, \t Total Gen Loss : 32.738101959228516, \t Total Dis Loss : 0.0010381124448031187\n",
      "Steps : 90200, \t Total Gen Loss : 32.02214050292969, \t Total Dis Loss : 0.0013316337717697024\n",
      "Steps : 90300, \t Total Gen Loss : 29.162551879882812, \t Total Dis Loss : 0.0009893968235701323\n",
      "Steps : 90400, \t Total Gen Loss : 25.448204040527344, \t Total Dis Loss : 0.005409348290413618\n",
      "Steps : 90500, \t Total Gen Loss : 34.98291015625, \t Total Dis Loss : 0.0004870761767961085\n",
      "Steps : 90600, \t Total Gen Loss : 25.704601287841797, \t Total Dis Loss : 0.0010377917205914855\n",
      "Steps : 90700, \t Total Gen Loss : 26.618404388427734, \t Total Dis Loss : 0.00018787480075843632\n",
      "Steps : 90800, \t Total Gen Loss : 27.690105438232422, \t Total Dis Loss : 0.0004499128845054656\n",
      "Steps : 90900, \t Total Gen Loss : 29.751686096191406, \t Total Dis Loss : 0.004162308294326067\n",
      "Steps : 91000, \t Total Gen Loss : 31.833168029785156, \t Total Dis Loss : 0.0005004684790037572\n",
      "Steps : 91100, \t Total Gen Loss : 27.91960334777832, \t Total Dis Loss : 0.0004209964827168733\n",
      "Steps : 91200, \t Total Gen Loss : 27.183549880981445, \t Total Dis Loss : 0.0018411545315757394\n",
      "Steps : 91300, \t Total Gen Loss : 28.866464614868164, \t Total Dis Loss : 0.0003652143641375005\n",
      "Steps : 91400, \t Total Gen Loss : 29.06500244140625, \t Total Dis Loss : 0.00019450217951089144\n",
      "Steps : 91500, \t Total Gen Loss : 27.51468276977539, \t Total Dis Loss : 0.0001926340046338737\n",
      "Steps : 91600, \t Total Gen Loss : 24.643482208251953, \t Total Dis Loss : 0.005032173823565245\n",
      "Steps : 91700, \t Total Gen Loss : 26.277774810791016, \t Total Dis Loss : 0.0006217841291800141\n",
      "Steps : 91800, \t Total Gen Loss : 28.59770965576172, \t Total Dis Loss : 0.06444957107305527\n",
      "Steps : 91900, \t Total Gen Loss : 32.738685607910156, \t Total Dis Loss : 0.0003898295108228922\n",
      "Steps : 92000, \t Total Gen Loss : 32.87126159667969, \t Total Dis Loss : 0.000799338158685714\n",
      "Steps : 92100, \t Total Gen Loss : 29.763547897338867, \t Total Dis Loss : 0.001816426170989871\n",
      "Steps : 92200, \t Total Gen Loss : 37.69744873046875, \t Total Dis Loss : 0.0009934557601809502\n",
      "Steps : 92300, \t Total Gen Loss : 27.910688400268555, \t Total Dis Loss : 0.003173853736370802\n",
      "Steps : 92400, \t Total Gen Loss : 30.312503814697266, \t Total Dis Loss : 0.07056047022342682\n",
      "Steps : 92500, \t Total Gen Loss : 27.691204071044922, \t Total Dis Loss : 0.005020154174417257\n",
      "Steps : 92600, \t Total Gen Loss : 29.200767517089844, \t Total Dis Loss : 0.0004916741163469851\n",
      "Steps : 92700, \t Total Gen Loss : 30.531404495239258, \t Total Dis Loss : 0.0003215062024537474\n",
      "Steps : 92800, \t Total Gen Loss : 33.62415313720703, \t Total Dis Loss : 0.0005105556920170784\n",
      "Steps : 92900, \t Total Gen Loss : 27.47067642211914, \t Total Dis Loss : 0.0008040225366130471\n",
      "Steps : 93000, \t Total Gen Loss : 33.28972625732422, \t Total Dis Loss : 0.0006686045089736581\n",
      "Steps : 93100, \t Total Gen Loss : 28.11773681640625, \t Total Dis Loss : 0.000807823846116662\n",
      "Steps : 93200, \t Total Gen Loss : 29.673274993896484, \t Total Dis Loss : 0.0007572982576675713\n",
      "Steps : 93300, \t Total Gen Loss : 27.309467315673828, \t Total Dis Loss : 0.0001865077210823074\n",
      "Steps : 93400, \t Total Gen Loss : 27.725343704223633, \t Total Dis Loss : 0.0005083336727693677\n",
      "Steps : 93500, \t Total Gen Loss : 31.365489959716797, \t Total Dis Loss : 0.0024296543560922146\n",
      "Steps : 93600, \t Total Gen Loss : 29.412187576293945, \t Total Dis Loss : 0.001560245524160564\n",
      "Steps : 93700, \t Total Gen Loss : 36.828372955322266, \t Total Dis Loss : 0.000780089118052274\n",
      "Steps : 93800, \t Total Gen Loss : 29.599594116210938, \t Total Dis Loss : 0.0006251601735129952\n",
      "Steps : 93900, \t Total Gen Loss : 33.772499084472656, \t Total Dis Loss : 0.0014220502926036716\n",
      "Steps : 94000, \t Total Gen Loss : 25.858217239379883, \t Total Dis Loss : 0.28822219371795654\n",
      "Steps : 94100, \t Total Gen Loss : 30.094026565551758, \t Total Dis Loss : 0.0010903211077675223\n",
      "Steps : 94200, \t Total Gen Loss : 34.585872650146484, \t Total Dis Loss : 0.0017619229620322585\n",
      "Steps : 94300, \t Total Gen Loss : 30.229354858398438, \t Total Dis Loss : 0.004597289953380823\n",
      "Steps : 94400, \t Total Gen Loss : 28.011226654052734, \t Total Dis Loss : 0.00027180343749932945\n",
      "Steps : 94500, \t Total Gen Loss : 28.58965301513672, \t Total Dis Loss : 0.00048443363630212843\n",
      "Steps : 94600, \t Total Gen Loss : 28.084217071533203, \t Total Dis Loss : 0.00023097866505850106\n",
      "Steps : 94700, \t Total Gen Loss : 27.711463928222656, \t Total Dis Loss : 0.0009393929503858089\n",
      "Steps : 94800, \t Total Gen Loss : 32.21260452270508, \t Total Dis Loss : 0.0661562904715538\n",
      "Steps : 94900, \t Total Gen Loss : 34.802154541015625, \t Total Dis Loss : 0.0010326512856408954\n",
      "Steps : 95000, \t Total Gen Loss : 29.881183624267578, \t Total Dis Loss : 0.0014071812620386481\n",
      "Steps : 95100, \t Total Gen Loss : 30.794633865356445, \t Total Dis Loss : 0.0017310191178694367\n",
      "Steps : 95200, \t Total Gen Loss : 29.637725830078125, \t Total Dis Loss : 0.005185672547668219\n",
      "Steps : 95300, \t Total Gen Loss : 30.23263931274414, \t Total Dis Loss : 0.0014134601224213839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 95400, \t Total Gen Loss : 29.30158233642578, \t Total Dis Loss : 0.0012563730124384165\n",
      "Steps : 95500, \t Total Gen Loss : 30.792383193969727, \t Total Dis Loss : 0.004637191072106361\n",
      "Steps : 95600, \t Total Gen Loss : 26.82655143737793, \t Total Dis Loss : 0.1007152870297432\n",
      "Time for epoch 17 is 69.01090955734253 sec\n",
      "Steps : 95700, \t Total Gen Loss : 33.745540618896484, \t Total Dis Loss : 0.002276881830766797\n",
      "Steps : 95800, \t Total Gen Loss : 28.283668518066406, \t Total Dis Loss : 0.0014840855728834867\n",
      "Steps : 95900, \t Total Gen Loss : 27.417606353759766, \t Total Dis Loss : 0.002368396846577525\n",
      "Steps : 96000, \t Total Gen Loss : 28.96259880065918, \t Total Dis Loss : 0.001187934773042798\n",
      "Steps : 96100, \t Total Gen Loss : 30.48229217529297, \t Total Dis Loss : 0.0008403990650549531\n",
      "Steps : 96200, \t Total Gen Loss : 26.563779830932617, \t Total Dis Loss : 0.00017402996309101582\n",
      "Steps : 96300, \t Total Gen Loss : 30.180498123168945, \t Total Dis Loss : 0.0014000465162098408\n",
      "Steps : 96400, \t Total Gen Loss : 32.87423324584961, \t Total Dis Loss : 0.00357398996129632\n",
      "Steps : 96500, \t Total Gen Loss : 30.599117279052734, \t Total Dis Loss : 0.00014104026195127517\n",
      "Steps : 96600, \t Total Gen Loss : 29.832815170288086, \t Total Dis Loss : 0.0013586775166913867\n",
      "Steps : 96700, \t Total Gen Loss : 29.951000213623047, \t Total Dis Loss : 0.0011057582451030612\n",
      "Steps : 96800, \t Total Gen Loss : 29.228107452392578, \t Total Dis Loss : 0.001838143216446042\n",
      "Steps : 96900, \t Total Gen Loss : 28.85821533203125, \t Total Dis Loss : 0.0008672000258229673\n",
      "Steps : 97000, \t Total Gen Loss : 33.5814208984375, \t Total Dis Loss : 0.0008369753486476839\n",
      "Steps : 97100, \t Total Gen Loss : 30.467132568359375, \t Total Dis Loss : 0.0010928695555776358\n",
      "Steps : 97200, \t Total Gen Loss : 28.914390563964844, \t Total Dis Loss : 0.00041077713831327856\n",
      "Steps : 97300, \t Total Gen Loss : 30.427406311035156, \t Total Dis Loss : 0.00034604023676365614\n",
      "Steps : 97400, \t Total Gen Loss : 30.08124542236328, \t Total Dis Loss : 0.0021788314916193485\n",
      "Steps : 97500, \t Total Gen Loss : 28.18737030029297, \t Total Dis Loss : 0.0011092220665886998\n",
      "Steps : 97600, \t Total Gen Loss : 28.310583114624023, \t Total Dis Loss : 0.004353036172688007\n",
      "Steps : 97700, \t Total Gen Loss : 29.938228607177734, \t Total Dis Loss : 0.0004961747326888144\n",
      "Steps : 97800, \t Total Gen Loss : 31.04266929626465, \t Total Dis Loss : 0.005378258880227804\n",
      "Steps : 97900, \t Total Gen Loss : 26.9779052734375, \t Total Dis Loss : 0.0007081637741066515\n",
      "Steps : 98000, \t Total Gen Loss : 31.4713191986084, \t Total Dis Loss : 0.004049246199429035\n",
      "Steps : 98100, \t Total Gen Loss : 31.320863723754883, \t Total Dis Loss : 0.0007978999055922031\n",
      "Steps : 98200, \t Total Gen Loss : 30.52458953857422, \t Total Dis Loss : 0.0008862934773787856\n",
      "Steps : 98300, \t Total Gen Loss : 31.426780700683594, \t Total Dis Loss : 0.05379761755466461\n",
      "Steps : 98400, \t Total Gen Loss : 31.67144012451172, \t Total Dis Loss : 0.0009449085337109864\n",
      "Steps : 98500, \t Total Gen Loss : 32.00595474243164, \t Total Dis Loss : 0.0004934822209179401\n",
      "Steps : 98600, \t Total Gen Loss : 29.76076889038086, \t Total Dis Loss : 0.0007567107677459717\n",
      "Steps : 98700, \t Total Gen Loss : 29.79790496826172, \t Total Dis Loss : 0.0021881284192204475\n",
      "Steps : 98800, \t Total Gen Loss : 30.296337127685547, \t Total Dis Loss : 0.00023993330250959843\n",
      "Steps : 98900, \t Total Gen Loss : 33.80049133300781, \t Total Dis Loss : 0.0013684257864952087\n",
      "Steps : 99000, \t Total Gen Loss : 30.345447540283203, \t Total Dis Loss : 0.0003627463593147695\n",
      "Steps : 99100, \t Total Gen Loss : 29.907649993896484, \t Total Dis Loss : 0.0003844203893095255\n",
      "Steps : 99200, \t Total Gen Loss : 34.790340423583984, \t Total Dis Loss : 0.004151687026023865\n",
      "Steps : 99300, \t Total Gen Loss : 31.408809661865234, \t Total Dis Loss : 0.003480874700471759\n",
      "Steps : 99400, \t Total Gen Loss : 30.206823348999023, \t Total Dis Loss : 0.0003959169262088835\n",
      "Steps : 99500, \t Total Gen Loss : 29.533044815063477, \t Total Dis Loss : 0.0028586264234036207\n",
      "Steps : 99600, \t Total Gen Loss : 26.754558563232422, \t Total Dis Loss : 0.002339386148378253\n",
      "Steps : 99700, \t Total Gen Loss : 29.938486099243164, \t Total Dis Loss : 0.00029974235803820193\n",
      "Steps : 99800, \t Total Gen Loss : 32.05184555053711, \t Total Dis Loss : 0.00046304293209686875\n",
      "Steps : 99900, \t Total Gen Loss : 28.78257179260254, \t Total Dis Loss : 0.0007832663832232356\n",
      "Steps : 100000, \t Total Gen Loss : 27.671377182006836, \t Total Dis Loss : 0.0007144977571442723\n",
      "Steps : 100100, \t Total Gen Loss : 30.73619842529297, \t Total Dis Loss : 0.0009061813470907509\n",
      "Steps : 100200, \t Total Gen Loss : 32.08317947387695, \t Total Dis Loss : 0.0003278625081293285\n",
      "Steps : 100300, \t Total Gen Loss : 30.176746368408203, \t Total Dis Loss : 0.00021391993504948914\n",
      "Steps : 100400, \t Total Gen Loss : 29.294342041015625, \t Total Dis Loss : 0.0002558160340413451\n",
      "Steps : 100500, \t Total Gen Loss : 30.814016342163086, \t Total Dis Loss : 0.00037420590524561703\n",
      "Steps : 100600, \t Total Gen Loss : 29.934415817260742, \t Total Dis Loss : 0.000672701804433018\n",
      "Steps : 100700, \t Total Gen Loss : 24.9825382232666, \t Total Dis Loss : 0.00021694906172342598\n",
      "Steps : 100800, \t Total Gen Loss : 31.175678253173828, \t Total Dis Loss : 0.0004523050447460264\n",
      "Steps : 100900, \t Total Gen Loss : 30.556398391723633, \t Total Dis Loss : 0.0008537992835044861\n",
      "Steps : 101000, \t Total Gen Loss : 28.373708724975586, \t Total Dis Loss : 0.00035821920027956367\n",
      "Steps : 101100, \t Total Gen Loss : 31.265625, \t Total Dis Loss : 0.00044431182323023677\n",
      "Steps : 101200, \t Total Gen Loss : 31.303607940673828, \t Total Dis Loss : 0.0004146769642829895\n",
      "Time for epoch 18 is 68.99945616722107 sec\n",
      "Steps : 101300, \t Total Gen Loss : 31.961320877075195, \t Total Dis Loss : 0.0007821032195352018\n",
      "Steps : 101400, \t Total Gen Loss : 33.585838317871094, \t Total Dis Loss : 0.0006833880906924605\n",
      "Steps : 101500, \t Total Gen Loss : 28.766386032104492, \t Total Dis Loss : 0.0036729543935507536\n",
      "Steps : 101600, \t Total Gen Loss : 29.770130157470703, \t Total Dis Loss : 0.0019904100336134434\n",
      "Steps : 101700, \t Total Gen Loss : 33.75421905517578, \t Total Dis Loss : 0.000605172710493207\n",
      "Steps : 101800, \t Total Gen Loss : 30.101457595825195, \t Total Dis Loss : 0.00032240079599432647\n",
      "Steps : 101900, \t Total Gen Loss : 32.632652282714844, \t Total Dis Loss : 0.0006358321406878531\n",
      "Steps : 102000, \t Total Gen Loss : 30.847055435180664, \t Total Dis Loss : 0.00037221648381091654\n",
      "Steps : 102100, \t Total Gen Loss : 33.496009826660156, \t Total Dis Loss : 0.00023105752188712358\n",
      "Steps : 102200, \t Total Gen Loss : 33.16835021972656, \t Total Dis Loss : 0.0007079041097313166\n",
      "Steps : 102300, \t Total Gen Loss : 32.121028900146484, \t Total Dis Loss : 0.0006785535369999707\n",
      "Steps : 102400, \t Total Gen Loss : 29.400283813476562, \t Total Dis Loss : 0.0007276462856680155\n",
      "Steps : 102500, \t Total Gen Loss : 31.212383270263672, \t Total Dis Loss : 0.0006323481793515384\n",
      "Steps : 102600, \t Total Gen Loss : 30.569637298583984, \t Total Dis Loss : 0.00030902065918780863\n",
      "Steps : 102700, \t Total Gen Loss : 25.113140106201172, \t Total Dis Loss : 0.0006408780463971198\n",
      "Steps : 102800, \t Total Gen Loss : 28.16075897216797, \t Total Dis Loss : 0.00023906480055302382\n",
      "Steps : 102900, \t Total Gen Loss : 28.765527725219727, \t Total Dis Loss : 0.0005831068847328424\n",
      "Steps : 103000, \t Total Gen Loss : 30.574234008789062, \t Total Dis Loss : 0.0033929075580090284\n",
      "Steps : 103100, \t Total Gen Loss : 35.023887634277344, \t Total Dis Loss : 0.0009699153597466648\n",
      "Steps : 103200, \t Total Gen Loss : 32.59308624267578, \t Total Dis Loss : 0.0008726365631446242\n",
      "Steps : 103300, \t Total Gen Loss : 26.236286163330078, \t Total Dis Loss : 0.0008933293865993619\n",
      "Steps : 103400, \t Total Gen Loss : 29.564199447631836, \t Total Dis Loss : 0.004198837094008923\n",
      "Steps : 103500, \t Total Gen Loss : 32.1143798828125, \t Total Dis Loss : 0.0010106787085533142\n",
      "Steps : 103600, \t Total Gen Loss : 28.776966094970703, \t Total Dis Loss : 0.0004571341269183904\n",
      "Steps : 103700, \t Total Gen Loss : 28.847055435180664, \t Total Dis Loss : 0.0007083546370267868\n",
      "Steps : 103800, \t Total Gen Loss : 31.202659606933594, \t Total Dis Loss : 0.0017056164797395468\n",
      "Steps : 103900, \t Total Gen Loss : 25.9273624420166, \t Total Dis Loss : 0.01113995909690857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 104000, \t Total Gen Loss : 30.859085083007812, \t Total Dis Loss : 0.00014088941679801792\n",
      "Steps : 104100, \t Total Gen Loss : 27.183372497558594, \t Total Dis Loss : 0.0012994001153856516\n",
      "Steps : 104200, \t Total Gen Loss : 29.202329635620117, \t Total Dis Loss : 0.002120631979778409\n",
      "Steps : 104300, \t Total Gen Loss : 27.942794799804688, \t Total Dis Loss : 0.0004033167497254908\n",
      "Steps : 104400, \t Total Gen Loss : 28.237003326416016, \t Total Dis Loss : 0.00031087189563550055\n",
      "Steps : 104500, \t Total Gen Loss : 32.08171463012695, \t Total Dis Loss : 0.0018552467226982117\n",
      "Steps : 104600, \t Total Gen Loss : 29.351585388183594, \t Total Dis Loss : 0.0011391190346330404\n",
      "Steps : 104700, \t Total Gen Loss : 25.50890350341797, \t Total Dis Loss : 0.0006382527062669396\n",
      "Steps : 104800, \t Total Gen Loss : 32.75901794433594, \t Total Dis Loss : 0.00042760593350976706\n",
      "Steps : 104900, \t Total Gen Loss : 30.863920211791992, \t Total Dis Loss : 0.0008131960639730096\n",
      "Steps : 105000, \t Total Gen Loss : 34.87215042114258, \t Total Dis Loss : 0.00023621732543688267\n",
      "Steps : 105100, \t Total Gen Loss : 31.987342834472656, \t Total Dis Loss : 0.0003982068446930498\n",
      "Steps : 105200, \t Total Gen Loss : 29.513200759887695, \t Total Dis Loss : 0.00035143992863595486\n",
      "Steps : 105300, \t Total Gen Loss : 27.502347946166992, \t Total Dis Loss : 0.3067862093448639\n",
      "Steps : 105400, \t Total Gen Loss : 29.066421508789062, \t Total Dis Loss : 0.0010139798978343606\n",
      "Steps : 105500, \t Total Gen Loss : 29.235498428344727, \t Total Dis Loss : 0.0004944037063978612\n",
      "Steps : 105600, \t Total Gen Loss : 27.830867767333984, \t Total Dis Loss : 0.00046828019549138844\n",
      "Steps : 105700, \t Total Gen Loss : 32.896060943603516, \t Total Dis Loss : 0.024207983165979385\n",
      "Steps : 105800, \t Total Gen Loss : 32.01214599609375, \t Total Dis Loss : 0.005049531813710928\n",
      "Steps : 105900, \t Total Gen Loss : 35.94798278808594, \t Total Dis Loss : 0.00012254182365722954\n",
      "Steps : 106000, \t Total Gen Loss : 32.05989074707031, \t Total Dis Loss : 0.0006185884703882039\n",
      "Steps : 106100, \t Total Gen Loss : 29.99679946899414, \t Total Dis Loss : 0.024369435384869576\n",
      "Steps : 106200, \t Total Gen Loss : 31.550588607788086, \t Total Dis Loss : 0.0007200598483905196\n",
      "Steps : 106300, \t Total Gen Loss : 30.772438049316406, \t Total Dis Loss : 0.0003097833541687578\n",
      "Steps : 106400, \t Total Gen Loss : 34.170066833496094, \t Total Dis Loss : 0.0007189533207565546\n",
      "Steps : 106500, \t Total Gen Loss : 30.986230850219727, \t Total Dis Loss : 0.001909890677779913\n",
      "Steps : 106600, \t Total Gen Loss : 30.420955657958984, \t Total Dis Loss : 0.00015466248441953212\n",
      "Steps : 106700, \t Total Gen Loss : 31.871660232543945, \t Total Dis Loss : 0.001092959544621408\n",
      "Steps : 106800, \t Total Gen Loss : 33.74812316894531, \t Total Dis Loss : 0.0006221494404599071\n",
      "Time for epoch 19 is 69.02012300491333 sec\n",
      "Steps : 106900, \t Total Gen Loss : 32.08172607421875, \t Total Dis Loss : 0.00017358602781314403\n",
      "Steps : 107000, \t Total Gen Loss : 25.320594787597656, \t Total Dis Loss : 0.0053342352621257305\n",
      "Steps : 107100, \t Total Gen Loss : 26.58000946044922, \t Total Dis Loss : 0.0018447054317221045\n",
      "Steps : 107200, \t Total Gen Loss : 32.90032196044922, \t Total Dis Loss : 0.0004908522241748869\n",
      "Steps : 107300, \t Total Gen Loss : 31.41393280029297, \t Total Dis Loss : 0.0033129872754216194\n",
      "Steps : 107400, \t Total Gen Loss : 33.3232421875, \t Total Dis Loss : 0.000648649875074625\n",
      "Steps : 107500, \t Total Gen Loss : 32.0047492980957, \t Total Dis Loss : 0.0010359891457483172\n",
      "Steps : 107600, \t Total Gen Loss : 29.11187744140625, \t Total Dis Loss : 0.0012552443658933043\n",
      "Steps : 107700, \t Total Gen Loss : 29.161378860473633, \t Total Dis Loss : 0.00040088777313940227\n",
      "Steps : 107800, \t Total Gen Loss : 27.303321838378906, \t Total Dis Loss : 0.000799474015366286\n",
      "Steps : 107900, \t Total Gen Loss : 28.58136558532715, \t Total Dis Loss : 0.00010867904347833246\n",
      "Steps : 108000, \t Total Gen Loss : 28.608976364135742, \t Total Dis Loss : 0.00035407106042839587\n",
      "Steps : 108100, \t Total Gen Loss : 29.28371810913086, \t Total Dis Loss : 0.00032414396991953254\n",
      "Steps : 108200, \t Total Gen Loss : 29.793720245361328, \t Total Dis Loss : 0.00019535883620847017\n",
      "Steps : 108300, \t Total Gen Loss : 31.571975708007812, \t Total Dis Loss : 0.0022974845487624407\n",
      "Steps : 108400, \t Total Gen Loss : 33.7550048828125, \t Total Dis Loss : 0.0007810827810317278\n",
      "Steps : 108500, \t Total Gen Loss : 30.318445205688477, \t Total Dis Loss : 0.00035758057492785156\n",
      "Steps : 108600, \t Total Gen Loss : 32.74437713623047, \t Total Dis Loss : 0.0004163983394391835\n",
      "Steps : 108700, \t Total Gen Loss : 29.64108657836914, \t Total Dis Loss : 7.041032949928194e-05\n",
      "Steps : 108800, \t Total Gen Loss : 31.40445327758789, \t Total Dis Loss : 0.0001828217355068773\n",
      "Steps : 108900, \t Total Gen Loss : 31.51886558532715, \t Total Dis Loss : 0.00021563074551522732\n",
      "Steps : 109000, \t Total Gen Loss : 29.899436950683594, \t Total Dis Loss : 0.0004076454206369817\n",
      "Steps : 109100, \t Total Gen Loss : 31.95191192626953, \t Total Dis Loss : 0.00038209743797779083\n",
      "Steps : 109200, \t Total Gen Loss : 31.255775451660156, \t Total Dis Loss : 0.0004106858978047967\n",
      "Steps : 109300, \t Total Gen Loss : 32.35126876831055, \t Total Dis Loss : 0.0004656313394661993\n",
      "Steps : 109400, \t Total Gen Loss : 28.543527603149414, \t Total Dis Loss : 0.0003469639632385224\n",
      "Steps : 109500, \t Total Gen Loss : 29.593055725097656, \t Total Dis Loss : 0.0002626492059789598\n",
      "Steps : 109600, \t Total Gen Loss : 28.68824005126953, \t Total Dis Loss : 0.00023936160141602159\n",
      "Steps : 109700, \t Total Gen Loss : 31.36293601989746, \t Total Dis Loss : 0.0002444024430587888\n",
      "Steps : 109800, \t Total Gen Loss : 29.979055404663086, \t Total Dis Loss : 0.00012152815179433674\n",
      "Steps : 109900, \t Total Gen Loss : 29.25037384033203, \t Total Dis Loss : 0.00023429651628248394\n",
      "Steps : 110000, \t Total Gen Loss : 30.234575271606445, \t Total Dis Loss : 0.0007020042976364493\n",
      "Steps : 110100, \t Total Gen Loss : 29.572002410888672, \t Total Dis Loss : 0.0003509827656671405\n",
      "Steps : 110200, \t Total Gen Loss : 31.28345489501953, \t Total Dis Loss : 0.00021169590763747692\n",
      "Steps : 110300, \t Total Gen Loss : 27.932247161865234, \t Total Dis Loss : 0.0001625462609808892\n",
      "Steps : 110400, \t Total Gen Loss : 28.302047729492188, \t Total Dis Loss : 6.593711441382766e-05\n",
      "Steps : 110500, \t Total Gen Loss : 34.1148567199707, \t Total Dis Loss : 6.472301902249455e-05\n",
      "Steps : 110600, \t Total Gen Loss : 28.855056762695312, \t Total Dis Loss : 0.00018351792823523283\n",
      "Steps : 110700, \t Total Gen Loss : 31.62639808654785, \t Total Dis Loss : 0.00027077438426204026\n",
      "Steps : 110800, \t Total Gen Loss : 28.96419334411621, \t Total Dis Loss : 0.0002799328067339957\n",
      "Steps : 110900, \t Total Gen Loss : 28.169282913208008, \t Total Dis Loss : 0.0003900907759089023\n",
      "Steps : 111000, \t Total Gen Loss : 29.60560417175293, \t Total Dis Loss : 0.0004793271655216813\n",
      "Steps : 111100, \t Total Gen Loss : 31.02859878540039, \t Total Dis Loss : 0.00014983606524765491\n",
      "Steps : 111200, \t Total Gen Loss : 28.994792938232422, \t Total Dis Loss : 0.0005645252531394362\n",
      "Steps : 111300, \t Total Gen Loss : 28.588558197021484, \t Total Dis Loss : 0.0011691342806443572\n",
      "Steps : 111400, \t Total Gen Loss : 32.828582763671875, \t Total Dis Loss : 0.001824269536882639\n",
      "Steps : 111500, \t Total Gen Loss : 31.6164493560791, \t Total Dis Loss : 0.00017478392692282796\n",
      "Steps : 111600, \t Total Gen Loss : 27.192758560180664, \t Total Dis Loss : 0.0010072451550513506\n",
      "Steps : 111700, \t Total Gen Loss : 28.343189239501953, \t Total Dis Loss : 0.000879345927387476\n",
      "Steps : 111800, \t Total Gen Loss : 31.635845184326172, \t Total Dis Loss : 8.516444358974695e-05\n",
      "Steps : 111900, \t Total Gen Loss : 27.53083038330078, \t Total Dis Loss : 0.0002231858961749822\n",
      "Steps : 112000, \t Total Gen Loss : 34.1268310546875, \t Total Dis Loss : 0.0016161763342097402\n",
      "Steps : 112100, \t Total Gen Loss : 29.86757469177246, \t Total Dis Loss : 0.0012670792639255524\n",
      "Steps : 112200, \t Total Gen Loss : 30.26042938232422, \t Total Dis Loss : 0.00148853100836277\n",
      "Steps : 112300, \t Total Gen Loss : 24.720108032226562, \t Total Dis Loss : 0.0001526812993688509\n",
      "Steps : 112400, \t Total Gen Loss : 28.95718765258789, \t Total Dis Loss : 0.0005026403232477605\n",
      "Steps : 112500, \t Total Gen Loss : 29.49897003173828, \t Total Dis Loss : 0.0006007925840094686\n",
      "Time for epoch 20 is 69.3313934803009 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 112600, \t Total Gen Loss : 28.295486450195312, \t Total Dis Loss : 0.00048746258835308254\n",
      "Steps : 112700, \t Total Gen Loss : 28.55373764038086, \t Total Dis Loss : 0.014646264724433422\n",
      "Steps : 112800, \t Total Gen Loss : 28.984210968017578, \t Total Dis Loss : 0.0002704070939216763\n",
      "Steps : 112900, \t Total Gen Loss : 27.808366775512695, \t Total Dis Loss : 0.00034577675978653133\n",
      "Steps : 113000, \t Total Gen Loss : 30.370437622070312, \t Total Dis Loss : 0.0002704088401515037\n",
      "Steps : 113100, \t Total Gen Loss : 28.03650665283203, \t Total Dis Loss : 0.00012375260121189058\n",
      "Steps : 113200, \t Total Gen Loss : 29.814804077148438, \t Total Dis Loss : 0.00027427414897829294\n",
      "Steps : 113300, \t Total Gen Loss : 29.926788330078125, \t Total Dis Loss : 9.921492164721712e-05\n",
      "Steps : 113400, \t Total Gen Loss : 26.542377471923828, \t Total Dis Loss : 0.0004678941913880408\n",
      "Steps : 113500, \t Total Gen Loss : 25.88748550415039, \t Total Dis Loss : 0.0008205553749576211\n",
      "Steps : 113600, \t Total Gen Loss : 32.31391906738281, \t Total Dis Loss : 0.0002737140457611531\n",
      "Steps : 113700, \t Total Gen Loss : 28.178150177001953, \t Total Dis Loss : 0.00037051632534712553\n",
      "Steps : 113800, \t Total Gen Loss : 28.632953643798828, \t Total Dis Loss : 0.0003791585913859308\n",
      "Steps : 113900, \t Total Gen Loss : 29.462417602539062, \t Total Dis Loss : 0.00048477656673640013\n",
      "Steps : 114000, \t Total Gen Loss : 26.801528930664062, \t Total Dis Loss : 0.0009281503735110164\n",
      "Steps : 114100, \t Total Gen Loss : 29.523681640625, \t Total Dis Loss : 0.0006818774854764342\n",
      "Steps : 114200, \t Total Gen Loss : 25.504100799560547, \t Total Dis Loss : 0.0006104299682192504\n",
      "Steps : 114300, \t Total Gen Loss : 30.05451011657715, \t Total Dis Loss : 0.0004346767964307219\n",
      "Steps : 114400, \t Total Gen Loss : 33.12779998779297, \t Total Dis Loss : 0.00011434878979343921\n",
      "Steps : 114500, \t Total Gen Loss : 31.156986236572266, \t Total Dis Loss : 0.00018154780264012516\n",
      "Steps : 114600, \t Total Gen Loss : 28.69093894958496, \t Total Dis Loss : 0.00022156286286190152\n",
      "Steps : 114700, \t Total Gen Loss : 27.034664154052734, \t Total Dis Loss : 0.000412352557759732\n",
      "Steps : 114800, \t Total Gen Loss : 29.443124771118164, \t Total Dis Loss : 0.0001909086131490767\n",
      "Steps : 114900, \t Total Gen Loss : 32.47895431518555, \t Total Dis Loss : 0.002002179389819503\n",
      "Steps : 115000, \t Total Gen Loss : 26.22783660888672, \t Total Dis Loss : 0.00026732159312814474\n",
      "Steps : 115100, \t Total Gen Loss : 29.345014572143555, \t Total Dis Loss : 0.0003066830977331847\n",
      "Steps : 115200, \t Total Gen Loss : 32.63465881347656, \t Total Dis Loss : 0.0005120012792758644\n",
      "Steps : 115300, \t Total Gen Loss : 29.976581573486328, \t Total Dis Loss : 0.0033054722007364035\n",
      "Steps : 115400, \t Total Gen Loss : 27.6370849609375, \t Total Dis Loss : 0.0007922120857983828\n",
      "Steps : 115500, \t Total Gen Loss : 27.51995849609375, \t Total Dis Loss : 0.0001899448106996715\n",
      "Steps : 115600, \t Total Gen Loss : 26.790485382080078, \t Total Dis Loss : 0.0004226565361022949\n",
      "Steps : 115700, \t Total Gen Loss : 30.72333526611328, \t Total Dis Loss : 0.00021855795057490468\n",
      "Steps : 115800, \t Total Gen Loss : 29.6689395904541, \t Total Dis Loss : 0.0004493104061111808\n",
      "Steps : 115900, \t Total Gen Loss : 27.055612564086914, \t Total Dis Loss : 0.000177899535628967\n",
      "Steps : 116000, \t Total Gen Loss : 28.806995391845703, \t Total Dis Loss : 0.00014726478548254818\n",
      "Steps : 116100, \t Total Gen Loss : 27.718957901000977, \t Total Dis Loss : 8.579582208767533e-05\n",
      "Steps : 116200, \t Total Gen Loss : 29.24853515625, \t Total Dis Loss : 0.00025657942751422524\n",
      "Steps : 116300, \t Total Gen Loss : 27.28662872314453, \t Total Dis Loss : 0.0007926596445031464\n",
      "Steps : 116400, \t Total Gen Loss : 28.423667907714844, \t Total Dis Loss : 0.0009436062537133694\n",
      "Steps : 116500, \t Total Gen Loss : 30.491107940673828, \t Total Dis Loss : 0.00014038773952051997\n",
      "Steps : 116600, \t Total Gen Loss : 26.894527435302734, \t Total Dis Loss : 0.0003172449651174247\n",
      "Steps : 116700, \t Total Gen Loss : 24.4901123046875, \t Total Dis Loss : 0.7288824915885925\n",
      "Steps : 116800, \t Total Gen Loss : 28.516090393066406, \t Total Dis Loss : 0.0016530647408217192\n",
      "Steps : 116900, \t Total Gen Loss : 30.983810424804688, \t Total Dis Loss : 0.0007700818823650479\n",
      "Steps : 117000, \t Total Gen Loss : 30.739646911621094, \t Total Dis Loss : 0.0006457884446717799\n",
      "Steps : 117100, \t Total Gen Loss : 28.49050521850586, \t Total Dis Loss : 0.0010417756857350469\n",
      "Steps : 117200, \t Total Gen Loss : 29.46548843383789, \t Total Dis Loss : 0.00021977981668896973\n",
      "Steps : 117300, \t Total Gen Loss : 32.67631912231445, \t Total Dis Loss : 0.0008996485266834497\n",
      "Steps : 117400, \t Total Gen Loss : 32.244102478027344, \t Total Dis Loss : 0.00018297570932190865\n",
      "Steps : 117500, \t Total Gen Loss : 24.992034912109375, \t Total Dis Loss : 0.000671762740239501\n",
      "Steps : 117600, \t Total Gen Loss : 30.500682830810547, \t Total Dis Loss : 0.00046871768427081406\n",
      "Steps : 117700, \t Total Gen Loss : 27.849075317382812, \t Total Dis Loss : 0.005278636701405048\n",
      "Steps : 117800, \t Total Gen Loss : 26.96677017211914, \t Total Dis Loss : 0.00032344128703698516\n",
      "Steps : 117900, \t Total Gen Loss : 33.073978424072266, \t Total Dis Loss : 0.00045565591426566243\n",
      "Steps : 118000, \t Total Gen Loss : 30.05319595336914, \t Total Dis Loss : 0.0001802102487999946\n",
      "Steps : 118100, \t Total Gen Loss : 27.96273422241211, \t Total Dis Loss : 0.00030062661971896887\n",
      "Time for epoch 21 is 68.94630217552185 sec\n",
      "Steps : 118200, \t Total Gen Loss : 26.892911911010742, \t Total Dis Loss : 0.0032765897922217846\n",
      "Steps : 118300, \t Total Gen Loss : 28.34085464477539, \t Total Dis Loss : 0.0003379459958523512\n",
      "Steps : 118400, \t Total Gen Loss : 22.114566802978516, \t Total Dis Loss : 0.017899075523018837\n",
      "Steps : 118500, \t Total Gen Loss : 23.804798126220703, \t Total Dis Loss : 0.0002824435359798372\n",
      "Steps : 118600, \t Total Gen Loss : 26.756092071533203, \t Total Dis Loss : 0.00046909513184800744\n",
      "Steps : 118700, \t Total Gen Loss : 32.74653625488281, \t Total Dis Loss : 0.0003835192765109241\n",
      "Steps : 118800, \t Total Gen Loss : 29.361082077026367, \t Total Dis Loss : 0.00018388066382613033\n",
      "Steps : 118900, \t Total Gen Loss : 30.419452667236328, \t Total Dis Loss : 0.0003294218040537089\n",
      "Steps : 119000, \t Total Gen Loss : 29.259693145751953, \t Total Dis Loss : 0.001633811742067337\n",
      "Steps : 119100, \t Total Gen Loss : 28.6151123046875, \t Total Dis Loss : 0.0003856768016703427\n",
      "Steps : 119200, \t Total Gen Loss : 32.46603775024414, \t Total Dis Loss : 0.00020954756473656744\n",
      "Steps : 119300, \t Total Gen Loss : 28.867076873779297, \t Total Dis Loss : 0.00045241756015457213\n",
      "Steps : 119400, \t Total Gen Loss : 27.74248695373535, \t Total Dis Loss : 0.0003728065057657659\n",
      "Steps : 119500, \t Total Gen Loss : 29.177833557128906, \t Total Dis Loss : 0.0003751163312699646\n",
      "Steps : 119600, \t Total Gen Loss : 29.700889587402344, \t Total Dis Loss : 0.00017323139763902873\n",
      "Steps : 119700, \t Total Gen Loss : 30.565689086914062, \t Total Dis Loss : 0.000365622341632843\n",
      "Steps : 119800, \t Total Gen Loss : 25.96615982055664, \t Total Dis Loss : 0.0012336797080934048\n",
      "Steps : 119900, \t Total Gen Loss : 31.757366180419922, \t Total Dis Loss : 0.0002757463080342859\n",
      "Steps : 120000, \t Total Gen Loss : 29.66884994506836, \t Total Dis Loss : 0.0004665339074563235\n",
      "Steps : 120100, \t Total Gen Loss : 28.34040069580078, \t Total Dis Loss : 0.00026172958314418793\n",
      "Steps : 120200, \t Total Gen Loss : 29.80069351196289, \t Total Dis Loss : 8.417073695454746e-05\n",
      "Steps : 120300, \t Total Gen Loss : 31.895532608032227, \t Total Dis Loss : 0.0007873512804508209\n",
      "Steps : 120400, \t Total Gen Loss : 31.880996704101562, \t Total Dis Loss : 0.00011125394667033106\n",
      "Steps : 120500, \t Total Gen Loss : 29.289363861083984, \t Total Dis Loss : 0.00041805297951214015\n",
      "Steps : 120600, \t Total Gen Loss : 28.75760269165039, \t Total Dis Loss : 0.0004349802911747247\n",
      "Steps : 120700, \t Total Gen Loss : 33.741580963134766, \t Total Dis Loss : 0.0001992820471059531\n",
      "Steps : 120800, \t Total Gen Loss : 33.28917694091797, \t Total Dis Loss : 0.00015654051094315946\n",
      "Steps : 120900, \t Total Gen Loss : 26.292421340942383, \t Total Dis Loss : 0.00028059660689905286\n",
      "Steps : 121000, \t Total Gen Loss : 28.275901794433594, \t Total Dis Loss : 0.0005293514695949852\n",
      "Steps : 121100, \t Total Gen Loss : 28.66375732421875, \t Total Dis Loss : 0.00025682459818199277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 121200, \t Total Gen Loss : 29.11285400390625, \t Total Dis Loss : 0.0005129770142957568\n",
      "Steps : 121300, \t Total Gen Loss : 28.833194732666016, \t Total Dis Loss : 0.00019127471023239195\n",
      "Steps : 121400, \t Total Gen Loss : 27.909038543701172, \t Total Dis Loss : 0.0002420934324618429\n",
      "Steps : 121500, \t Total Gen Loss : 29.60148048400879, \t Total Dis Loss : 0.00023271411191672087\n",
      "Steps : 121600, \t Total Gen Loss : 28.34048843383789, \t Total Dis Loss : 0.001638330752030015\n",
      "Steps : 121700, \t Total Gen Loss : 32.166168212890625, \t Total Dis Loss : 0.0004882742650806904\n",
      "Steps : 121800, \t Total Gen Loss : 29.778087615966797, \t Total Dis Loss : 0.000655705516692251\n",
      "Steps : 121900, \t Total Gen Loss : 26.379806518554688, \t Total Dis Loss : 0.0002054856449831277\n",
      "Steps : 122000, \t Total Gen Loss : 31.11840057373047, \t Total Dis Loss : 0.00014132921933196485\n",
      "Steps : 122100, \t Total Gen Loss : 27.15765953063965, \t Total Dis Loss : 0.00015589808754157275\n",
      "Steps : 122200, \t Total Gen Loss : 31.17723846435547, \t Total Dis Loss : 0.00032266462221741676\n",
      "Steps : 122300, \t Total Gen Loss : 29.822708129882812, \t Total Dis Loss : 0.00023281917674466968\n",
      "Steps : 122400, \t Total Gen Loss : 29.191505432128906, \t Total Dis Loss : 6.42108207102865e-05\n",
      "Steps : 122500, \t Total Gen Loss : 30.28961181640625, \t Total Dis Loss : 0.00012493939721025527\n",
      "Steps : 122600, \t Total Gen Loss : 29.972164154052734, \t Total Dis Loss : 5.404031981015578e-05\n",
      "Steps : 122700, \t Total Gen Loss : 28.048992156982422, \t Total Dis Loss : 0.00017121192649938166\n",
      "Steps : 122800, \t Total Gen Loss : 30.94745445251465, \t Total Dis Loss : 3.4553628211142495e-05\n",
      "Steps : 122900, \t Total Gen Loss : 31.60235595703125, \t Total Dis Loss : 7.265488966368139e-05\n",
      "Steps : 123000, \t Total Gen Loss : 30.271747589111328, \t Total Dis Loss : 0.00015796194202266634\n",
      "Steps : 123100, \t Total Gen Loss : 35.05376052856445, \t Total Dis Loss : 0.0005118139670230448\n",
      "Steps : 123200, \t Total Gen Loss : 33.16786193847656, \t Total Dis Loss : 0.00011345555685693398\n",
      "Steps : 123300, \t Total Gen Loss : 26.734094619750977, \t Total Dis Loss : 0.0219638142734766\n",
      "Steps : 123400, \t Total Gen Loss : 29.765178680419922, \t Total Dis Loss : 0.0021148964297026396\n",
      "Steps : 123500, \t Total Gen Loss : 29.37075424194336, \t Total Dis Loss : 0.0010415776632726192\n",
      "Steps : 123600, \t Total Gen Loss : 30.391660690307617, \t Total Dis Loss : 0.0003982210182584822\n",
      "Steps : 123700, \t Total Gen Loss : 28.477628707885742, \t Total Dis Loss : 0.0003500881721265614\n",
      "Time for epoch 22 is 68.94740056991577 sec\n",
      "Steps : 123800, \t Total Gen Loss : 29.594688415527344, \t Total Dis Loss : 0.0005849886802025139\n",
      "Steps : 123900, \t Total Gen Loss : 31.739017486572266, \t Total Dis Loss : 0.00025427338550798595\n",
      "Steps : 124000, \t Total Gen Loss : 28.565303802490234, \t Total Dis Loss : 0.00021942410967312753\n",
      "Steps : 124100, \t Total Gen Loss : 31.794166564941406, \t Total Dis Loss : 0.00034727485035546124\n",
      "Steps : 124200, \t Total Gen Loss : 31.217729568481445, \t Total Dis Loss : 0.0012138272868469357\n",
      "Steps : 124300, \t Total Gen Loss : 31.944002151489258, \t Total Dis Loss : 0.00019190579769201577\n",
      "Steps : 124400, \t Total Gen Loss : 30.456607818603516, \t Total Dis Loss : 0.00015247160627041012\n",
      "Steps : 124500, \t Total Gen Loss : 30.836206436157227, \t Total Dis Loss : 0.00028897670563310385\n",
      "Steps : 124600, \t Total Gen Loss : 30.62297821044922, \t Total Dis Loss : 0.0005114579107612371\n",
      "Steps : 124700, \t Total Gen Loss : 30.49460220336914, \t Total Dis Loss : 0.000596563913859427\n",
      "Steps : 124800, \t Total Gen Loss : 30.650127410888672, \t Total Dis Loss : 0.0002943329163827002\n",
      "Steps : 124900, \t Total Gen Loss : 34.06014633178711, \t Total Dis Loss : 0.00012352514022495598\n",
      "Steps : 125000, \t Total Gen Loss : 33.615570068359375, \t Total Dis Loss : 2.268812204420101e-05\n",
      "Steps : 125100, \t Total Gen Loss : 32.56166458129883, \t Total Dis Loss : 0.000203322313609533\n",
      "Steps : 125200, \t Total Gen Loss : 32.56858825683594, \t Total Dis Loss : 0.00026254812837578356\n",
      "Steps : 125300, \t Total Gen Loss : 31.26849365234375, \t Total Dis Loss : 5.558226257562637e-05\n",
      "Steps : 125400, \t Total Gen Loss : 27.844703674316406, \t Total Dis Loss : 0.0005849725566804409\n",
      "Steps : 125500, \t Total Gen Loss : 28.974864959716797, \t Total Dis Loss : 0.0003403369919396937\n",
      "Steps : 125600, \t Total Gen Loss : 29.974332809448242, \t Total Dis Loss : 0.00017669051885604858\n",
      "Steps : 125700, \t Total Gen Loss : 26.348434448242188, \t Total Dis Loss : 0.00047866234672255814\n",
      "Steps : 125800, \t Total Gen Loss : 34.112327575683594, \t Total Dis Loss : 0.002203590702265501\n",
      "Steps : 125900, \t Total Gen Loss : 39.77239990234375, \t Total Dis Loss : 0.00029854330932721496\n",
      "Steps : 126000, \t Total Gen Loss : 32.14868927001953, \t Total Dis Loss : 0.00029374490259215236\n",
      "Steps : 126100, \t Total Gen Loss : 29.136629104614258, \t Total Dis Loss : 0.0002877594670280814\n",
      "Steps : 126200, \t Total Gen Loss : 33.74518585205078, \t Total Dis Loss : 0.00025263853603973985\n",
      "Steps : 126300, \t Total Gen Loss : 30.883075714111328, \t Total Dis Loss : 0.0010372174438089132\n",
      "Steps : 126400, \t Total Gen Loss : 38.176231384277344, \t Total Dis Loss : 0.00027544883778318763\n",
      "Steps : 126500, \t Total Gen Loss : 32.83405685424805, \t Total Dis Loss : 0.0006194940651766956\n",
      "Steps : 126600, \t Total Gen Loss : 30.684904098510742, \t Total Dis Loss : 0.00031887053046375513\n",
      "Steps : 126700, \t Total Gen Loss : 37.22883224487305, \t Total Dis Loss : 0.0002811109588947147\n",
      "Steps : 126800, \t Total Gen Loss : 31.40964698791504, \t Total Dis Loss : 0.0007492576260119677\n",
      "Steps : 126900, \t Total Gen Loss : 28.68947410583496, \t Total Dis Loss : 0.00042662129271775484\n",
      "Steps : 127000, \t Total Gen Loss : 27.648958206176758, \t Total Dis Loss : 0.0005668862722814083\n",
      "Steps : 127100, \t Total Gen Loss : 29.256750106811523, \t Total Dis Loss : 0.0004527151177171618\n",
      "Steps : 127200, \t Total Gen Loss : 30.474761962890625, \t Total Dis Loss : 0.0002329775452381\n",
      "Steps : 127300, \t Total Gen Loss : 31.12717628479004, \t Total Dis Loss : 0.00012368019088171422\n",
      "Steps : 127400, \t Total Gen Loss : 28.228227615356445, \t Total Dis Loss : 0.0003075158456340432\n",
      "Steps : 127500, \t Total Gen Loss : 26.324058532714844, \t Total Dis Loss : 0.00029383297078311443\n",
      "Steps : 127600, \t Total Gen Loss : 26.366222381591797, \t Total Dis Loss : 0.0009243754320777953\n",
      "Steps : 127700, \t Total Gen Loss : 26.51561164855957, \t Total Dis Loss : 0.00028922525234520435\n",
      "Steps : 127800, \t Total Gen Loss : 28.086111068725586, \t Total Dis Loss : 0.000327317975461483\n",
      "Steps : 127900, \t Total Gen Loss : 29.823654174804688, \t Total Dis Loss : 0.0007143914699554443\n",
      "Steps : 128000, \t Total Gen Loss : 36.31371307373047, \t Total Dis Loss : 0.0007137438515201211\n",
      "Steps : 128100, \t Total Gen Loss : 29.382469177246094, \t Total Dis Loss : 6.993519491516054e-05\n",
      "Steps : 128200, \t Total Gen Loss : 32.7860221862793, \t Total Dis Loss : 0.00014382769586518407\n",
      "Steps : 128300, \t Total Gen Loss : 29.254108428955078, \t Total Dis Loss : 0.0002569496864452958\n",
      "Steps : 128400, \t Total Gen Loss : 30.568153381347656, \t Total Dis Loss : 0.0012727170251309872\n",
      "Steps : 128500, \t Total Gen Loss : 31.56412696838379, \t Total Dis Loss : 0.0003327345475554466\n",
      "Steps : 128600, \t Total Gen Loss : 29.040050506591797, \t Total Dis Loss : 8.218562288675457e-05\n",
      "Steps : 128700, \t Total Gen Loss : 29.715282440185547, \t Total Dis Loss : 0.0005374343600124121\n",
      "Steps : 128800, \t Total Gen Loss : 26.977340698242188, \t Total Dis Loss : 0.00011511567572597414\n",
      "Steps : 128900, \t Total Gen Loss : 27.636905670166016, \t Total Dis Loss : 0.0001615817309357226\n",
      "Steps : 129000, \t Total Gen Loss : 27.35103988647461, \t Total Dis Loss : 0.00011473838094389066\n",
      "Steps : 129100, \t Total Gen Loss : 33.18975830078125, \t Total Dis Loss : 5.6673045037314296e-05\n",
      "Steps : 129200, \t Total Gen Loss : 27.87187957763672, \t Total Dis Loss : 0.00021342678519431502\n",
      "Steps : 129300, \t Total Gen Loss : 28.54911231994629, \t Total Dis Loss : 0.00010044360533356667\n",
      "Time for epoch 23 is 68.93873047828674 sec\n",
      "Steps : 129400, \t Total Gen Loss : 28.62271499633789, \t Total Dis Loss : 0.0002459324605297297\n",
      "Steps : 129500, \t Total Gen Loss : 29.37044906616211, \t Total Dis Loss : 0.00021322430984582752\n",
      "Steps : 129600, \t Total Gen Loss : 28.590557098388672, \t Total Dis Loss : 0.0004007879178971052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 129700, \t Total Gen Loss : 31.287399291992188, \t Total Dis Loss : 0.00029891435406170785\n",
      "Steps : 129800, \t Total Gen Loss : 29.116920471191406, \t Total Dis Loss : 0.0001482439402025193\n",
      "Steps : 129900, \t Total Gen Loss : 29.161792755126953, \t Total Dis Loss : 0.00011758208711398765\n",
      "Steps : 130000, \t Total Gen Loss : 32.639076232910156, \t Total Dis Loss : 8.261225593741983e-05\n",
      "Steps : 130100, \t Total Gen Loss : 28.107751846313477, \t Total Dis Loss : 7.673110667383298e-05\n",
      "Steps : 130200, \t Total Gen Loss : 27.077123641967773, \t Total Dis Loss : 7.235088560264558e-05\n",
      "Steps : 130300, \t Total Gen Loss : 28.15975570678711, \t Total Dis Loss : 4.6208984713302925e-05\n",
      "Steps : 130400, \t Total Gen Loss : 31.890380859375, \t Total Dis Loss : 0.00014455773634836078\n",
      "Steps : 130500, \t Total Gen Loss : 28.276506423950195, \t Total Dis Loss : 2.7281541406409815e-05\n",
      "Steps : 130600, \t Total Gen Loss : 25.742568969726562, \t Total Dis Loss : 0.0013357060961425304\n",
      "Steps : 130700, \t Total Gen Loss : 38.503395080566406, \t Total Dis Loss : 0.0064764427952468395\n",
      "Steps : 130800, \t Total Gen Loss : 31.564697265625, \t Total Dis Loss : 0.0003142168861813843\n",
      "Steps : 130900, \t Total Gen Loss : 32.118896484375, \t Total Dis Loss : 0.017024436965584755\n",
      "Steps : 131000, \t Total Gen Loss : 30.23303985595703, \t Total Dis Loss : 0.0005516046076081693\n",
      "Steps : 131100, \t Total Gen Loss : 27.263309478759766, \t Total Dis Loss : 0.0002600266598165035\n",
      "Steps : 131200, \t Total Gen Loss : 29.08165740966797, \t Total Dis Loss : 0.0002775561879388988\n",
      "Steps : 131300, \t Total Gen Loss : 29.7324161529541, \t Total Dis Loss : 0.00014247599756345153\n",
      "Steps : 131400, \t Total Gen Loss : 32.756980895996094, \t Total Dis Loss : 0.00010981762898154557\n",
      "Steps : 131500, \t Total Gen Loss : 36.66067123413086, \t Total Dis Loss : 0.00019368104403838515\n",
      "Steps : 131600, \t Total Gen Loss : 28.830249786376953, \t Total Dis Loss : 0.00045173102989792824\n",
      "Steps : 131700, \t Total Gen Loss : 32.23776626586914, \t Total Dis Loss : 0.019673291593790054\n",
      "Steps : 131800, \t Total Gen Loss : 29.10879898071289, \t Total Dis Loss : 0.0003121969639323652\n",
      "Steps : 131900, \t Total Gen Loss : 32.730186462402344, \t Total Dis Loss : 0.0005373165477067232\n",
      "Steps : 132000, \t Total Gen Loss : 27.94317626953125, \t Total Dis Loss : 0.0006471291417255998\n",
      "Steps : 132100, \t Total Gen Loss : 36.532562255859375, \t Total Dis Loss : 0.00017680678865872324\n",
      "Steps : 132200, \t Total Gen Loss : 32.4390983581543, \t Total Dis Loss : 0.00018668487609829754\n",
      "Steps : 132300, \t Total Gen Loss : 28.49675750732422, \t Total Dis Loss : 0.0008340714266523719\n",
      "Steps : 132400, \t Total Gen Loss : 31.605762481689453, \t Total Dis Loss : 0.0006738494266755879\n",
      "Steps : 132500, \t Total Gen Loss : 31.466732025146484, \t Total Dis Loss : 0.0001044817763613537\n",
      "Steps : 132600, \t Total Gen Loss : 29.19222068786621, \t Total Dis Loss : 0.0002538889821153134\n",
      "Steps : 132700, \t Total Gen Loss : 30.307579040527344, \t Total Dis Loss : 0.002594382967799902\n",
      "Steps : 132800, \t Total Gen Loss : 30.6214656829834, \t Total Dis Loss : 0.0019841655157506466\n",
      "Steps : 132900, \t Total Gen Loss : 31.873706817626953, \t Total Dis Loss : 0.0002875354839488864\n",
      "Steps : 133000, \t Total Gen Loss : 31.368297576904297, \t Total Dis Loss : 0.00023081862309481949\n",
      "Steps : 133100, \t Total Gen Loss : 33.3731803894043, \t Total Dis Loss : 5.567440530285239e-05\n",
      "Steps : 133200, \t Total Gen Loss : 33.019596099853516, \t Total Dis Loss : 0.00018445137538947165\n",
      "Steps : 133300, \t Total Gen Loss : 35.09790802001953, \t Total Dis Loss : 4.095016629435122e-05\n",
      "Steps : 133400, \t Total Gen Loss : 31.3479061126709, \t Total Dis Loss : 5.3712738008471206e-05\n",
      "Steps : 133500, \t Total Gen Loss : 30.55703353881836, \t Total Dis Loss : 9.090894309338182e-05\n",
      "Steps : 133600, \t Total Gen Loss : 31.13128662109375, \t Total Dis Loss : 0.0005304720252752304\n",
      "Steps : 133700, \t Total Gen Loss : 30.3258113861084, \t Total Dis Loss : 0.00045744702219963074\n",
      "Steps : 133800, \t Total Gen Loss : 32.652130126953125, \t Total Dis Loss : 0.0002705192309804261\n",
      "Steps : 133900, \t Total Gen Loss : 31.53135871887207, \t Total Dis Loss : 0.000182681018486619\n",
      "Steps : 134000, \t Total Gen Loss : 31.691448211669922, \t Total Dis Loss : 0.000156533598783426\n",
      "Steps : 134100, \t Total Gen Loss : 28.180316925048828, \t Total Dis Loss : 0.0008093351498246193\n",
      "Steps : 134200, \t Total Gen Loss : 26.980432510375977, \t Total Dis Loss : 0.0008576729451306164\n",
      "Steps : 134300, \t Total Gen Loss : 26.098506927490234, \t Total Dis Loss : 0.000797915447037667\n",
      "Steps : 134400, \t Total Gen Loss : 34.2668342590332, \t Total Dis Loss : 0.0004007761017419398\n",
      "Steps : 134500, \t Total Gen Loss : 24.899320602416992, \t Total Dis Loss : 0.0010856129229068756\n",
      "Steps : 134600, \t Total Gen Loss : 34.16856384277344, \t Total Dis Loss : 0.00011311430716887116\n",
      "Steps : 134700, \t Total Gen Loss : 29.559669494628906, \t Total Dis Loss : 0.0005343889351934195\n",
      "Steps : 134800, \t Total Gen Loss : 27.858057022094727, \t Total Dis Loss : 0.00016106691327877343\n",
      "Steps : 134900, \t Total Gen Loss : 30.471248626708984, \t Total Dis Loss : 0.00038339593447744846\n",
      "Steps : 135000, \t Total Gen Loss : 29.935945510864258, \t Total Dis Loss : 0.0005199031438678503\n",
      "Time for epoch 24 is 69.46648073196411 sec\n",
      "Steps : 135100, \t Total Gen Loss : 28.593490600585938, \t Total Dis Loss : 0.000321788975270465\n",
      "Steps : 135200, \t Total Gen Loss : 31.66362953186035, \t Total Dis Loss : 0.0003493213444016874\n",
      "Steps : 135300, \t Total Gen Loss : 35.6780891418457, \t Total Dis Loss : 0.00024004894657991827\n",
      "Steps : 135400, \t Total Gen Loss : 29.89551544189453, \t Total Dis Loss : 9.13145049707964e-05\n",
      "Steps : 135500, \t Total Gen Loss : 29.9627742767334, \t Total Dis Loss : 0.00023605470778420568\n",
      "Steps : 135600, \t Total Gen Loss : 26.987213134765625, \t Total Dis Loss : 0.00019377980788704008\n",
      "Steps : 135700, \t Total Gen Loss : 29.74974822998047, \t Total Dis Loss : 0.00022745371097698808\n",
      "Steps : 135800, \t Total Gen Loss : 31.4493408203125, \t Total Dis Loss : 0.00021635535813402385\n",
      "Steps : 135900, \t Total Gen Loss : 31.855588912963867, \t Total Dis Loss : 0.0001368572993669659\n",
      "Steps : 136000, \t Total Gen Loss : 30.643138885498047, \t Total Dis Loss : 0.00040805531898513436\n",
      "Steps : 136100, \t Total Gen Loss : 27.186479568481445, \t Total Dis Loss : 0.00010967097477987409\n",
      "Steps : 136200, \t Total Gen Loss : 33.39397048950195, \t Total Dis Loss : 6.638468039454892e-05\n",
      "Steps : 136300, \t Total Gen Loss : 31.562522888183594, \t Total Dis Loss : 7.669311889912933e-05\n",
      "Steps : 136400, \t Total Gen Loss : 27.067604064941406, \t Total Dis Loss : 0.00020657213462982327\n",
      "Steps : 136500, \t Total Gen Loss : 30.37682342529297, \t Total Dis Loss : 5.993006925564259e-05\n",
      "Steps : 136600, \t Total Gen Loss : 28.72992515563965, \t Total Dis Loss : 4.466806058189832e-05\n",
      "Steps : 136700, \t Total Gen Loss : 28.658416748046875, \t Total Dis Loss : 6.65679617668502e-05\n",
      "Steps : 136800, \t Total Gen Loss : 27.463153839111328, \t Total Dis Loss : 0.0001656664244364947\n",
      "Steps : 136900, \t Total Gen Loss : 28.902721405029297, \t Total Dis Loss : 0.00047114351764321327\n",
      "Steps : 137000, \t Total Gen Loss : 32.63029098510742, \t Total Dis Loss : 0.0007253038347698748\n",
      "Steps : 137100, \t Total Gen Loss : 30.830034255981445, \t Total Dis Loss : 5.1511291530914605e-05\n",
      "Steps : 137200, \t Total Gen Loss : 32.990684509277344, \t Total Dis Loss : 0.00033628373057581484\n",
      "Steps : 137300, \t Total Gen Loss : 30.649795532226562, \t Total Dis Loss : 7.695659587625414e-05\n",
      "Steps : 137400, \t Total Gen Loss : 31.594026565551758, \t Total Dis Loss : 7.928976992843673e-05\n",
      "Steps : 137500, \t Total Gen Loss : 25.203754425048828, \t Total Dis Loss : 0.0005229794187471271\n",
      "Steps : 137600, \t Total Gen Loss : 24.01238250732422, \t Total Dis Loss : 0.0005560065037570894\n",
      "Steps : 137700, \t Total Gen Loss : 27.517141342163086, \t Total Dis Loss : 0.0013136669294908643\n",
      "Steps : 137800, \t Total Gen Loss : 31.63155174255371, \t Total Dis Loss : 0.0001791984832379967\n",
      "Steps : 137900, \t Total Gen Loss : 33.55809783935547, \t Total Dis Loss : 0.00018901261501014233\n",
      "Steps : 138000, \t Total Gen Loss : 31.8023681640625, \t Total Dis Loss : 5.534601223189384e-05\n",
      "Steps : 138100, \t Total Gen Loss : 29.307811737060547, \t Total Dis Loss : 4.012729550595395e-05\n",
      "Steps : 138200, \t Total Gen Loss : 28.06325912475586, \t Total Dis Loss : 0.00020702059555333108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 138300, \t Total Gen Loss : 29.7689208984375, \t Total Dis Loss : 0.00019480490300338715\n",
      "Steps : 138400, \t Total Gen Loss : 26.009349822998047, \t Total Dis Loss : 7.600161188747734e-05\n",
      "Steps : 138500, \t Total Gen Loss : 28.846965789794922, \t Total Dis Loss : 5.506006345967762e-05\n",
      "Steps : 138600, \t Total Gen Loss : 32.240909576416016, \t Total Dis Loss : 7.522731175413355e-05\n",
      "Steps : 138700, \t Total Gen Loss : 31.2525691986084, \t Total Dis Loss : 5.839898585691117e-05\n",
      "Steps : 138800, \t Total Gen Loss : 29.107717514038086, \t Total Dis Loss : 0.0002475076471455395\n",
      "Steps : 138900, \t Total Gen Loss : 28.646108627319336, \t Total Dis Loss : 0.0005284418002702296\n",
      "Steps : 139000, \t Total Gen Loss : 26.854427337646484, \t Total Dis Loss : 9.857624536380172e-05\n",
      "Steps : 139100, \t Total Gen Loss : 31.818538665771484, \t Total Dis Loss : 5.2793540817219764e-05\n",
      "Steps : 139200, \t Total Gen Loss : 28.301191329956055, \t Total Dis Loss : 0.00021318913786672056\n",
      "Steps : 139300, \t Total Gen Loss : 28.550201416015625, \t Total Dis Loss : 0.0003167936811223626\n",
      "Steps : 139400, \t Total Gen Loss : 28.140453338623047, \t Total Dis Loss : 0.0002084078878397122\n",
      "Steps : 139500, \t Total Gen Loss : 30.409814834594727, \t Total Dis Loss : 0.0008872855105437338\n",
      "Steps : 139600, \t Total Gen Loss : 32.301239013671875, \t Total Dis Loss : 7.739222201053053e-05\n",
      "Steps : 139700, \t Total Gen Loss : 28.071725845336914, \t Total Dis Loss : 0.00017015439516399056\n",
      "Steps : 139800, \t Total Gen Loss : 29.065826416015625, \t Total Dis Loss : 0.00012104446796001866\n",
      "Steps : 139900, \t Total Gen Loss : 34.127601623535156, \t Total Dis Loss : 3.306366124888882e-05\n",
      "Steps : 140000, \t Total Gen Loss : 32.67139434814453, \t Total Dis Loss : 0.0012214173329994082\n",
      "Steps : 140100, \t Total Gen Loss : 40.503395080566406, \t Total Dis Loss : 4.390009053167887e-05\n",
      "Steps : 140200, \t Total Gen Loss : 31.796173095703125, \t Total Dis Loss : 3.788650064961985e-05\n",
      "Steps : 140300, \t Total Gen Loss : 29.22391128540039, \t Total Dis Loss : 0.00034616212360560894\n",
      "Steps : 140400, \t Total Gen Loss : 26.357418060302734, \t Total Dis Loss : 0.0007989913574419916\n",
      "Steps : 140500, \t Total Gen Loss : 28.2596435546875, \t Total Dis Loss : 0.0002705275546759367\n",
      "Steps : 140600, \t Total Gen Loss : 32.65972137451172, \t Total Dis Loss : 7.978527719387785e-05\n",
      "Time for epoch 25 is 69.12876653671265 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f6070688d50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible type conversion requested to type 'uint8' for variable of type 'float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    927\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4cf47a560719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0man_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0man_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-cc2e7a4b41a9>\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(test_dataset, set_lambda)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-55b8941a211c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0men_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0men_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0men_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-dd5033dbc73b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1104\u001b[0m           call_from_convolution=False)\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[1;32m   2012\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m                            name=name)\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \"'conv2d' Op, not %r.\" % dilations)\n\u001b[1;32m   1014\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_inputs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inputs_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, default_dtype)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;31m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;31m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1236\u001b[0m       raise ValueError(\n\u001b[1;32m   1237\u001b[0m           \u001b[0;34m\"Incompatible type conversion requested to type {!r} for variable \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m           \"of type {!r}\".format(dtype.name, self.dtype.name))\n\u001b[0m\u001b[1;32m   1239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible type conversion requested to type 'uint8' for variable of type 'float32'"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(an_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
