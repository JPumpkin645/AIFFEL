{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 프로젝트는 지금까지의 실습과 동일한 방법으로 CIFAR-10 데이터셋에 대해 진행해 보겠습니다. 여러분들이 만들어야 할 모델은 CIFAR-10의 10가지 클래스 중 개구리 라벨을 이상 데이터로 처리하는 모델입니다. 혹시 개구리가 출현할 경우 이를 감지하여 이상감지 경고를 발생시키는 개구리 감지 모델이라고 할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "- Skip-GANomaly 모델의 구현\n",
    "- 모델의 학습과 검증\n",
    "- 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_backup = train_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pixel: 255\n",
      "min pixel: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"max pixel:\", train_data.max())\n",
    "print(\"min pixel:\", train_data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_data):\n",
    "\n",
    "    normalized_train_data = (train_data - 127.5) / 127.5\n",
    "    \n",
    "    return normalized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pixel: 1.0\n",
      "min pixel: -1.0\n"
     ]
    }
   ],
   "source": [
    "train_data = normalization(train_data)\n",
    "test_data = normalization(test_data)\n",
    "\n",
    "print(\"max pixel:\", train_data.max())\n",
    "print(\"min pixel:\", train_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reshape(train_data.shape[0], 32, 32, 3).astype('float32')\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.reshape(test_data.shape[0], 32, 32, 3).astype('float32')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 시각화하여 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((train_data[0].reshape(32, 32, 3)+1)/2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow((train_data[i].reshape(32, 32, 3)+1)/2)\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개구리 라벨을 이상치로 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   # frog: 6\n",
    "            new_t_labels.append([0])  # Frog 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator가 32X32X3 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 15.525558471679688, \t Total Dis Loss : 0.6883153915405273\n",
      "Steps : 200, \t Total Gen Loss : 19.19690704345703, \t Total Dis Loss : 0.6553553938865662\n",
      "Steps : 300, \t Total Gen Loss : 14.556232452392578, \t Total Dis Loss : 0.5421937704086304\n",
      "Steps : 400, \t Total Gen Loss : 15.659435272216797, \t Total Dis Loss : 0.36897850036621094\n",
      "Steps : 500, \t Total Gen Loss : 13.881799697875977, \t Total Dis Loss : 0.19962096214294434\n",
      "Steps : 600, \t Total Gen Loss : 17.92521095275879, \t Total Dis Loss : 0.283000648021698\n",
      "Steps : 700, \t Total Gen Loss : 17.612777709960938, \t Total Dis Loss : 0.2558635473251343\n",
      "Steps : 800, \t Total Gen Loss : 17.9074764251709, \t Total Dis Loss : 0.1332038938999176\n",
      "Steps : 900, \t Total Gen Loss : 17.125673294067383, \t Total Dis Loss : 0.07405248284339905\n",
      "Steps : 1000, \t Total Gen Loss : 16.448806762695312, \t Total Dis Loss : 0.09978803992271423\n",
      "Steps : 1100, \t Total Gen Loss : 18.467267990112305, \t Total Dis Loss : 0.031266797333955765\n",
      "Steps : 1200, \t Total Gen Loss : 18.079315185546875, \t Total Dis Loss : 0.07406184077262878\n",
      "Steps : 1300, \t Total Gen Loss : 17.561351776123047, \t Total Dis Loss : 0.031972624361515045\n",
      "Steps : 1400, \t Total Gen Loss : 21.144010543823242, \t Total Dis Loss : 0.09513945132493973\n",
      "Steps : 1500, \t Total Gen Loss : 21.22152328491211, \t Total Dis Loss : 0.02698005735874176\n",
      "Steps : 1600, \t Total Gen Loss : 21.103796005249023, \t Total Dis Loss : 0.01845437102019787\n",
      "Steps : 1700, \t Total Gen Loss : 18.557289123535156, \t Total Dis Loss : 0.05200881138443947\n",
      "Steps : 1800, \t Total Gen Loss : 20.487377166748047, \t Total Dis Loss : 0.023521997034549713\n",
      "Steps : 1900, \t Total Gen Loss : 20.087509155273438, \t Total Dis Loss : 0.051509395241737366\n",
      "Steps : 2000, \t Total Gen Loss : 20.059326171875, \t Total Dis Loss : 0.01785845309495926\n",
      "Steps : 2100, \t Total Gen Loss : 17.064029693603516, \t Total Dis Loss : 0.029761359095573425\n",
      "Steps : 2200, \t Total Gen Loss : 21.368946075439453, \t Total Dis Loss : 0.011538689956068993\n",
      "Steps : 2300, \t Total Gen Loss : 18.658815383911133, \t Total Dis Loss : 0.04610016942024231\n",
      "Steps : 2400, \t Total Gen Loss : 22.950366973876953, \t Total Dis Loss : 0.020320197567343712\n",
      "Steps : 2500, \t Total Gen Loss : 19.30218505859375, \t Total Dis Loss : 0.022085629403591156\n",
      "Steps : 2600, \t Total Gen Loss : 20.22827911376953, \t Total Dis Loss : 0.020889416337013245\n",
      "Steps : 2700, \t Total Gen Loss : 22.195858001708984, \t Total Dis Loss : 0.010744920000433922\n",
      "Steps : 2800, \t Total Gen Loss : 20.179616928100586, \t Total Dis Loss : 0.010718983598053455\n",
      "Steps : 2900, \t Total Gen Loss : 20.697919845581055, \t Total Dis Loss : 0.011551029980182648\n",
      "Steps : 3000, \t Total Gen Loss : 21.635765075683594, \t Total Dis Loss : 0.009873914532363415\n",
      "Steps : 3100, \t Total Gen Loss : 19.46670913696289, \t Total Dis Loss : 0.010003815405070782\n",
      "Steps : 3200, \t Total Gen Loss : 22.70451545715332, \t Total Dis Loss : 0.005578628741204739\n",
      "Steps : 3300, \t Total Gen Loss : 22.524259567260742, \t Total Dis Loss : 0.007728898897767067\n",
      "Steps : 3400, \t Total Gen Loss : 20.290056228637695, \t Total Dis Loss : 0.029288340359926224\n",
      "Steps : 3500, \t Total Gen Loss : 21.510744094848633, \t Total Dis Loss : 0.011577727273106575\n",
      "Steps : 3600, \t Total Gen Loss : 19.91161346435547, \t Total Dis Loss : 0.022895609959959984\n",
      "Steps : 3700, \t Total Gen Loss : 22.29450035095215, \t Total Dis Loss : 0.006626916117966175\n",
      "Steps : 3800, \t Total Gen Loss : 21.190792083740234, \t Total Dis Loss : 0.006640903651714325\n",
      "Steps : 3900, \t Total Gen Loss : 22.303874969482422, \t Total Dis Loss : 0.009090254083275795\n",
      "Steps : 4000, \t Total Gen Loss : 22.820262908935547, \t Total Dis Loss : 0.04068842530250549\n",
      "Steps : 4100, \t Total Gen Loss : 21.467676162719727, \t Total Dis Loss : 0.00644873408600688\n",
      "Steps : 4200, \t Total Gen Loss : 21.731361389160156, \t Total Dis Loss : 0.02359858900308609\n",
      "Steps : 4300, \t Total Gen Loss : 23.65615463256836, \t Total Dis Loss : 0.006534276530146599\n",
      "Steps : 4400, \t Total Gen Loss : 22.62643814086914, \t Total Dis Loss : 0.009717699140310287\n",
      "Steps : 4500, \t Total Gen Loss : 21.66924476623535, \t Total Dis Loss : 0.009447968564927578\n",
      "Steps : 4600, \t Total Gen Loss : 21.428001403808594, \t Total Dis Loss : 0.0082094706594944\n",
      "Steps : 4700, \t Total Gen Loss : 22.41855812072754, \t Total Dis Loss : 0.005641179159283638\n",
      "Steps : 4800, \t Total Gen Loss : 18.530580520629883, \t Total Dis Loss : 0.005314870737493038\n",
      "Steps : 4900, \t Total Gen Loss : 22.350488662719727, \t Total Dis Loss : 0.004588641691952944\n",
      "Steps : 5000, \t Total Gen Loss : 20.608549118041992, \t Total Dis Loss : 0.0018065022304654121\n",
      "Steps : 5100, \t Total Gen Loss : 21.103618621826172, \t Total Dis Loss : 0.007784455083310604\n",
      "Steps : 5200, \t Total Gen Loss : 21.868404388427734, \t Total Dis Loss : 0.005456489510834217\n",
      "Steps : 5300, \t Total Gen Loss : 22.61174774169922, \t Total Dis Loss : 0.0026680363807827234\n",
      "Steps : 5400, \t Total Gen Loss : 20.2894287109375, \t Total Dis Loss : 0.006129009649157524\n",
      "Steps : 5500, \t Total Gen Loss : 18.917362213134766, \t Total Dis Loss : 0.0035160339903086424\n",
      "Steps : 5600, \t Total Gen Loss : 24.0867862701416, \t Total Dis Loss : 0.0021269414573907852\n",
      "Time for epoch 1 is 80.37240839004517 sec\n",
      "Steps : 5700, \t Total Gen Loss : 20.7424373626709, \t Total Dis Loss : 0.0034906193614006042\n",
      "Steps : 5800, \t Total Gen Loss : 24.230937957763672, \t Total Dis Loss : 0.004523689858615398\n",
      "Steps : 5900, \t Total Gen Loss : 21.430267333984375, \t Total Dis Loss : 0.007424077484756708\n",
      "Steps : 6000, \t Total Gen Loss : 24.03911018371582, \t Total Dis Loss : 0.0017793945735320449\n",
      "Steps : 6100, \t Total Gen Loss : 18.783674240112305, \t Total Dis Loss : 0.13606198132038116\n",
      "Steps : 6200, \t Total Gen Loss : 22.64641571044922, \t Total Dis Loss : 0.004785751923918724\n",
      "Steps : 6300, \t Total Gen Loss : 22.998397827148438, \t Total Dis Loss : 0.002625809982419014\n",
      "Steps : 6400, \t Total Gen Loss : 21.39643096923828, \t Total Dis Loss : 0.013395562767982483\n",
      "Steps : 6500, \t Total Gen Loss : 24.46670913696289, \t Total Dis Loss : 0.0013525206595659256\n",
      "Steps : 6600, \t Total Gen Loss : 21.848922729492188, \t Total Dis Loss : 0.014880897477269173\n",
      "Steps : 6700, \t Total Gen Loss : 21.486553192138672, \t Total Dis Loss : 0.024730224162340164\n",
      "Steps : 6800, \t Total Gen Loss : 22.43441390991211, \t Total Dis Loss : 0.003836464136838913\n",
      "Steps : 6900, \t Total Gen Loss : 23.862409591674805, \t Total Dis Loss : 0.0016342962626367807\n",
      "Steps : 7000, \t Total Gen Loss : 25.684412002563477, \t Total Dis Loss : 0.004130060784518719\n",
      "Steps : 7100, \t Total Gen Loss : 24.437679290771484, \t Total Dis Loss : 0.007367649581283331\n",
      "Steps : 7200, \t Total Gen Loss : 24.899967193603516, \t Total Dis Loss : 0.0019157282076776028\n",
      "Steps : 7300, \t Total Gen Loss : 19.89518165588379, \t Total Dis Loss : 0.012997348792850971\n",
      "Steps : 7400, \t Total Gen Loss : 23.255229949951172, \t Total Dis Loss : 0.006120313890278339\n",
      "Steps : 7500, \t Total Gen Loss : 20.851947784423828, \t Total Dis Loss : 0.006426381878554821\n",
      "Steps : 7600, \t Total Gen Loss : 24.0051326751709, \t Total Dis Loss : 0.0035097720101475716\n",
      "Steps : 7700, \t Total Gen Loss : 23.951587677001953, \t Total Dis Loss : 0.002484638476744294\n",
      "Steps : 7800, \t Total Gen Loss : 24.535564422607422, \t Total Dis Loss : 0.0015949468361213803\n",
      "Steps : 7900, \t Total Gen Loss : 24.03691864013672, \t Total Dis Loss : 0.002708476036787033\n",
      "Steps : 8000, \t Total Gen Loss : 23.85712432861328, \t Total Dis Loss : 0.0011192281963303685\n",
      "Steps : 8100, \t Total Gen Loss : 21.941173553466797, \t Total Dis Loss : 0.0010765886399894953\n",
      "Steps : 8200, \t Total Gen Loss : 23.839374542236328, \t Total Dis Loss : 0.0006245350232347846\n",
      "Steps : 8300, \t Total Gen Loss : 21.460834503173828, \t Total Dis Loss : 0.0012920494191348553\n",
      "Steps : 8400, \t Total Gen Loss : 26.192638397216797, \t Total Dis Loss : 0.000745082797948271\n",
      "Steps : 8500, \t Total Gen Loss : 23.68695068359375, \t Total Dis Loss : 0.0010456969030201435\n",
      "Steps : 8600, \t Total Gen Loss : 20.88137435913086, \t Total Dis Loss : 0.016710108146071434\n",
      "Steps : 8700, \t Total Gen Loss : 20.624643325805664, \t Total Dis Loss : 0.0015763918636366725\n",
      "Steps : 8800, \t Total Gen Loss : 23.948116302490234, \t Total Dis Loss : 0.004710372071713209\n",
      "Steps : 8900, \t Total Gen Loss : 22.905494689941406, \t Total Dis Loss : 0.006592330988496542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9000, \t Total Gen Loss : 23.982154846191406, \t Total Dis Loss : 0.0006542904884554446\n",
      "Steps : 9100, \t Total Gen Loss : 23.592823028564453, \t Total Dis Loss : 0.0010842910269275308\n",
      "Steps : 9200, \t Total Gen Loss : 22.56211280822754, \t Total Dis Loss : 0.1676875799894333\n",
      "Steps : 9300, \t Total Gen Loss : 22.050151824951172, \t Total Dis Loss : 0.0037762350402772427\n",
      "Steps : 9400, \t Total Gen Loss : 28.154659271240234, \t Total Dis Loss : 0.0007089936407282948\n",
      "Steps : 9500, \t Total Gen Loss : 28.27748680114746, \t Total Dis Loss : 0.001583893783390522\n",
      "Steps : 9600, \t Total Gen Loss : 24.54677963256836, \t Total Dis Loss : 0.0005513500655069947\n",
      "Steps : 9700, \t Total Gen Loss : 23.33477783203125, \t Total Dis Loss : 0.00043200384243391454\n",
      "Steps : 9800, \t Total Gen Loss : 27.725688934326172, \t Total Dis Loss : 0.0027792928740382195\n",
      "Steps : 9900, \t Total Gen Loss : 26.826791763305664, \t Total Dis Loss : 0.0005860623205080628\n",
      "Steps : 10000, \t Total Gen Loss : 23.331329345703125, \t Total Dis Loss : 0.005651244428008795\n",
      "Steps : 10100, \t Total Gen Loss : 25.393770217895508, \t Total Dis Loss : 0.006796755362302065\n",
      "Steps : 10200, \t Total Gen Loss : 18.489267349243164, \t Total Dis Loss : 0.4260396361351013\n",
      "Steps : 10300, \t Total Gen Loss : 24.644359588623047, \t Total Dis Loss : 0.0007535810582339764\n",
      "Steps : 10400, \t Total Gen Loss : 25.304157257080078, \t Total Dis Loss : 0.004578125663101673\n",
      "Steps : 10500, \t Total Gen Loss : 24.5623779296875, \t Total Dis Loss : 0.001196861732751131\n",
      "Steps : 10600, \t Total Gen Loss : 25.056602478027344, \t Total Dis Loss : 0.0011497421655803919\n",
      "Steps : 10700, \t Total Gen Loss : 23.548458099365234, \t Total Dis Loss : 0.005390554666519165\n",
      "Steps : 10800, \t Total Gen Loss : 26.391170501708984, \t Total Dis Loss : 0.011546829715371132\n",
      "Steps : 10900, \t Total Gen Loss : 21.04414176940918, \t Total Dis Loss : 0.00900993775576353\n",
      "Steps : 11000, \t Total Gen Loss : 20.00751495361328, \t Total Dis Loss : 0.006132925860583782\n",
      "Steps : 11100, \t Total Gen Loss : 24.532962799072266, \t Total Dis Loss : 0.0016064243391156197\n",
      "Steps : 11200, \t Total Gen Loss : 22.772201538085938, \t Total Dis Loss : 0.0045632775872945786\n",
      "Time for epoch 2 is 78.85125684738159 sec\n",
      "Steps : 11300, \t Total Gen Loss : 23.71903419494629, \t Total Dis Loss : 0.004651137627661228\n",
      "Steps : 11400, \t Total Gen Loss : 21.124244689941406, \t Total Dis Loss : 0.013123770244419575\n",
      "Steps : 11500, \t Total Gen Loss : 23.323848724365234, \t Total Dis Loss : 0.004804017022252083\n",
      "Steps : 11600, \t Total Gen Loss : 23.275691986083984, \t Total Dis Loss : 0.0018850703490898013\n",
      "Steps : 11700, \t Total Gen Loss : 21.25810432434082, \t Total Dis Loss : 0.004463632591068745\n",
      "Steps : 11800, \t Total Gen Loss : 19.863122940063477, \t Total Dis Loss : 0.002834377344697714\n",
      "Steps : 11900, \t Total Gen Loss : 25.73662567138672, \t Total Dis Loss : 0.0007277092663571239\n",
      "Steps : 12000, \t Total Gen Loss : 22.02977752685547, \t Total Dis Loss : 0.0025228015147149563\n",
      "Steps : 12100, \t Total Gen Loss : 23.3724365234375, \t Total Dis Loss : 0.0014334394363686442\n",
      "Steps : 12200, \t Total Gen Loss : 25.004505157470703, \t Total Dis Loss : 0.0015012676594778895\n",
      "Steps : 12300, \t Total Gen Loss : 25.56972885131836, \t Total Dis Loss : 0.0019035842269659042\n",
      "Steps : 12400, \t Total Gen Loss : 25.15712547302246, \t Total Dis Loss : 0.0009784866124391556\n",
      "Steps : 12500, \t Total Gen Loss : 23.392471313476562, \t Total Dis Loss : 0.007033398374915123\n",
      "Steps : 12600, \t Total Gen Loss : 22.921812057495117, \t Total Dis Loss : 0.0013758030254393816\n",
      "Steps : 12700, \t Total Gen Loss : 22.206130981445312, \t Total Dis Loss : 0.0006196409231051803\n",
      "Steps : 12800, \t Total Gen Loss : 26.08359146118164, \t Total Dis Loss : 0.00040741098928265274\n",
      "Steps : 12900, \t Total Gen Loss : 22.1800479888916, \t Total Dis Loss : 0.001435492536984384\n",
      "Steps : 13000, \t Total Gen Loss : 21.076961517333984, \t Total Dis Loss : 0.003688808297738433\n",
      "Steps : 13100, \t Total Gen Loss : 23.38787841796875, \t Total Dis Loss : 0.0054449462331831455\n",
      "Steps : 13200, \t Total Gen Loss : 23.004669189453125, \t Total Dis Loss : 0.0031408327631652355\n",
      "Steps : 13300, \t Total Gen Loss : 22.614124298095703, \t Total Dis Loss : 0.16172882914543152\n",
      "Steps : 13400, \t Total Gen Loss : 21.474239349365234, \t Total Dis Loss : 0.000562615052331239\n",
      "Steps : 13500, \t Total Gen Loss : 23.561830520629883, \t Total Dis Loss : 0.0003981413901783526\n",
      "Steps : 13600, \t Total Gen Loss : 23.56639862060547, \t Total Dis Loss : 0.001876315800473094\n",
      "Steps : 13700, \t Total Gen Loss : 23.525554656982422, \t Total Dis Loss : 0.0011486804578453302\n",
      "Steps : 13800, \t Total Gen Loss : 23.625762939453125, \t Total Dis Loss : 0.0012759420787915587\n",
      "Steps : 13900, \t Total Gen Loss : 25.806032180786133, \t Total Dis Loss : 0.00029186811298131943\n",
      "Steps : 14000, \t Total Gen Loss : 22.790760040283203, \t Total Dis Loss : 0.006066204980015755\n",
      "Steps : 14100, \t Total Gen Loss : 25.01055145263672, \t Total Dis Loss : 0.005971730221062899\n",
      "Steps : 14200, \t Total Gen Loss : 24.042526245117188, \t Total Dis Loss : 0.0011003584368154407\n",
      "Steps : 14300, \t Total Gen Loss : 23.099088668823242, \t Total Dis Loss : 0.0021792154293507338\n",
      "Steps : 14400, \t Total Gen Loss : 24.176761627197266, \t Total Dis Loss : 0.00045609616790898144\n",
      "Steps : 14500, \t Total Gen Loss : 27.354610443115234, \t Total Dis Loss : 0.00043475936399772763\n",
      "Steps : 14600, \t Total Gen Loss : 22.670791625976562, \t Total Dis Loss : 0.00079354178160429\n",
      "Steps : 14700, \t Total Gen Loss : 23.271535873413086, \t Total Dis Loss : 0.0022484951186925173\n",
      "Steps : 14800, \t Total Gen Loss : 24.244930267333984, \t Total Dis Loss : 0.00024646837846376\n",
      "Steps : 14900, \t Total Gen Loss : 22.638195037841797, \t Total Dis Loss : 0.001466631074436009\n",
      "Steps : 15000, \t Total Gen Loss : 21.57134246826172, \t Total Dis Loss : 0.0007515486213378608\n",
      "Steps : 15100, \t Total Gen Loss : 24.214731216430664, \t Total Dis Loss : 0.001234692637808621\n",
      "Steps : 15200, \t Total Gen Loss : 24.61121368408203, \t Total Dis Loss : 0.0014920898247510195\n",
      "Steps : 15300, \t Total Gen Loss : 23.54134178161621, \t Total Dis Loss : 0.0011008717119693756\n",
      "Steps : 15400, \t Total Gen Loss : 21.239662170410156, \t Total Dis Loss : 0.0017599092097952962\n",
      "Steps : 15500, \t Total Gen Loss : 20.777976989746094, \t Total Dis Loss : 0.0009537166333757341\n",
      "Steps : 15600, \t Total Gen Loss : 25.0996150970459, \t Total Dis Loss : 0.0005558115663006902\n",
      "Steps : 15700, \t Total Gen Loss : 21.84414291381836, \t Total Dis Loss : 0.0014841469237580895\n",
      "Steps : 15800, \t Total Gen Loss : 25.0175724029541, \t Total Dis Loss : 0.0009571524569764733\n",
      "Steps : 15900, \t Total Gen Loss : 19.301795959472656, \t Total Dis Loss : 0.004765467252582312\n",
      "Steps : 16000, \t Total Gen Loss : 21.64722442626953, \t Total Dis Loss : 0.0009747008443810046\n",
      "Steps : 16100, \t Total Gen Loss : 22.859634399414062, \t Total Dis Loss : 0.001072712941095233\n",
      "Steps : 16200, \t Total Gen Loss : 22.7253360748291, \t Total Dis Loss : 0.0005861357785761356\n",
      "Steps : 16300, \t Total Gen Loss : 22.193927764892578, \t Total Dis Loss : 0.00034419650910422206\n",
      "Steps : 16400, \t Total Gen Loss : 22.721107482910156, \t Total Dis Loss : 0.03562594950199127\n",
      "Steps : 16500, \t Total Gen Loss : 27.832670211791992, \t Total Dis Loss : 0.0006753577617928386\n",
      "Steps : 16600, \t Total Gen Loss : 27.553565979003906, \t Total Dis Loss : 0.00023489430896006525\n",
      "Steps : 16700, \t Total Gen Loss : 26.79435157775879, \t Total Dis Loss : 0.038127392530441284\n",
      "Steps : 16800, \t Total Gen Loss : 25.532434463500977, \t Total Dis Loss : 0.00013544067041948438\n",
      "Time for epoch 3 is 76.06804037094116 sec\n",
      "Steps : 16900, \t Total Gen Loss : 26.01997184753418, \t Total Dis Loss : 0.00039613479748368263\n",
      "Steps : 17000, \t Total Gen Loss : 24.402921676635742, \t Total Dis Loss : 0.000573705998249352\n",
      "Steps : 17100, \t Total Gen Loss : 24.750423431396484, \t Total Dis Loss : 0.0006721725221723318\n",
      "Steps : 17200, \t Total Gen Loss : 23.871923446655273, \t Total Dis Loss : 0.000864534405991435\n",
      "Steps : 17300, \t Total Gen Loss : 25.6491641998291, \t Total Dis Loss : 0.0005198766011744738\n",
      "Steps : 17400, \t Total Gen Loss : 27.7357177734375, \t Total Dis Loss : 0.0011078454554080963\n",
      "Steps : 17500, \t Total Gen Loss : 23.108196258544922, \t Total Dis Loss : 0.001989430282264948\n",
      "Steps : 17600, \t Total Gen Loss : 30.304080963134766, \t Total Dis Loss : 0.0005680685280822217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17700, \t Total Gen Loss : 29.92558479309082, \t Total Dis Loss : 0.01049258653074503\n",
      "Steps : 17800, \t Total Gen Loss : 24.338756561279297, \t Total Dis Loss : 0.0014922600239515305\n",
      "Steps : 17900, \t Total Gen Loss : 18.923095703125, \t Total Dis Loss : 0.027318434789776802\n",
      "Steps : 18000, \t Total Gen Loss : 24.17660140991211, \t Total Dis Loss : 0.0014223019825294614\n",
      "Steps : 18100, \t Total Gen Loss : 23.9598331451416, \t Total Dis Loss : 0.0011182402959093451\n",
      "Steps : 18200, \t Total Gen Loss : 22.39436912536621, \t Total Dis Loss : 0.0051276409067213535\n",
      "Steps : 18300, \t Total Gen Loss : 21.799457550048828, \t Total Dis Loss : 0.008304932154715061\n",
      "Steps : 18400, \t Total Gen Loss : 21.02541732788086, \t Total Dis Loss : 0.0036064826417714357\n",
      "Steps : 18500, \t Total Gen Loss : 22.293292999267578, \t Total Dis Loss : 0.0015456646215170622\n",
      "Steps : 18600, \t Total Gen Loss : 21.701744079589844, \t Total Dis Loss : 0.0013315487885847688\n",
      "Steps : 18700, \t Total Gen Loss : 21.803848266601562, \t Total Dis Loss : 0.0014762482605874538\n",
      "Steps : 18800, \t Total Gen Loss : 23.5369815826416, \t Total Dis Loss : 0.00038628827314823866\n",
      "Steps : 18900, \t Total Gen Loss : 19.030704498291016, \t Total Dis Loss : 0.008632596582174301\n",
      "Steps : 19000, \t Total Gen Loss : 20.7546443939209, \t Total Dis Loss : 0.0009833022486418486\n",
      "Steps : 19100, \t Total Gen Loss : 21.21489906311035, \t Total Dis Loss : 0.007599723059684038\n",
      "Steps : 19200, \t Total Gen Loss : 25.076250076293945, \t Total Dis Loss : 0.0009945138590410352\n",
      "Steps : 19300, \t Total Gen Loss : 21.32193946838379, \t Total Dis Loss : 0.0074621448293328285\n",
      "Steps : 19400, \t Total Gen Loss : 21.696849822998047, \t Total Dis Loss : 0.0016218892997130752\n",
      "Steps : 19500, \t Total Gen Loss : 20.21525001525879, \t Total Dis Loss : 0.0062834834679961205\n",
      "Steps : 19600, \t Total Gen Loss : 22.32575225830078, \t Total Dis Loss : 0.001965827774256468\n",
      "Steps : 19700, \t Total Gen Loss : 22.569000244140625, \t Total Dis Loss : 0.0010387402726337314\n",
      "Steps : 19800, \t Total Gen Loss : 26.18975257873535, \t Total Dis Loss : 0.021149050444364548\n",
      "Steps : 19900, \t Total Gen Loss : 23.34923553466797, \t Total Dis Loss : 0.0002872307086363435\n",
      "Steps : 20000, \t Total Gen Loss : 25.036251068115234, \t Total Dis Loss : 0.0010669557377696037\n",
      "Steps : 20100, \t Total Gen Loss : 21.71506118774414, \t Total Dis Loss : 0.0019712711218744516\n",
      "Steps : 20200, \t Total Gen Loss : 22.276226043701172, \t Total Dis Loss : 0.002879363251850009\n",
      "Steps : 20300, \t Total Gen Loss : 20.878623962402344, \t Total Dis Loss : 0.0009459350840188563\n",
      "Steps : 20400, \t Total Gen Loss : 22.21234893798828, \t Total Dis Loss : 0.0014951892662793398\n",
      "Steps : 20500, \t Total Gen Loss : 22.15765380859375, \t Total Dis Loss : 0.0006225307006388903\n",
      "Steps : 20600, \t Total Gen Loss : 27.254928588867188, \t Total Dis Loss : 0.0004631654592230916\n",
      "Steps : 20700, \t Total Gen Loss : 23.50593376159668, \t Total Dis Loss : 0.0015415148809552193\n",
      "Steps : 20800, \t Total Gen Loss : 24.292695999145508, \t Total Dis Loss : 0.001833694870583713\n",
      "Steps : 20900, \t Total Gen Loss : 26.243637084960938, \t Total Dis Loss : 0.006184753496199846\n",
      "Steps : 21000, \t Total Gen Loss : 22.376928329467773, \t Total Dis Loss : 0.008819852024316788\n",
      "Steps : 21100, \t Total Gen Loss : 25.16971206665039, \t Total Dis Loss : 0.0010049112606793642\n",
      "Steps : 21200, \t Total Gen Loss : 22.09185028076172, \t Total Dis Loss : 0.0031597113702446222\n",
      "Steps : 21300, \t Total Gen Loss : 22.696279525756836, \t Total Dis Loss : 0.0012826659949496388\n",
      "Steps : 21400, \t Total Gen Loss : 24.698902130126953, \t Total Dis Loss : 0.0010350118391215801\n",
      "Steps : 21500, \t Total Gen Loss : 29.45965576171875, \t Total Dis Loss : 0.0024849986657500267\n",
      "Steps : 21600, \t Total Gen Loss : 25.325122833251953, \t Total Dis Loss : 0.0009028248605318367\n",
      "Steps : 21700, \t Total Gen Loss : 22.594383239746094, \t Total Dis Loss : 0.0013801848981529474\n",
      "Steps : 21800, \t Total Gen Loss : 20.647602081298828, \t Total Dis Loss : 0.008436750620603561\n",
      "Steps : 21900, \t Total Gen Loss : 23.57933807373047, \t Total Dis Loss : 0.0017215722473338246\n",
      "Steps : 22000, \t Total Gen Loss : 24.23577308654785, \t Total Dis Loss : 0.0025924418587237597\n",
      "Steps : 22100, \t Total Gen Loss : 24.226163864135742, \t Total Dis Loss : 0.0023241392336785793\n",
      "Steps : 22200, \t Total Gen Loss : 23.238243103027344, \t Total Dis Loss : 0.00112869159784168\n",
      "Steps : 22300, \t Total Gen Loss : 21.725509643554688, \t Total Dis Loss : 0.0009978932794183493\n",
      "Steps : 22400, \t Total Gen Loss : 26.326820373535156, \t Total Dis Loss : 0.0018159367609769106\n",
      "Steps : 22500, \t Total Gen Loss : 24.027936935424805, \t Total Dis Loss : 0.005612621083855629\n",
      "Time for epoch 4 is 76.49585056304932 sec\n",
      "Steps : 22600, \t Total Gen Loss : 25.218791961669922, \t Total Dis Loss : 0.0014511478366330266\n",
      "Steps : 22700, \t Total Gen Loss : 28.42181396484375, \t Total Dis Loss : 0.0007431322010233998\n",
      "Steps : 22800, \t Total Gen Loss : 26.159317016601562, \t Total Dis Loss : 0.0011819328647106886\n",
      "Steps : 22900, \t Total Gen Loss : 24.045568466186523, \t Total Dis Loss : 0.0027904685121029615\n",
      "Steps : 23000, \t Total Gen Loss : 26.736642837524414, \t Total Dis Loss : 0.0006698978831991553\n",
      "Steps : 23100, \t Total Gen Loss : 22.028793334960938, \t Total Dis Loss : 0.0012398993130773306\n",
      "Steps : 23200, \t Total Gen Loss : 26.293453216552734, \t Total Dis Loss : 0.0006687983404844999\n",
      "Steps : 23300, \t Total Gen Loss : 25.783910751342773, \t Total Dis Loss : 0.0007778095314279199\n",
      "Steps : 23400, \t Total Gen Loss : 21.05321502685547, \t Total Dis Loss : 0.0010493617737665772\n",
      "Steps : 23500, \t Total Gen Loss : 22.756607055664062, \t Total Dis Loss : 0.0008810792933218181\n",
      "Steps : 23600, \t Total Gen Loss : 21.243484497070312, \t Total Dis Loss : 0.0036193374544382095\n",
      "Steps : 23700, \t Total Gen Loss : 22.658252716064453, \t Total Dis Loss : 0.0007437460590153933\n",
      "Steps : 23800, \t Total Gen Loss : 22.231185913085938, \t Total Dis Loss : 0.0008092704229056835\n",
      "Steps : 23900, \t Total Gen Loss : 24.03871726989746, \t Total Dis Loss : 0.0005470688920468092\n",
      "Steps : 24000, \t Total Gen Loss : 21.90801429748535, \t Total Dis Loss : 0.0021769560407847166\n",
      "Steps : 24100, \t Total Gen Loss : 20.479095458984375, \t Total Dis Loss : 0.0007189149037003517\n",
      "Steps : 24200, \t Total Gen Loss : 25.298526763916016, \t Total Dis Loss : 0.00029654812533408403\n",
      "Steps : 24300, \t Total Gen Loss : 24.18979835510254, \t Total Dis Loss : 0.0005078675458207726\n",
      "Steps : 24400, \t Total Gen Loss : 23.98409652709961, \t Total Dis Loss : 0.0003490883391350508\n",
      "Steps : 24500, \t Total Gen Loss : 24.602548599243164, \t Total Dis Loss : 0.01340913400053978\n",
      "Steps : 24600, \t Total Gen Loss : 24.421558380126953, \t Total Dis Loss : 0.0005570743232965469\n",
      "Steps : 24700, \t Total Gen Loss : 24.026527404785156, \t Total Dis Loss : 0.0005769010167568922\n",
      "Steps : 24800, \t Total Gen Loss : 24.371267318725586, \t Total Dis Loss : 0.001334062428213656\n",
      "Steps : 24900, \t Total Gen Loss : 24.87978744506836, \t Total Dis Loss : 0.0004083818639628589\n",
      "Steps : 25000, \t Total Gen Loss : 25.704086303710938, \t Total Dis Loss : 0.00026446685660630465\n",
      "Steps : 25100, \t Total Gen Loss : 24.605703353881836, \t Total Dis Loss : 0.0010598303051665425\n",
      "Steps : 25200, \t Total Gen Loss : 23.54186248779297, \t Total Dis Loss : 0.0016265352023765445\n",
      "Steps : 25300, \t Total Gen Loss : 22.457042694091797, \t Total Dis Loss : 0.000777576700784266\n",
      "Steps : 25400, \t Total Gen Loss : 23.024316787719727, \t Total Dis Loss : 0.0006820610724389553\n",
      "Steps : 25500, \t Total Gen Loss : 24.930179595947266, \t Total Dis Loss : 0.0013768697390332818\n",
      "Steps : 25600, \t Total Gen Loss : 25.457319259643555, \t Total Dis Loss : 0.0008556854445487261\n",
      "Steps : 25700, \t Total Gen Loss : 20.522653579711914, \t Total Dis Loss : 0.002029500203207135\n",
      "Steps : 25800, \t Total Gen Loss : 24.250743865966797, \t Total Dis Loss : 0.0004811593098565936\n",
      "Steps : 25900, \t Total Gen Loss : 21.868690490722656, \t Total Dis Loss : 0.0019726124592125416\n",
      "Steps : 26000, \t Total Gen Loss : 24.271018981933594, \t Total Dis Loss : 0.0008923240238800645\n",
      "Steps : 26100, \t Total Gen Loss : 22.56134796142578, \t Total Dis Loss : 0.0004795244603883475\n",
      "Steps : 26200, \t Total Gen Loss : 21.268165588378906, \t Total Dis Loss : 0.00038779081660322845\n",
      "Steps : 26300, \t Total Gen Loss : 25.10601043701172, \t Total Dis Loss : 0.0005475755897350609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26400, \t Total Gen Loss : 24.21955108642578, \t Total Dis Loss : 0.0006037604180164635\n",
      "Steps : 26500, \t Total Gen Loss : 24.7957706451416, \t Total Dis Loss : 0.0006024367175996304\n",
      "Steps : 26600, \t Total Gen Loss : 23.6038875579834, \t Total Dis Loss : 0.0009787131566554308\n",
      "Steps : 26700, \t Total Gen Loss : 24.79753875732422, \t Total Dis Loss : 0.0003452589735388756\n",
      "Steps : 26800, \t Total Gen Loss : 24.062835693359375, \t Total Dis Loss : 0.00022695292136631906\n",
      "Steps : 26900, \t Total Gen Loss : 21.910572052001953, \t Total Dis Loss : 0.0009151857811957598\n",
      "Steps : 27000, \t Total Gen Loss : 22.81311798095703, \t Total Dis Loss : 0.0002807255950756371\n",
      "Steps : 27100, \t Total Gen Loss : 23.758159637451172, \t Total Dis Loss : 0.0025926795788109303\n",
      "Steps : 27200, \t Total Gen Loss : 23.418914794921875, \t Total Dis Loss : 0.0008764040539972484\n",
      "Steps : 27300, \t Total Gen Loss : 22.468143463134766, \t Total Dis Loss : 0.012409024871885777\n",
      "Steps : 27400, \t Total Gen Loss : 25.08653450012207, \t Total Dis Loss : 0.007836932316422462\n",
      "Steps : 27500, \t Total Gen Loss : 23.757705688476562, \t Total Dis Loss : 0.0004473401349969208\n",
      "Steps : 27600, \t Total Gen Loss : 22.996417999267578, \t Total Dis Loss : 0.004243229981511831\n",
      "Steps : 27700, \t Total Gen Loss : 21.962024688720703, \t Total Dis Loss : 0.001648311852477491\n",
      "Steps : 27800, \t Total Gen Loss : 24.767486572265625, \t Total Dis Loss : 0.0021137886215001345\n",
      "Steps : 27900, \t Total Gen Loss : 22.555999755859375, \t Total Dis Loss : 0.0027589108794927597\n",
      "Steps : 28000, \t Total Gen Loss : 22.571086883544922, \t Total Dis Loss : 0.0007806072826497257\n",
      "Steps : 28100, \t Total Gen Loss : 26.299427032470703, \t Total Dis Loss : 0.0007998530636541545\n",
      "Time for epoch 5 is 77.7085485458374 sec\n",
      "Steps : 28200, \t Total Gen Loss : 21.971532821655273, \t Total Dis Loss : 0.0003865867911372334\n",
      "Steps : 28300, \t Total Gen Loss : 24.736413955688477, \t Total Dis Loss : 0.0005541710997931659\n",
      "Steps : 28400, \t Total Gen Loss : 23.2811336517334, \t Total Dis Loss : 0.000504843657836318\n",
      "Steps : 28500, \t Total Gen Loss : 22.96661376953125, \t Total Dis Loss : 0.0007861804333515465\n",
      "Steps : 28600, \t Total Gen Loss : 24.857555389404297, \t Total Dis Loss : 0.0003350560727994889\n",
      "Steps : 28700, \t Total Gen Loss : 25.79339599609375, \t Total Dis Loss : 0.003866448299959302\n",
      "Steps : 28800, \t Total Gen Loss : 24.69171905517578, \t Total Dis Loss : 0.0011815307661890984\n",
      "Steps : 28900, \t Total Gen Loss : 25.473682403564453, \t Total Dis Loss : 0.000218351895455271\n",
      "Steps : 29000, \t Total Gen Loss : 25.170129776000977, \t Total Dis Loss : 0.00012361255357973278\n",
      "Steps : 29100, \t Total Gen Loss : 23.036964416503906, \t Total Dis Loss : 0.0019034065771847963\n",
      "Steps : 29200, \t Total Gen Loss : 23.971904754638672, \t Total Dis Loss : 0.00022081533097662032\n",
      "Steps : 29300, \t Total Gen Loss : 23.89559555053711, \t Total Dis Loss : 0.0004397595184855163\n",
      "Steps : 29400, \t Total Gen Loss : 26.456031799316406, \t Total Dis Loss : 0.0002799075737129897\n",
      "Steps : 29500, \t Total Gen Loss : 25.493812561035156, \t Total Dis Loss : 0.001408176263794303\n",
      "Steps : 29600, \t Total Gen Loss : 22.413190841674805, \t Total Dis Loss : 0.0026106522418558598\n",
      "Steps : 29700, \t Total Gen Loss : 25.96558380126953, \t Total Dis Loss : 0.0008053358178585768\n",
      "Steps : 29800, \t Total Gen Loss : 22.593791961669922, \t Total Dis Loss : 0.004763164557516575\n",
      "Steps : 29900, \t Total Gen Loss : 24.822208404541016, \t Total Dis Loss : 0.0016929993871599436\n",
      "Steps : 30000, \t Total Gen Loss : 23.603946685791016, \t Total Dis Loss : 0.0006775731453672051\n",
      "Steps : 30100, \t Total Gen Loss : 23.984264373779297, \t Total Dis Loss : 0.0005988693446852267\n",
      "Steps : 30200, \t Total Gen Loss : 22.021360397338867, \t Total Dis Loss : 0.0013958190102130175\n",
      "Steps : 30300, \t Total Gen Loss : 26.09217071533203, \t Total Dis Loss : 0.0004202906566206366\n",
      "Steps : 30400, \t Total Gen Loss : 23.100696563720703, \t Total Dis Loss : 0.000484806252643466\n",
      "Steps : 30500, \t Total Gen Loss : 26.63890266418457, \t Total Dis Loss : 0.0002841937530320138\n",
      "Steps : 30600, \t Total Gen Loss : 22.331226348876953, \t Total Dis Loss : 0.0005116137908771634\n",
      "Steps : 30700, \t Total Gen Loss : 24.680435180664062, \t Total Dis Loss : 0.0001233526272699237\n",
      "Steps : 30800, \t Total Gen Loss : 21.267559051513672, \t Total Dis Loss : 0.0015113899717107415\n",
      "Steps : 30900, \t Total Gen Loss : 26.980897903442383, \t Total Dis Loss : 0.0010583687108010054\n",
      "Steps : 31000, \t Total Gen Loss : 24.307126998901367, \t Total Dis Loss : 0.0008294827421195805\n",
      "Steps : 31100, \t Total Gen Loss : 25.04546546936035, \t Total Dis Loss : 0.06157180666923523\n",
      "Steps : 31200, \t Total Gen Loss : 22.47130584716797, \t Total Dis Loss : 0.0016553838504478335\n",
      "Steps : 31300, \t Total Gen Loss : 31.964326858520508, \t Total Dis Loss : 0.004016149323433638\n",
      "Steps : 31400, \t Total Gen Loss : 25.898712158203125, \t Total Dis Loss : 0.0005719706532545388\n",
      "Steps : 31500, \t Total Gen Loss : 25.52683448791504, \t Total Dis Loss : 0.0007479538908228278\n",
      "Steps : 31600, \t Total Gen Loss : 23.93805503845215, \t Total Dis Loss : 0.043872199952602386\n",
      "Steps : 31700, \t Total Gen Loss : 21.757282257080078, \t Total Dis Loss : 0.0003359806723892689\n",
      "Steps : 31800, \t Total Gen Loss : 25.68971824645996, \t Total Dis Loss : 0.0003352651256136596\n",
      "Steps : 31900, \t Total Gen Loss : 25.84777069091797, \t Total Dis Loss : 0.0006672298768535256\n",
      "Steps : 32000, \t Total Gen Loss : 26.040390014648438, \t Total Dis Loss : 0.0010837236186489463\n",
      "Steps : 32100, \t Total Gen Loss : 23.224756240844727, \t Total Dis Loss : 0.0015322479885071516\n",
      "Steps : 32200, \t Total Gen Loss : 23.362876892089844, \t Total Dis Loss : 0.0009611453860998154\n",
      "Steps : 32300, \t Total Gen Loss : 25.159360885620117, \t Total Dis Loss : 0.0002485547447577119\n",
      "Steps : 32400, \t Total Gen Loss : 31.001251220703125, \t Total Dis Loss : 6.424928869819269e-05\n",
      "Steps : 32500, \t Total Gen Loss : 22.734390258789062, \t Total Dis Loss : 0.0009273365722037852\n",
      "Steps : 32600, \t Total Gen Loss : 21.94973373413086, \t Total Dis Loss : 0.0007025976665318012\n",
      "Steps : 32700, \t Total Gen Loss : 22.969097137451172, \t Total Dis Loss : 0.0003318345989100635\n",
      "Steps : 32800, \t Total Gen Loss : 27.198415756225586, \t Total Dis Loss : 0.00028336921241134405\n",
      "Steps : 32900, \t Total Gen Loss : 27.58960723876953, \t Total Dis Loss : 9.363015124108642e-05\n",
      "Steps : 33000, \t Total Gen Loss : 25.446178436279297, \t Total Dis Loss : 0.000551213975995779\n",
      "Steps : 33100, \t Total Gen Loss : 25.867740631103516, \t Total Dis Loss : 0.002277206163853407\n",
      "Steps : 33200, \t Total Gen Loss : 27.445531845092773, \t Total Dis Loss : 0.0018531266832724214\n",
      "Steps : 33300, \t Total Gen Loss : 24.331079483032227, \t Total Dis Loss : 0.00018504576291888952\n",
      "Steps : 33400, \t Total Gen Loss : 25.808521270751953, \t Total Dis Loss : 0.0032982835546135902\n",
      "Steps : 33500, \t Total Gen Loss : 23.412750244140625, \t Total Dis Loss : 0.0020477622747421265\n",
      "Steps : 33600, \t Total Gen Loss : 25.616758346557617, \t Total Dis Loss : 0.0005294497241266072\n",
      "Steps : 33700, \t Total Gen Loss : 22.474266052246094, \t Total Dis Loss : 0.0010291074868291616\n",
      "Time for epoch 6 is 73.83080887794495 sec\n",
      "Steps : 33800, \t Total Gen Loss : 24.28803062438965, \t Total Dis Loss : 0.0016599693335592747\n",
      "Steps : 33900, \t Total Gen Loss : 23.38644790649414, \t Total Dis Loss : 0.001112555619329214\n",
      "Steps : 34000, \t Total Gen Loss : 25.159221649169922, \t Total Dis Loss : 0.0006880962755531073\n",
      "Steps : 34100, \t Total Gen Loss : 21.908830642700195, \t Total Dis Loss : 0.0004389329405967146\n",
      "Steps : 34200, \t Total Gen Loss : 24.646564483642578, \t Total Dis Loss : 0.000305220833979547\n",
      "Steps : 34300, \t Total Gen Loss : 23.276569366455078, \t Total Dis Loss : 0.005065589211881161\n",
      "Steps : 34400, \t Total Gen Loss : 24.647472381591797, \t Total Dis Loss : 0.00048659450840204954\n",
      "Steps : 34500, \t Total Gen Loss : 25.259727478027344, \t Total Dis Loss : 0.0007478473125956953\n",
      "Steps : 34600, \t Total Gen Loss : 21.375438690185547, \t Total Dis Loss : 0.0019509069388732314\n",
      "Steps : 34700, \t Total Gen Loss : 23.31243133544922, \t Total Dis Loss : 0.0006867250776849687\n",
      "Steps : 34800, \t Total Gen Loss : 26.27269744873047, \t Total Dis Loss : 0.0002957946853712201\n",
      "Steps : 34900, \t Total Gen Loss : 25.262868881225586, \t Total Dis Loss : 0.00012030939979013056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35000, \t Total Gen Loss : 22.021760940551758, \t Total Dis Loss : 0.0011512526543810964\n",
      "Steps : 35100, \t Total Gen Loss : 23.7304630279541, \t Total Dis Loss : 0.0007336577400565147\n",
      "Steps : 35200, \t Total Gen Loss : 25.92569351196289, \t Total Dis Loss : 0.0012968407245352864\n",
      "Steps : 35300, \t Total Gen Loss : 21.87417984008789, \t Total Dis Loss : 0.0010203226702287793\n",
      "Steps : 35400, \t Total Gen Loss : 27.10287094116211, \t Total Dis Loss : 0.002849413314834237\n",
      "Steps : 35500, \t Total Gen Loss : 23.945674896240234, \t Total Dis Loss : 0.001865267870016396\n",
      "Steps : 35600, \t Total Gen Loss : 26.043394088745117, \t Total Dis Loss : 0.0010851356200873852\n",
      "Steps : 35700, \t Total Gen Loss : 23.342561721801758, \t Total Dis Loss : 0.003360106609761715\n",
      "Steps : 35800, \t Total Gen Loss : 22.766040802001953, \t Total Dis Loss : 0.0019478766480460763\n",
      "Steps : 35900, \t Total Gen Loss : 24.272991180419922, \t Total Dis Loss : 0.0004889557021670043\n",
      "Steps : 36000, \t Total Gen Loss : 22.791820526123047, \t Total Dis Loss : 0.1043507531285286\n",
      "Steps : 36100, \t Total Gen Loss : 22.610260009765625, \t Total Dis Loss : 0.0018718736246228218\n",
      "Steps : 36200, \t Total Gen Loss : 30.5843563079834, \t Total Dis Loss : 0.0010643431451171637\n",
      "Steps : 36300, \t Total Gen Loss : 28.7768611907959, \t Total Dis Loss : 0.006437602918595076\n",
      "Steps : 36400, \t Total Gen Loss : 21.53675079345703, \t Total Dis Loss : 0.10421299189329147\n",
      "Steps : 36500, \t Total Gen Loss : 25.09989356994629, \t Total Dis Loss : 0.00039883883437141776\n",
      "Steps : 36600, \t Total Gen Loss : 24.252973556518555, \t Total Dis Loss : 0.0005664365598931909\n",
      "Steps : 36700, \t Total Gen Loss : 26.52713394165039, \t Total Dis Loss : 0.001164429122582078\n",
      "Steps : 36800, \t Total Gen Loss : 23.621536254882812, \t Total Dis Loss : 0.0003130248805973679\n",
      "Steps : 36900, \t Total Gen Loss : 23.501413345336914, \t Total Dis Loss : 0.00024440494598820806\n",
      "Steps : 37000, \t Total Gen Loss : 22.242809295654297, \t Total Dis Loss : 0.0013136229244992137\n",
      "Steps : 37100, \t Total Gen Loss : 23.639318466186523, \t Total Dis Loss : 0.000374632072634995\n",
      "Steps : 37200, \t Total Gen Loss : 24.047069549560547, \t Total Dis Loss : 0.00018335459753870964\n",
      "Steps : 37300, \t Total Gen Loss : 25.250118255615234, \t Total Dis Loss : 0.0005849831504747272\n",
      "Steps : 37400, \t Total Gen Loss : 23.752710342407227, \t Total Dis Loss : 0.00029099787934683263\n",
      "Steps : 37500, \t Total Gen Loss : 26.520904541015625, \t Total Dis Loss : 0.0008076808881014585\n",
      "Steps : 37600, \t Total Gen Loss : 23.05591583251953, \t Total Dis Loss : 0.0003656813642010093\n",
      "Steps : 37700, \t Total Gen Loss : 21.881147384643555, \t Total Dis Loss : 0.002523628296330571\n",
      "Steps : 37800, \t Total Gen Loss : 23.289112091064453, \t Total Dis Loss : 0.0010063815861940384\n",
      "Steps : 37900, \t Total Gen Loss : 22.596555709838867, \t Total Dis Loss : 0.0006368322647176683\n",
      "Steps : 38000, \t Total Gen Loss : 21.925508499145508, \t Total Dis Loss : 0.0006167767569422722\n",
      "Steps : 38100, \t Total Gen Loss : 21.692842483520508, \t Total Dis Loss : 0.003967720549553633\n",
      "Steps : 38200, \t Total Gen Loss : 26.750686645507812, \t Total Dis Loss : 0.0001830733090173453\n",
      "Steps : 38300, \t Total Gen Loss : 28.582426071166992, \t Total Dis Loss : 8.850396261550486e-05\n",
      "Steps : 38400, \t Total Gen Loss : 21.755029678344727, \t Total Dis Loss : 0.0009076594142243266\n",
      "Steps : 38500, \t Total Gen Loss : 25.586641311645508, \t Total Dis Loss : 0.0001929665741045028\n",
      "Steps : 38600, \t Total Gen Loss : 20.838747024536133, \t Total Dis Loss : 0.0024632816202938557\n",
      "Steps : 38700, \t Total Gen Loss : 26.896989822387695, \t Total Dis Loss : 0.00019188818987458944\n",
      "Steps : 38800, \t Total Gen Loss : 23.571889877319336, \t Total Dis Loss : 0.0011265825014561415\n",
      "Steps : 38900, \t Total Gen Loss : 22.346153259277344, \t Total Dis Loss : 0.0011798435589298606\n",
      "Steps : 39000, \t Total Gen Loss : 26.615907669067383, \t Total Dis Loss : 0.0001631147024454549\n",
      "Steps : 39100, \t Total Gen Loss : 26.76755714416504, \t Total Dis Loss : 0.00021000461129005998\n",
      "Steps : 39200, \t Total Gen Loss : 26.056367874145508, \t Total Dis Loss : 0.0002107724139932543\n",
      "Steps : 39300, \t Total Gen Loss : 25.266197204589844, \t Total Dis Loss : 0.00020916380162816495\n",
      "Time for epoch 7 is 73.84572386741638 sec\n",
      "Steps : 39400, \t Total Gen Loss : 25.86456871032715, \t Total Dis Loss : 0.0018133176490664482\n",
      "Steps : 39500, \t Total Gen Loss : 26.686771392822266, \t Total Dis Loss : 0.00042552282684482634\n",
      "Steps : 39600, \t Total Gen Loss : 24.533411026000977, \t Total Dis Loss : 0.000774179701693356\n",
      "Steps : 39700, \t Total Gen Loss : 23.087120056152344, \t Total Dis Loss : 0.0002584216417744756\n",
      "Steps : 39800, \t Total Gen Loss : 24.362743377685547, \t Total Dis Loss : 0.00011987694597337395\n",
      "Steps : 39900, \t Total Gen Loss : 21.783878326416016, \t Total Dis Loss : 0.0004269625060260296\n",
      "Steps : 40000, \t Total Gen Loss : 21.787452697753906, \t Total Dis Loss : 0.0007266947068274021\n",
      "Steps : 40100, \t Total Gen Loss : 19.676183700561523, \t Total Dis Loss : 0.0011021342361345887\n",
      "Steps : 40200, \t Total Gen Loss : 24.824716567993164, \t Total Dis Loss : 0.00048431940376758575\n",
      "Steps : 40300, \t Total Gen Loss : 24.93528175354004, \t Total Dis Loss : 0.00035130471223965287\n",
      "Steps : 40400, \t Total Gen Loss : 22.814102172851562, \t Total Dis Loss : 0.00023346766829490662\n",
      "Steps : 40500, \t Total Gen Loss : 22.180583953857422, \t Total Dis Loss : 0.00027325539849698544\n",
      "Steps : 40600, \t Total Gen Loss : 24.714427947998047, \t Total Dis Loss : 0.00022185867419466376\n",
      "Steps : 40700, \t Total Gen Loss : 23.430574417114258, \t Total Dis Loss : 0.000835018465295434\n",
      "Steps : 40800, \t Total Gen Loss : 25.907291412353516, \t Total Dis Loss : 0.00010038728214567527\n",
      "Steps : 40900, \t Total Gen Loss : 21.766159057617188, \t Total Dis Loss : 0.0013036364689469337\n",
      "Steps : 41000, \t Total Gen Loss : 23.901573181152344, \t Total Dis Loss : 0.0038820381741970778\n",
      "Steps : 41100, \t Total Gen Loss : 23.900726318359375, \t Total Dis Loss : 0.0004687177133746445\n",
      "Steps : 41200, \t Total Gen Loss : 22.864585876464844, \t Total Dis Loss : 0.0006086413632147014\n",
      "Steps : 41300, \t Total Gen Loss : 24.671688079833984, \t Total Dis Loss : 0.0002797383931465447\n",
      "Steps : 41400, \t Total Gen Loss : 24.0858154296875, \t Total Dis Loss : 0.00035621857387013733\n",
      "Steps : 41500, \t Total Gen Loss : 21.228260040283203, \t Total Dis Loss : 0.00026349961990490556\n",
      "Steps : 41600, \t Total Gen Loss : 23.254898071289062, \t Total Dis Loss : 0.0002060160768451169\n",
      "Steps : 41700, \t Total Gen Loss : 24.269519805908203, \t Total Dis Loss : 0.00015784011338837445\n",
      "Steps : 41800, \t Total Gen Loss : 23.121658325195312, \t Total Dis Loss : 0.0013030151603743434\n",
      "Steps : 41900, \t Total Gen Loss : 23.696664810180664, \t Total Dis Loss : 0.008091436699032784\n",
      "Steps : 42000, \t Total Gen Loss : 24.508520126342773, \t Total Dis Loss : 0.00679726991802454\n",
      "Steps : 42100, \t Total Gen Loss : 25.802352905273438, \t Total Dis Loss : 0.0006434546667151153\n",
      "Steps : 42200, \t Total Gen Loss : 26.224407196044922, \t Total Dis Loss : 0.08874333649873734\n",
      "Steps : 42300, \t Total Gen Loss : 32.87101745605469, \t Total Dis Loss : 0.0021564788185060024\n",
      "Steps : 42400, \t Total Gen Loss : 25.472801208496094, \t Total Dis Loss : 0.007844524458050728\n",
      "Steps : 42500, \t Total Gen Loss : 27.585269927978516, \t Total Dis Loss : 0.001728250877931714\n",
      "Steps : 42600, \t Total Gen Loss : 28.698291778564453, \t Total Dis Loss : 0.00046533189015462995\n",
      "Steps : 42700, \t Total Gen Loss : 28.396390914916992, \t Total Dis Loss : 0.0003706993884406984\n",
      "Steps : 42800, \t Total Gen Loss : 28.17950439453125, \t Total Dis Loss : 0.00035797173040919006\n",
      "Steps : 42900, \t Total Gen Loss : 28.006258010864258, \t Total Dis Loss : 0.0002120863791787997\n",
      "Steps : 43000, \t Total Gen Loss : 29.315242767333984, \t Total Dis Loss : 0.00013217816012911499\n",
      "Steps : 43100, \t Total Gen Loss : 23.913663864135742, \t Total Dis Loss : 0.0016257485840469599\n",
      "Steps : 43200, \t Total Gen Loss : 28.47470474243164, \t Total Dis Loss : 0.0001026564568746835\n",
      "Steps : 43300, \t Total Gen Loss : 25.378253936767578, \t Total Dis Loss : 0.0009140403708443046\n",
      "Steps : 43400, \t Total Gen Loss : 26.93321990966797, \t Total Dis Loss : 0.0006055920966900885\n",
      "Steps : 43500, \t Total Gen Loss : 28.049148559570312, \t Total Dis Loss : 0.00044611230259761214\n",
      "Steps : 43600, \t Total Gen Loss : 24.730255126953125, \t Total Dis Loss : 0.00013890440459363163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43700, \t Total Gen Loss : 25.849441528320312, \t Total Dis Loss : 0.002382514998316765\n",
      "Steps : 43800, \t Total Gen Loss : 26.787105560302734, \t Total Dis Loss : 0.0004666591703426093\n",
      "Steps : 43900, \t Total Gen Loss : 24.186006546020508, \t Total Dis Loss : 0.000394828908611089\n",
      "Steps : 44000, \t Total Gen Loss : 23.434871673583984, \t Total Dis Loss : 0.01095381285995245\n",
      "Steps : 44100, \t Total Gen Loss : 29.92234992980957, \t Total Dis Loss : 0.00017040030797943473\n",
      "Steps : 44200, \t Total Gen Loss : 29.480844497680664, \t Total Dis Loss : 0.000131036460516043\n",
      "Steps : 44300, \t Total Gen Loss : 27.908674240112305, \t Total Dis Loss : 0.00015661350334994495\n",
      "Steps : 44400, \t Total Gen Loss : 32.149234771728516, \t Total Dis Loss : 0.00017344624211546034\n",
      "Steps : 44500, \t Total Gen Loss : 24.867565155029297, \t Total Dis Loss : 6.135614967206493e-05\n",
      "Steps : 44600, \t Total Gen Loss : 25.961212158203125, \t Total Dis Loss : 0.00015670435095671564\n",
      "Steps : 44700, \t Total Gen Loss : 33.128509521484375, \t Total Dis Loss : 3.828870103461668e-05\n",
      "Steps : 44800, \t Total Gen Loss : 28.726804733276367, \t Total Dis Loss : 0.0014150686329230666\n",
      "Steps : 44900, \t Total Gen Loss : 24.27659797668457, \t Total Dis Loss : 0.00022101840295363218\n",
      "Steps : 45000, \t Total Gen Loss : 26.41299819946289, \t Total Dis Loss : 5.27931479155086e-05\n",
      "Time for epoch 8 is 73.90067553520203 sec\n",
      "Steps : 45100, \t Total Gen Loss : 27.16277503967285, \t Total Dis Loss : 4.200606417725794e-05\n",
      "Steps : 45200, \t Total Gen Loss : 26.962629318237305, \t Total Dis Loss : 0.00015889914357103407\n",
      "Steps : 45300, \t Total Gen Loss : 23.258365631103516, \t Total Dis Loss : 6.766289880033582e-05\n",
      "Steps : 45400, \t Total Gen Loss : 27.23695182800293, \t Total Dis Loss : 1.710429205559194e-05\n",
      "Steps : 45500, \t Total Gen Loss : 25.70064353942871, \t Total Dis Loss : 3.318415838293731e-05\n",
      "Steps : 45600, \t Total Gen Loss : 25.60897445678711, \t Total Dis Loss : 4.7388995881192386e-05\n",
      "Steps : 45700, \t Total Gen Loss : 26.861328125, \t Total Dis Loss : 0.002825716743245721\n",
      "Steps : 45800, \t Total Gen Loss : 25.715951919555664, \t Total Dis Loss : 0.0037909126840531826\n",
      "Steps : 45900, \t Total Gen Loss : 24.571739196777344, \t Total Dis Loss : 0.00036188281956128776\n",
      "Steps : 46000, \t Total Gen Loss : 24.451190948486328, \t Total Dis Loss : 0.00010588107397779822\n",
      "Steps : 46100, \t Total Gen Loss : 24.90260124206543, \t Total Dis Loss : 0.00016517640324309468\n",
      "Steps : 46200, \t Total Gen Loss : 27.499359130859375, \t Total Dis Loss : 0.0013387001818045974\n",
      "Steps : 46300, \t Total Gen Loss : 24.248668670654297, \t Total Dis Loss : 0.00027152313850820065\n",
      "Steps : 46400, \t Total Gen Loss : 23.424911499023438, \t Total Dis Loss : 0.022181563079357147\n",
      "Steps : 46500, \t Total Gen Loss : 24.790264129638672, \t Total Dis Loss : 0.0003151191340293735\n",
      "Steps : 46600, \t Total Gen Loss : 25.46257972717285, \t Total Dis Loss : 0.0003256437194067985\n",
      "Steps : 46700, \t Total Gen Loss : 26.611175537109375, \t Total Dis Loss : 0.0003035313857253641\n",
      "Steps : 46800, \t Total Gen Loss : 23.769466400146484, \t Total Dis Loss : 0.00036503272713162005\n",
      "Steps : 46900, \t Total Gen Loss : 25.287059783935547, \t Total Dis Loss : 0.00013694792869500816\n",
      "Steps : 47000, \t Total Gen Loss : 25.995712280273438, \t Total Dis Loss : 0.00015436470857821405\n",
      "Steps : 47100, \t Total Gen Loss : 21.71788787841797, \t Total Dis Loss : 0.0025216194335371256\n",
      "Steps : 47200, \t Total Gen Loss : 24.25336456298828, \t Total Dis Loss : 0.0008039934327825904\n",
      "Steps : 47300, \t Total Gen Loss : 26.232311248779297, \t Total Dis Loss : 0.00023262029571924359\n",
      "Steps : 47400, \t Total Gen Loss : 24.592369079589844, \t Total Dis Loss : 0.002373552182689309\n",
      "Steps : 47500, \t Total Gen Loss : 24.578004837036133, \t Total Dis Loss : 0.00010923585068667307\n",
      "Steps : 47600, \t Total Gen Loss : 35.80859375, \t Total Dis Loss : 0.00015091868408489972\n",
      "Steps : 47700, \t Total Gen Loss : 23.409671783447266, \t Total Dis Loss : 0.003672436811029911\n",
      "Steps : 47800, \t Total Gen Loss : 21.103824615478516, \t Total Dis Loss : 0.0012283835094422102\n",
      "Steps : 47900, \t Total Gen Loss : 24.0886173248291, \t Total Dis Loss : 0.0004929115530103445\n",
      "Steps : 48000, \t Total Gen Loss : 22.06355094909668, \t Total Dis Loss : 0.0014847590355202556\n",
      "Steps : 48100, \t Total Gen Loss : 24.888389587402344, \t Total Dis Loss : 0.0006840035202912986\n",
      "Steps : 48200, \t Total Gen Loss : 22.7557373046875, \t Total Dis Loss : 0.00048739678459241986\n",
      "Steps : 48300, \t Total Gen Loss : 27.620288848876953, \t Total Dis Loss : 4.555236591841094e-05\n",
      "Steps : 48400, \t Total Gen Loss : 23.869247436523438, \t Total Dis Loss : 0.00023709257948212326\n",
      "Steps : 48500, \t Total Gen Loss : 24.837024688720703, \t Total Dis Loss : 0.0005144538590684533\n",
      "Steps : 48600, \t Total Gen Loss : 23.875751495361328, \t Total Dis Loss : 0.0005277759046293795\n",
      "Steps : 48700, \t Total Gen Loss : 23.836000442504883, \t Total Dis Loss : 0.00024138105800375342\n",
      "Steps : 48800, \t Total Gen Loss : 25.609968185424805, \t Total Dis Loss : 0.00015563784108962864\n",
      "Steps : 48900, \t Total Gen Loss : 24.013822555541992, \t Total Dis Loss : 0.00015235700993798673\n",
      "Steps : 49000, \t Total Gen Loss : 26.274669647216797, \t Total Dis Loss : 0.00018123410700354725\n",
      "Steps : 49100, \t Total Gen Loss : 23.82416534423828, \t Total Dis Loss : 0.0008917023078538477\n",
      "Steps : 49200, \t Total Gen Loss : 19.24844741821289, \t Total Dis Loss : 0.005115020088851452\n",
      "Steps : 49300, \t Total Gen Loss : 25.804119110107422, \t Total Dis Loss : 0.00044252158841118217\n",
      "Steps : 49400, \t Total Gen Loss : 25.407325744628906, \t Total Dis Loss : 0.0003184411325491965\n",
      "Steps : 49500, \t Total Gen Loss : 23.537961959838867, \t Total Dis Loss : 0.00014400883810594678\n",
      "Steps : 49600, \t Total Gen Loss : 24.172683715820312, \t Total Dis Loss : 0.00034430110827088356\n",
      "Steps : 49700, \t Total Gen Loss : 18.533021926879883, \t Total Dis Loss : 0.12987405061721802\n",
      "Steps : 49800, \t Total Gen Loss : 24.0485897064209, \t Total Dis Loss : 0.00044360844185575843\n",
      "Steps : 49900, \t Total Gen Loss : 22.261981964111328, \t Total Dis Loss : 0.00019314368546474725\n",
      "Steps : 50000, \t Total Gen Loss : 22.94527816772461, \t Total Dis Loss : 0.00013409732491709292\n",
      "Steps : 50100, \t Total Gen Loss : 27.17154312133789, \t Total Dis Loss : 5.665494973072782e-05\n",
      "Steps : 50200, \t Total Gen Loss : 27.527624130249023, \t Total Dis Loss : 8.040842658374459e-05\n",
      "Steps : 50300, \t Total Gen Loss : 26.500865936279297, \t Total Dis Loss : 8.674280252307653e-05\n",
      "Steps : 50400, \t Total Gen Loss : 23.865880966186523, \t Total Dis Loss : 0.00015746477583888918\n",
      "Steps : 50500, \t Total Gen Loss : 25.253551483154297, \t Total Dis Loss : 2.577512350399047e-05\n",
      "Steps : 50600, \t Total Gen Loss : 27.37579345703125, \t Total Dis Loss : 0.00018289839499630034\n",
      "Time for epoch 9 is 73.79860663414001 sec\n",
      "Steps : 50700, \t Total Gen Loss : 26.638959884643555, \t Total Dis Loss : 0.0005895828944630921\n",
      "Steps : 50800, \t Total Gen Loss : 23.61147117614746, \t Total Dis Loss : 0.00017310248222202063\n",
      "Steps : 50900, \t Total Gen Loss : 24.773834228515625, \t Total Dis Loss : 0.00011337340401951224\n",
      "Steps : 51000, \t Total Gen Loss : 26.078889846801758, \t Total Dis Loss : 8.257392619270831e-05\n",
      "Steps : 51100, \t Total Gen Loss : 23.490800857543945, \t Total Dis Loss : 0.0005732089630328119\n",
      "Steps : 51200, \t Total Gen Loss : 22.868438720703125, \t Total Dis Loss : 0.00018234002345707268\n",
      "Steps : 51300, \t Total Gen Loss : 23.256141662597656, \t Total Dis Loss : 0.0007900398341007531\n",
      "Steps : 51400, \t Total Gen Loss : 25.566532135009766, \t Total Dis Loss : 0.00031106529058888555\n",
      "Steps : 51500, \t Total Gen Loss : 27.42718505859375, \t Total Dis Loss : 0.0002948830951936543\n",
      "Steps : 51600, \t Total Gen Loss : 23.425098419189453, \t Total Dis Loss : 0.00013589393347501755\n",
      "Steps : 51700, \t Total Gen Loss : 21.92867088317871, \t Total Dis Loss : 0.00850923452526331\n",
      "Steps : 51800, \t Total Gen Loss : 24.697561264038086, \t Total Dis Loss : 0.0002907735179178417\n",
      "Steps : 51900, \t Total Gen Loss : 24.02444839477539, \t Total Dis Loss : 0.0008052848861552775\n",
      "Steps : 52000, \t Total Gen Loss : 25.619178771972656, \t Total Dis Loss : 0.000497790053486824\n",
      "Steps : 52100, \t Total Gen Loss : 24.66994857788086, \t Total Dis Loss : 7.65742952353321e-05\n",
      "Steps : 52200, \t Total Gen Loss : 26.078893661499023, \t Total Dis Loss : 8.208398503484204e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52300, \t Total Gen Loss : 28.38894271850586, \t Total Dis Loss : 5.907741069677286e-05\n",
      "Steps : 52400, \t Total Gen Loss : 26.712905883789062, \t Total Dis Loss : 0.00036753452150151134\n",
      "Steps : 52500, \t Total Gen Loss : 24.880298614501953, \t Total Dis Loss : 9.59582903306e-05\n",
      "Steps : 52600, \t Total Gen Loss : 28.496009826660156, \t Total Dis Loss : 8.356283797184005e-05\n",
      "Steps : 52700, \t Total Gen Loss : 25.323957443237305, \t Total Dis Loss : 0.00011121245916001499\n",
      "Steps : 52800, \t Total Gen Loss : 26.276893615722656, \t Total Dis Loss : 0.00013609770394396037\n",
      "Steps : 52900, \t Total Gen Loss : 25.80139923095703, \t Total Dis Loss : 8.500365220243111e-05\n",
      "Steps : 53000, \t Total Gen Loss : 24.0645751953125, \t Total Dis Loss : 0.0010404204949736595\n",
      "Steps : 53100, \t Total Gen Loss : 26.775012969970703, \t Total Dis Loss : 0.0004408344393596053\n",
      "Steps : 53200, \t Total Gen Loss : 24.648792266845703, \t Total Dis Loss : 0.00025708251632750034\n",
      "Steps : 53300, \t Total Gen Loss : 25.704042434692383, \t Total Dis Loss : 0.1727641522884369\n",
      "Steps : 53400, \t Total Gen Loss : 26.06096649169922, \t Total Dis Loss : 0.0004108503635507077\n",
      "Steps : 53500, \t Total Gen Loss : 26.13479995727539, \t Total Dis Loss : 0.00028187091811560094\n",
      "Steps : 53600, \t Total Gen Loss : 24.067062377929688, \t Total Dis Loss : 0.00025688757887110114\n",
      "Steps : 53700, \t Total Gen Loss : 24.275962829589844, \t Total Dis Loss : 3.498672231216915e-05\n",
      "Steps : 53800, \t Total Gen Loss : 28.59122085571289, \t Total Dis Loss : 4.6192933950806037e-05\n",
      "Steps : 53900, \t Total Gen Loss : 27.58049201965332, \t Total Dis Loss : 8.218453331210185e-06\n",
      "Steps : 54000, \t Total Gen Loss : 23.36566162109375, \t Total Dis Loss : 0.0008391056908294559\n",
      "Steps : 54100, \t Total Gen Loss : 26.982973098754883, \t Total Dis Loss : 9.210706048179418e-05\n",
      "Steps : 54200, \t Total Gen Loss : 25.154539108276367, \t Total Dis Loss : 0.0004256233514752239\n",
      "Steps : 54300, \t Total Gen Loss : 22.980722427368164, \t Total Dis Loss : 0.0014664604095742106\n",
      "Steps : 54400, \t Total Gen Loss : 23.975914001464844, \t Total Dis Loss : 0.0013231347547844052\n",
      "Steps : 54500, \t Total Gen Loss : 23.150190353393555, \t Total Dis Loss : 0.00027545937336981297\n",
      "Steps : 54600, \t Total Gen Loss : 26.482229232788086, \t Total Dis Loss : 0.00019655950018204749\n",
      "Steps : 54700, \t Total Gen Loss : 23.147254943847656, \t Total Dis Loss : 0.03535812720656395\n",
      "Steps : 54800, \t Total Gen Loss : 22.860614776611328, \t Total Dis Loss : 0.0011371270520612597\n",
      "Steps : 54900, \t Total Gen Loss : 25.953044891357422, \t Total Dis Loss : 0.002956648822873831\n",
      "Steps : 55000, \t Total Gen Loss : 23.23587989807129, \t Total Dis Loss : 0.0003448932257015258\n",
      "Steps : 55100, \t Total Gen Loss : 25.401569366455078, \t Total Dis Loss : 0.0001974465121747926\n",
      "Steps : 55200, \t Total Gen Loss : 24.761112213134766, \t Total Dis Loss : 0.0002321160864084959\n",
      "Steps : 55300, \t Total Gen Loss : 23.29607391357422, \t Total Dis Loss : 9.192395373247564e-05\n",
      "Steps : 55400, \t Total Gen Loss : 24.419334411621094, \t Total Dis Loss : 0.00045211485121399164\n",
      "Steps : 55500, \t Total Gen Loss : 23.73206329345703, \t Total Dis Loss : 0.0005233567790128291\n",
      "Steps : 55600, \t Total Gen Loss : 21.9385929107666, \t Total Dis Loss : 0.0003104086499661207\n",
      "Steps : 55700, \t Total Gen Loss : 23.27889633178711, \t Total Dis Loss : 0.0007731050136499107\n",
      "Steps : 55800, \t Total Gen Loss : 23.168899536132812, \t Total Dis Loss : 0.0053240833804011345\n",
      "Steps : 55900, \t Total Gen Loss : 22.89337158203125, \t Total Dis Loss : 0.0027643409557640553\n",
      "Steps : 56000, \t Total Gen Loss : 19.96371078491211, \t Total Dis Loss : 0.001641433802433312\n",
      "Steps : 56100, \t Total Gen Loss : 22.190959930419922, \t Total Dis Loss : 0.0003954979474656284\n",
      "Steps : 56200, \t Total Gen Loss : 25.229475021362305, \t Total Dis Loss : 0.00020054983906447887\n",
      "Time for epoch 10 is 77.2040765285492 sec\n",
      "Steps : 56300, \t Total Gen Loss : 21.794570922851562, \t Total Dis Loss : 0.003484623972326517\n",
      "Steps : 56400, \t Total Gen Loss : 23.010089874267578, \t Total Dis Loss : 0.0014627203345298767\n",
      "Steps : 56500, \t Total Gen Loss : 22.762907028198242, \t Total Dis Loss : 0.00030281877843663096\n",
      "Steps : 56600, \t Total Gen Loss : 22.81186294555664, \t Total Dis Loss : 0.0005670082755386829\n",
      "Steps : 56700, \t Total Gen Loss : 22.735248565673828, \t Total Dis Loss : 0.0001508726563770324\n",
      "Steps : 56800, \t Total Gen Loss : 24.118146896362305, \t Total Dis Loss : 0.00013489009870681912\n",
      "Steps : 56900, \t Total Gen Loss : 24.267986297607422, \t Total Dis Loss : 0.0010266529861837626\n",
      "Steps : 57000, \t Total Gen Loss : 23.906274795532227, \t Total Dis Loss : 0.00020738838065881282\n",
      "Steps : 57100, \t Total Gen Loss : 31.077978134155273, \t Total Dis Loss : 0.023783840239048004\n",
      "Steps : 57200, \t Total Gen Loss : 24.872234344482422, \t Total Dis Loss : 0.00029544229619205\n",
      "Steps : 57300, \t Total Gen Loss : 24.68222427368164, \t Total Dis Loss : 7.125365664251149e-05\n",
      "Steps : 57400, \t Total Gen Loss : 25.62063980102539, \t Total Dis Loss : 0.0003965364012401551\n",
      "Steps : 57500, \t Total Gen Loss : 24.553028106689453, \t Total Dis Loss : 0.00046771453344263136\n",
      "Steps : 57600, \t Total Gen Loss : 29.251962661743164, \t Total Dis Loss : 0.00039054715307429433\n",
      "Steps : 57700, \t Total Gen Loss : 30.108211517333984, \t Total Dis Loss : 0.0012326420983299613\n",
      "Steps : 57800, \t Total Gen Loss : 33.30714416503906, \t Total Dis Loss : 0.06649010628461838\n",
      "Steps : 57900, \t Total Gen Loss : 26.75657081604004, \t Total Dis Loss : 0.0003228955902159214\n",
      "Steps : 58000, \t Total Gen Loss : 30.916582107543945, \t Total Dis Loss : 0.0007261986611410975\n",
      "Steps : 58100, \t Total Gen Loss : 30.179906845092773, \t Total Dis Loss : 0.0002577199775259942\n",
      "Steps : 58200, \t Total Gen Loss : 30.64067840576172, \t Total Dis Loss : 0.0001366555952699855\n",
      "Steps : 58300, \t Total Gen Loss : 30.31732940673828, \t Total Dis Loss : 9.989234240492806e-05\n",
      "Steps : 58400, \t Total Gen Loss : 23.938718795776367, \t Total Dis Loss : 0.0007939041242934763\n",
      "Steps : 58500, \t Total Gen Loss : 24.794254302978516, \t Total Dis Loss : 0.0006089400267228484\n",
      "Steps : 58600, \t Total Gen Loss : 25.407756805419922, \t Total Dis Loss : 0.00022357665875460953\n",
      "Steps : 58700, \t Total Gen Loss : 24.322568893432617, \t Total Dis Loss : 0.0009802042040973902\n",
      "Steps : 58800, \t Total Gen Loss : 25.366865158081055, \t Total Dis Loss : 0.0004339774022810161\n",
      "Steps : 58900, \t Total Gen Loss : 24.1295166015625, \t Total Dis Loss : 0.0012254633475095034\n",
      "Steps : 59000, \t Total Gen Loss : 25.079158782958984, \t Total Dis Loss : 0.00030189723474904895\n",
      "Steps : 59100, \t Total Gen Loss : 23.373477935791016, \t Total Dis Loss : 0.0002588983334135264\n",
      "Steps : 59200, \t Total Gen Loss : 25.13022232055664, \t Total Dis Loss : 0.000131581153254956\n",
      "Steps : 59300, \t Total Gen Loss : 24.114879608154297, \t Total Dis Loss : 0.00036111794179305434\n",
      "Steps : 59400, \t Total Gen Loss : 26.904220581054688, \t Total Dis Loss : 0.0004225867451168597\n",
      "Steps : 59500, \t Total Gen Loss : 25.252782821655273, \t Total Dis Loss : 0.0003683877002913505\n",
      "Steps : 59600, \t Total Gen Loss : 23.172012329101562, \t Total Dis Loss : 0.001453965320251882\n",
      "Steps : 59700, \t Total Gen Loss : 26.7534236907959, \t Total Dis Loss : 0.0007475301390513778\n",
      "Steps : 59800, \t Total Gen Loss : 23.755321502685547, \t Total Dis Loss : 0.0008769879932515323\n",
      "Steps : 59900, \t Total Gen Loss : 22.901878356933594, \t Total Dis Loss : 0.00016284032608382404\n",
      "Steps : 60000, \t Total Gen Loss : 25.28524398803711, \t Total Dis Loss : 0.0012000426650047302\n",
      "Steps : 60100, \t Total Gen Loss : 24.793663024902344, \t Total Dis Loss : 0.0004982278333045542\n",
      "Steps : 60200, \t Total Gen Loss : 24.24306869506836, \t Total Dis Loss : 0.0001363722694804892\n",
      "Steps : 60300, \t Total Gen Loss : 27.197832107543945, \t Total Dis Loss : 0.00015750501188449562\n",
      "Steps : 60400, \t Total Gen Loss : 27.34406280517578, \t Total Dis Loss : 0.0005684343632310629\n",
      "Steps : 60500, \t Total Gen Loss : 27.463703155517578, \t Total Dis Loss : 0.00011103924043709412\n",
      "Steps : 60600, \t Total Gen Loss : 24.004764556884766, \t Total Dis Loss : 0.0014162251027300954\n",
      "Steps : 60700, \t Total Gen Loss : 23.65182113647461, \t Total Dis Loss : 0.00015660996723454446\n",
      "Steps : 60800, \t Total Gen Loss : 25.682716369628906, \t Total Dis Loss : 0.00013744528405368328\n",
      "Steps : 60900, \t Total Gen Loss : 25.376680374145508, \t Total Dis Loss : 0.0010713896481320262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 61000, \t Total Gen Loss : 24.361412048339844, \t Total Dis Loss : 0.0024877318646758795\n",
      "Steps : 61100, \t Total Gen Loss : 22.71142578125, \t Total Dis Loss : 0.0010215741349384189\n",
      "Steps : 61200, \t Total Gen Loss : 24.894041061401367, \t Total Dis Loss : 0.00027173690614290535\n",
      "Steps : 61300, \t Total Gen Loss : 22.462705612182617, \t Total Dis Loss : 0.0013693494256585836\n",
      "Steps : 61400, \t Total Gen Loss : 26.5556640625, \t Total Dis Loss : 6.738409865647554e-05\n",
      "Steps : 61500, \t Total Gen Loss : 24.696069717407227, \t Total Dis Loss : 2.749762097664643e-05\n",
      "Steps : 61600, \t Total Gen Loss : 28.756980895996094, \t Total Dis Loss : 1.0781891432998236e-05\n",
      "Steps : 61700, \t Total Gen Loss : 24.416568756103516, \t Total Dis Loss : 0.00021965143969282508\n",
      "Steps : 61800, \t Total Gen Loss : 24.177824020385742, \t Total Dis Loss : 0.0004184748395346105\n",
      "Time for epoch 11 is 77.6122772693634 sec\n",
      "Steps : 61900, \t Total Gen Loss : 26.38888168334961, \t Total Dis Loss : 0.00013620562094729394\n",
      "Steps : 62000, \t Total Gen Loss : 23.44949722290039, \t Total Dis Loss : 0.00026917923241853714\n",
      "Steps : 62100, \t Total Gen Loss : 24.72667694091797, \t Total Dis Loss : 0.00014976727834437042\n",
      "Steps : 62200, \t Total Gen Loss : 24.292003631591797, \t Total Dis Loss : 0.015752336010336876\n",
      "Steps : 62300, \t Total Gen Loss : 26.044599533081055, \t Total Dis Loss : 0.00010336051491321996\n",
      "Steps : 62400, \t Total Gen Loss : 26.136428833007812, \t Total Dis Loss : 7.535816985182464e-05\n",
      "Steps : 62500, \t Total Gen Loss : 20.560382843017578, \t Total Dis Loss : 0.10423918068408966\n",
      "Steps : 62600, \t Total Gen Loss : 22.105655670166016, \t Total Dis Loss : 0.0012105923378840089\n",
      "Steps : 62700, \t Total Gen Loss : 24.536949157714844, \t Total Dis Loss : 0.00033586702193133533\n",
      "Steps : 62800, \t Total Gen Loss : 24.482288360595703, \t Total Dis Loss : 0.00018556814757175744\n",
      "Steps : 62900, \t Total Gen Loss : 24.272815704345703, \t Total Dis Loss : 0.00010752966772997752\n",
      "Steps : 63000, \t Total Gen Loss : 26.27099609375, \t Total Dis Loss : 0.00016434867575298995\n",
      "Steps : 63100, \t Total Gen Loss : 22.552772521972656, \t Total Dis Loss : 0.0015779766254127026\n",
      "Steps : 63200, \t Total Gen Loss : 24.40960693359375, \t Total Dis Loss : 0.00017373793525621295\n",
      "Steps : 63300, \t Total Gen Loss : 25.00863265991211, \t Total Dis Loss : 7.942716911202297e-05\n",
      "Steps : 63400, \t Total Gen Loss : 23.606475830078125, \t Total Dis Loss : 0.00012020362919429317\n",
      "Steps : 63500, \t Total Gen Loss : 23.933048248291016, \t Total Dis Loss : 0.0009498235303908587\n",
      "Steps : 63600, \t Total Gen Loss : 24.627323150634766, \t Total Dis Loss : 8.854822954162955e-05\n",
      "Steps : 63700, \t Total Gen Loss : 26.089595794677734, \t Total Dis Loss : 8.406224515056238e-05\n",
      "Steps : 63800, \t Total Gen Loss : 23.17446517944336, \t Total Dis Loss : 7.353378168772906e-05\n",
      "Steps : 63900, \t Total Gen Loss : 28.7106990814209, \t Total Dis Loss : 4.641706254915334e-05\n",
      "Steps : 64000, \t Total Gen Loss : 24.01616668701172, \t Total Dis Loss : 3.92624169762712e-05\n",
      "Steps : 64100, \t Total Gen Loss : 26.59787368774414, \t Total Dis Loss : 7.07677099853754e-05\n",
      "Steps : 64200, \t Total Gen Loss : 25.12537956237793, \t Total Dis Loss : 0.0001228721666848287\n",
      "Steps : 64300, \t Total Gen Loss : 27.588603973388672, \t Total Dis Loss : 0.00030614336719736457\n",
      "Steps : 64400, \t Total Gen Loss : 23.736892700195312, \t Total Dis Loss : 3.938141162507236e-05\n",
      "Steps : 64500, \t Total Gen Loss : 24.518041610717773, \t Total Dis Loss : 0.0010950070573017001\n",
      "Steps : 64600, \t Total Gen Loss : 24.96015739440918, \t Total Dis Loss : 9.220025822287425e-05\n",
      "Steps : 64700, \t Total Gen Loss : 25.58440399169922, \t Total Dis Loss : 5.3940202633384615e-05\n",
      "Steps : 64800, \t Total Gen Loss : 26.220149993896484, \t Total Dis Loss : 5.922811396885663e-05\n",
      "Steps : 64900, \t Total Gen Loss : 26.873485565185547, \t Total Dis Loss : 9.359072282677516e-05\n",
      "Steps : 65000, \t Total Gen Loss : 24.185047149658203, \t Total Dis Loss : 6.263098475756124e-05\n",
      "Steps : 65100, \t Total Gen Loss : 25.017520904541016, \t Total Dis Loss : 5.756799509981647e-05\n",
      "Steps : 65200, \t Total Gen Loss : 27.550994873046875, \t Total Dis Loss : 7.78920657467097e-05\n",
      "Steps : 65300, \t Total Gen Loss : 24.837902069091797, \t Total Dis Loss : 3.162614302709699e-05\n",
      "Steps : 65400, \t Total Gen Loss : 24.883628845214844, \t Total Dis Loss : 5.677065928466618e-05\n",
      "Steps : 65500, \t Total Gen Loss : 25.25106430053711, \t Total Dis Loss : 3.092199767706916e-05\n",
      "Steps : 65600, \t Total Gen Loss : 26.48798179626465, \t Total Dis Loss : 5.016121212975122e-05\n",
      "Steps : 65700, \t Total Gen Loss : 27.02194595336914, \t Total Dis Loss : 6.162861973280087e-05\n",
      "Steps : 65800, \t Total Gen Loss : 26.760772705078125, \t Total Dis Loss : 4.015519152744673e-05\n",
      "Steps : 65900, \t Total Gen Loss : 28.92038345336914, \t Total Dis Loss : 8.953044016379863e-05\n",
      "Steps : 66000, \t Total Gen Loss : 27.66512680053711, \t Total Dis Loss : 2.3608967239852063e-05\n",
      "Steps : 66100, \t Total Gen Loss : 23.45185089111328, \t Total Dis Loss : 0.001143921515904367\n",
      "Steps : 66200, \t Total Gen Loss : 27.669883728027344, \t Total Dis Loss : 2.8057116651325487e-05\n",
      "Steps : 66300, \t Total Gen Loss : 25.660999298095703, \t Total Dis Loss : 0.002039572224020958\n",
      "Steps : 66400, \t Total Gen Loss : 26.16280746459961, \t Total Dis Loss : 7.963916141306981e-05\n",
      "Steps : 66500, \t Total Gen Loss : 25.729248046875, \t Total Dis Loss : 3.153675788780674e-05\n",
      "Steps : 66600, \t Total Gen Loss : 27.182493209838867, \t Total Dis Loss : 2.9997634555911645e-05\n",
      "Steps : 66700, \t Total Gen Loss : 26.15789794921875, \t Total Dis Loss : 3.378332985448651e-05\n",
      "Steps : 66800, \t Total Gen Loss : 26.45132064819336, \t Total Dis Loss : 0.001024553901515901\n",
      "Steps : 66900, \t Total Gen Loss : 28.988012313842773, \t Total Dis Loss : 7.255785021698102e-05\n",
      "Steps : 67000, \t Total Gen Loss : 27.837230682373047, \t Total Dis Loss : 3.08923699776642e-05\n",
      "Steps : 67100, \t Total Gen Loss : 27.077404022216797, \t Total Dis Loss : 0.00010051566641777754\n",
      "Steps : 67200, \t Total Gen Loss : 25.664344787597656, \t Total Dis Loss : 0.0005728433025069535\n",
      "Steps : 67300, \t Total Gen Loss : 29.863603591918945, \t Total Dis Loss : 0.00036799057852476835\n",
      "Steps : 67400, \t Total Gen Loss : 28.68702507019043, \t Total Dis Loss : 0.00024178747844416648\n",
      "Steps : 67500, \t Total Gen Loss : 28.02532958984375, \t Total Dis Loss : 3.24478569382336e-05\n",
      "Time for epoch 12 is 75.73967289924622 sec\n",
      "Steps : 67600, \t Total Gen Loss : 24.557903289794922, \t Total Dis Loss : 6.043715256964788e-05\n",
      "Steps : 67700, \t Total Gen Loss : 30.186241149902344, \t Total Dis Loss : 2.7110463634016924e-05\n",
      "Steps : 67800, \t Total Gen Loss : 25.42818832397461, \t Total Dis Loss : 5.5798216635594144e-05\n",
      "Steps : 67900, \t Total Gen Loss : 26.7117919921875, \t Total Dis Loss : 5.181127198738977e-05\n",
      "Steps : 68000, \t Total Gen Loss : 27.279151916503906, \t Total Dis Loss : 0.0002145712642231956\n",
      "Steps : 68100, \t Total Gen Loss : 31.602054595947266, \t Total Dis Loss : 0.00017463383846916258\n",
      "Steps : 68200, \t Total Gen Loss : 27.494312286376953, \t Total Dis Loss : 0.00014263512275647372\n",
      "Steps : 68300, \t Total Gen Loss : 29.081544876098633, \t Total Dis Loss : 0.00012668006820604205\n",
      "Steps : 68400, \t Total Gen Loss : 29.833545684814453, \t Total Dis Loss : 0.0005575602408498526\n",
      "Steps : 68500, \t Total Gen Loss : 27.55791473388672, \t Total Dis Loss : 8.448364678770304e-05\n",
      "Steps : 68600, \t Total Gen Loss : 26.11487579345703, \t Total Dis Loss : 0.0003869814390782267\n",
      "Steps : 68700, \t Total Gen Loss : 27.324234008789062, \t Total Dis Loss : 0.00022144056856632233\n",
      "Steps : 68800, \t Total Gen Loss : 25.740100860595703, \t Total Dis Loss : 0.0002696378214750439\n",
      "Steps : 68900, \t Total Gen Loss : 24.39604949951172, \t Total Dis Loss : 0.000175880515598692\n",
      "Steps : 69000, \t Total Gen Loss : 26.677448272705078, \t Total Dis Loss : 0.00013515262980945408\n",
      "Steps : 69100, \t Total Gen Loss : 28.532054901123047, \t Total Dis Loss : 0.0001115344712161459\n",
      "Steps : 69200, \t Total Gen Loss : 26.44959831237793, \t Total Dis Loss : 0.0001797620061552152\n",
      "Steps : 69300, \t Total Gen Loss : 30.977724075317383, \t Total Dis Loss : 8.666695794090629e-05\n",
      "Steps : 69400, \t Total Gen Loss : 35.26155471801758, \t Total Dis Loss : 9.660390787757933e-05\n",
      "Steps : 69500, \t Total Gen Loss : 29.74846839904785, \t Total Dis Loss : 1.2086353308404796e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 69600, \t Total Gen Loss : 31.36318588256836, \t Total Dis Loss : 9.162066271528602e-05\n",
      "Steps : 69700, \t Total Gen Loss : 31.66241455078125, \t Total Dis Loss : 0.0001297804992645979\n",
      "Steps : 69800, \t Total Gen Loss : 32.1538200378418, \t Total Dis Loss : 7.470822311006486e-05\n",
      "Steps : 69900, \t Total Gen Loss : 31.45334243774414, \t Total Dis Loss : 0.00012211508874315768\n",
      "Steps : 70000, \t Total Gen Loss : 27.63759994506836, \t Total Dis Loss : 0.0008824485703371465\n",
      "Steps : 70100, \t Total Gen Loss : 30.526018142700195, \t Total Dis Loss : 0.0004812926927115768\n",
      "Steps : 70200, \t Total Gen Loss : 31.821847915649414, \t Total Dis Loss : 5.8105102652916685e-05\n",
      "Steps : 70300, \t Total Gen Loss : 30.877267837524414, \t Total Dis Loss : 7.655657827854156e-05\n",
      "Steps : 70400, \t Total Gen Loss : 26.36017417907715, \t Total Dis Loss : 0.0007570842863060534\n",
      "Steps : 70500, \t Total Gen Loss : 27.15565299987793, \t Total Dis Loss : 0.0005022487603127956\n",
      "Steps : 70600, \t Total Gen Loss : 23.835397720336914, \t Total Dis Loss : 0.00021967472275719047\n",
      "Steps : 70700, \t Total Gen Loss : 24.8955078125, \t Total Dis Loss : 0.000202040551812388\n",
      "Steps : 70800, \t Total Gen Loss : 25.949975967407227, \t Total Dis Loss : 0.00011860626545967534\n",
      "Steps : 70900, \t Total Gen Loss : 25.655540466308594, \t Total Dis Loss : 0.00011152207298437133\n",
      "Steps : 71000, \t Total Gen Loss : 26.23008918762207, \t Total Dis Loss : 0.00014913483755663037\n",
      "Steps : 71100, \t Total Gen Loss : 23.166566848754883, \t Total Dis Loss : 6.532926636282355e-05\n",
      "Steps : 71200, \t Total Gen Loss : 25.931093215942383, \t Total Dis Loss : 4.652237112168223e-05\n",
      "Steps : 71300, \t Total Gen Loss : 26.320880889892578, \t Total Dis Loss : 8.563392475480214e-05\n",
      "Steps : 71400, \t Total Gen Loss : 26.388687133789062, \t Total Dis Loss : 0.00017975013179238886\n",
      "Steps : 71500, \t Total Gen Loss : 27.618709564208984, \t Total Dis Loss : 0.00020807760301977396\n",
      "Steps : 71600, \t Total Gen Loss : 24.634384155273438, \t Total Dis Loss : 0.0001167176742455922\n",
      "Steps : 71700, \t Total Gen Loss : 24.595481872558594, \t Total Dis Loss : 0.00017989090702030808\n",
      "Steps : 71800, \t Total Gen Loss : 31.62734031677246, \t Total Dis Loss : 5.6232893257401884e-05\n",
      "Steps : 71900, \t Total Gen Loss : 30.829883575439453, \t Total Dis Loss : 0.00031207373831421137\n",
      "Steps : 72000, \t Total Gen Loss : 30.191139221191406, \t Total Dis Loss : 4.228674515616149e-05\n",
      "Steps : 72100, \t Total Gen Loss : 28.194902420043945, \t Total Dis Loss : 0.0032453674357384443\n",
      "Steps : 72200, \t Total Gen Loss : 27.552345275878906, \t Total Dis Loss : 0.0002048195747192949\n",
      "Steps : 72300, \t Total Gen Loss : 28.4625186920166, \t Total Dis Loss : 0.00043301042751409113\n",
      "Steps : 72400, \t Total Gen Loss : 29.627201080322266, \t Total Dis Loss : 0.005255629774183035\n",
      "Steps : 72500, \t Total Gen Loss : 27.01141357421875, \t Total Dis Loss : 0.003436397761106491\n",
      "Steps : 72600, \t Total Gen Loss : 21.899925231933594, \t Total Dis Loss : 0.0023689856752753258\n",
      "Steps : 72700, \t Total Gen Loss : 22.10638427734375, \t Total Dis Loss : 0.0009600226185284555\n",
      "Steps : 72800, \t Total Gen Loss : 25.684865951538086, \t Total Dis Loss : 0.0004727834602817893\n",
      "Steps : 72900, \t Total Gen Loss : 26.059467315673828, \t Total Dis Loss : 0.1431337594985962\n",
      "Steps : 73000, \t Total Gen Loss : 23.383691787719727, \t Total Dis Loss : 0.019670603796839714\n",
      "Steps : 73100, \t Total Gen Loss : 24.97110366821289, \t Total Dis Loss : 0.00018807611195370555\n",
      "Time for epoch 13 is 77.71956968307495 sec\n",
      "Steps : 73200, \t Total Gen Loss : 26.587413787841797, \t Total Dis Loss : 9.21183091122657e-05\n",
      "Steps : 73300, \t Total Gen Loss : 24.67681312561035, \t Total Dis Loss : 0.0005055346991866827\n",
      "Steps : 73400, \t Total Gen Loss : 21.67278289794922, \t Total Dis Loss : 0.0003945417993236333\n",
      "Steps : 73500, \t Total Gen Loss : 26.349117279052734, \t Total Dis Loss : 0.00022190783056430519\n",
      "Steps : 73600, \t Total Gen Loss : 23.690080642700195, \t Total Dis Loss : 0.00020769248658325523\n",
      "Steps : 73700, \t Total Gen Loss : 25.48186492919922, \t Total Dis Loss : 0.00022477742459159344\n",
      "Steps : 73800, \t Total Gen Loss : 25.69510269165039, \t Total Dis Loss : 0.00017235922859981656\n",
      "Steps : 73900, \t Total Gen Loss : 24.263195037841797, \t Total Dis Loss : 0.0005411627353169024\n",
      "Steps : 74000, \t Total Gen Loss : 27.702606201171875, \t Total Dis Loss : 0.0007130628800950944\n",
      "Steps : 74100, \t Total Gen Loss : 30.06940460205078, \t Total Dis Loss : 0.00016619522648397833\n",
      "Steps : 74200, \t Total Gen Loss : 29.035600662231445, \t Total Dis Loss : 2.4026629034779035e-05\n",
      "Steps : 74300, \t Total Gen Loss : 25.515512466430664, \t Total Dis Loss : 0.000211970807868056\n",
      "Steps : 74400, \t Total Gen Loss : 24.299304962158203, \t Total Dis Loss : 8.865175186656415e-05\n",
      "Steps : 74500, \t Total Gen Loss : 28.209308624267578, \t Total Dis Loss : 0.00013435818254947662\n",
      "Steps : 74600, \t Total Gen Loss : 24.598588943481445, \t Total Dis Loss : 6.223153468454257e-05\n",
      "Steps : 74700, \t Total Gen Loss : 31.080949783325195, \t Total Dis Loss : 0.00011354983143974096\n",
      "Steps : 74800, \t Total Gen Loss : 27.118927001953125, \t Total Dis Loss : 8.023962436709553e-05\n",
      "Steps : 74900, \t Total Gen Loss : 25.465391159057617, \t Total Dis Loss : 6.104091880843043e-05\n",
      "Steps : 75000, \t Total Gen Loss : 23.301822662353516, \t Total Dis Loss : 9.90075568552129e-05\n",
      "Steps : 75100, \t Total Gen Loss : 26.4352970123291, \t Total Dis Loss : 6.198873597895727e-05\n",
      "Steps : 75200, \t Total Gen Loss : 27.167163848876953, \t Total Dis Loss : 6.59125144011341e-05\n",
      "Steps : 75300, \t Total Gen Loss : 29.66385841369629, \t Total Dis Loss : 0.000122442637803033\n",
      "Steps : 75400, \t Total Gen Loss : 26.58534049987793, \t Total Dis Loss : 0.00010648291936377063\n",
      "Steps : 75500, \t Total Gen Loss : 28.979528427124023, \t Total Dis Loss : 0.00010899417975451797\n",
      "Steps : 75600, \t Total Gen Loss : 26.475547790527344, \t Total Dis Loss : 7.819056190783158e-05\n",
      "Steps : 75700, \t Total Gen Loss : 23.993833541870117, \t Total Dis Loss : 0.00014152548101264983\n",
      "Steps : 75800, \t Total Gen Loss : 25.620929718017578, \t Total Dis Loss : 0.0002336074539925903\n",
      "Steps : 75900, \t Total Gen Loss : 26.484477996826172, \t Total Dis Loss : 7.267441833391786e-05\n",
      "Steps : 76000, \t Total Gen Loss : 25.311994552612305, \t Total Dis Loss : 7.764880865579471e-05\n",
      "Steps : 76100, \t Total Gen Loss : 26.82337188720703, \t Total Dis Loss : 7.063644443405792e-05\n",
      "Steps : 76200, \t Total Gen Loss : 25.58843994140625, \t Total Dis Loss : 4.767363134305924e-05\n",
      "Steps : 76300, \t Total Gen Loss : 25.006671905517578, \t Total Dis Loss : 5.9070509450975806e-05\n",
      "Steps : 76400, \t Total Gen Loss : 28.632366180419922, \t Total Dis Loss : 8.348395203938708e-05\n",
      "Steps : 76500, \t Total Gen Loss : 29.50672149658203, \t Total Dis Loss : 0.000380576093448326\n",
      "Steps : 76600, \t Total Gen Loss : 23.858779907226562, \t Total Dis Loss : 0.0005858132499270141\n",
      "Steps : 76700, \t Total Gen Loss : 24.64153289794922, \t Total Dis Loss : 0.00020613589731510729\n",
      "Steps : 76800, \t Total Gen Loss : 24.692996978759766, \t Total Dis Loss : 0.00017048152221832424\n",
      "Steps : 76900, \t Total Gen Loss : 26.557476043701172, \t Total Dis Loss : 0.0012639939086511731\n",
      "Steps : 77000, \t Total Gen Loss : 25.271793365478516, \t Total Dis Loss : 0.0005193938268348575\n",
      "Steps : 77100, \t Total Gen Loss : 26.378467559814453, \t Total Dis Loss : 0.00036584536428563297\n",
      "Steps : 77200, \t Total Gen Loss : 24.92591094970703, \t Total Dis Loss : 0.00011473716585896909\n",
      "Steps : 77300, \t Total Gen Loss : 26.940900802612305, \t Total Dis Loss : 0.0001823475759010762\n",
      "Steps : 77400, \t Total Gen Loss : 22.30254364013672, \t Total Dis Loss : 0.0016959711210802197\n",
      "Steps : 77500, \t Total Gen Loss : 25.105335235595703, \t Total Dis Loss : 0.00010480095079401508\n",
      "Steps : 77600, \t Total Gen Loss : 28.016210556030273, \t Total Dis Loss : 0.00013611574831884354\n",
      "Steps : 77700, \t Total Gen Loss : 29.40367889404297, \t Total Dis Loss : 0.00012905489711556584\n",
      "Steps : 77800, \t Total Gen Loss : 28.288818359375, \t Total Dis Loss : 0.00024065462639555335\n",
      "Steps : 77900, \t Total Gen Loss : 24.39128303527832, \t Total Dis Loss : 6.463085446739569e-05\n",
      "Steps : 78000, \t Total Gen Loss : 35.93742370605469, \t Total Dis Loss : 0.001665370655246079\n",
      "Steps : 78100, \t Total Gen Loss : 27.548213958740234, \t Total Dis Loss : 0.0008034646743908525\n",
      "Steps : 78200, \t Total Gen Loss : 28.354358673095703, \t Total Dis Loss : 0.0003050676023121923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 78300, \t Total Gen Loss : 23.440067291259766, \t Total Dis Loss : 0.0008594687096774578\n",
      "Steps : 78400, \t Total Gen Loss : 24.661922454833984, \t Total Dis Loss : 0.002954881638288498\n",
      "Steps : 78500, \t Total Gen Loss : 25.161151885986328, \t Total Dis Loss : 0.0005398355424404144\n",
      "Steps : 78600, \t Total Gen Loss : 26.536434173583984, \t Total Dis Loss : 0.0001991312310565263\n",
      "Steps : 78700, \t Total Gen Loss : 23.520225524902344, \t Total Dis Loss : 0.00030617869924753904\n",
      "Time for epoch 14 is 78.4686975479126 sec\n",
      "Steps : 78800, \t Total Gen Loss : 26.62391471862793, \t Total Dis Loss : 0.00029826757963746786\n",
      "Steps : 78900, \t Total Gen Loss : 26.36783218383789, \t Total Dis Loss : 0.00031694574863649905\n",
      "Steps : 79000, \t Total Gen Loss : 24.367216110229492, \t Total Dis Loss : 0.000313209107844159\n",
      "Steps : 79100, \t Total Gen Loss : 25.141874313354492, \t Total Dis Loss : 0.0003260136872995645\n",
      "Steps : 79200, \t Total Gen Loss : 26.19994354248047, \t Total Dis Loss : 0.0007218787795864046\n",
      "Steps : 79300, \t Total Gen Loss : 22.090946197509766, \t Total Dis Loss : 0.005304031539708376\n",
      "Steps : 79400, \t Total Gen Loss : 25.665851593017578, \t Total Dis Loss : 0.012308715842664242\n",
      "Steps : 79500, \t Total Gen Loss : 27.6499080657959, \t Total Dis Loss : 0.0008933515637181699\n",
      "Steps : 79600, \t Total Gen Loss : 24.14583969116211, \t Total Dis Loss : 0.0002571491932030767\n",
      "Steps : 79700, \t Total Gen Loss : 26.549198150634766, \t Total Dis Loss : 0.0006184307858347893\n",
      "Steps : 79800, \t Total Gen Loss : 22.2691650390625, \t Total Dis Loss : 0.000891636882442981\n",
      "Steps : 79900, \t Total Gen Loss : 26.159963607788086, \t Total Dis Loss : 0.00031156413024291396\n",
      "Steps : 80000, \t Total Gen Loss : 24.69506072998047, \t Total Dis Loss : 0.00022056078887544572\n",
      "Steps : 80100, \t Total Gen Loss : 25.872573852539062, \t Total Dis Loss : 6.907255738042295e-05\n",
      "Steps : 80200, \t Total Gen Loss : 25.60224151611328, \t Total Dis Loss : 8.250208338722587e-05\n",
      "Steps : 80300, \t Total Gen Loss : 28.111244201660156, \t Total Dis Loss : 0.0001806102372938767\n",
      "Steps : 80400, \t Total Gen Loss : 24.896509170532227, \t Total Dis Loss : 7.62943527661264e-05\n",
      "Steps : 80500, \t Total Gen Loss : 24.016260147094727, \t Total Dis Loss : 0.00046053496771492064\n",
      "Steps : 80600, \t Total Gen Loss : 24.272581100463867, \t Total Dis Loss : 0.00010437321907375008\n",
      "Steps : 80700, \t Total Gen Loss : 26.714130401611328, \t Total Dis Loss : 0.00025414032279513776\n",
      "Steps : 80800, \t Total Gen Loss : 23.786441802978516, \t Total Dis Loss : 0.00018187945534009486\n",
      "Steps : 80900, \t Total Gen Loss : 24.72638702392578, \t Total Dis Loss : 0.00010917064355453476\n",
      "Steps : 81000, \t Total Gen Loss : 27.945892333984375, \t Total Dis Loss : 0.0005042094271630049\n",
      "Steps : 81100, \t Total Gen Loss : 25.97451400756836, \t Total Dis Loss : 0.00018463749438524246\n",
      "Steps : 81200, \t Total Gen Loss : 23.16913414001465, \t Total Dis Loss : 0.00030511795193888247\n",
      "Steps : 81300, \t Total Gen Loss : 27.890201568603516, \t Total Dis Loss : 9.527825750410557e-05\n",
      "Steps : 81400, \t Total Gen Loss : 21.316356658935547, \t Total Dis Loss : 0.00029886787524446845\n",
      "Steps : 81500, \t Total Gen Loss : 21.8427734375, \t Total Dis Loss : 0.00022578093921765685\n",
      "Steps : 81600, \t Total Gen Loss : 23.037918090820312, \t Total Dis Loss : 0.0018132546683773398\n",
      "Steps : 81700, \t Total Gen Loss : 24.813331604003906, \t Total Dis Loss : 0.00017296549049206078\n",
      "Steps : 81800, \t Total Gen Loss : 28.465539932250977, \t Total Dis Loss : 0.0003311526379548013\n",
      "Steps : 81900, \t Total Gen Loss : 29.950925827026367, \t Total Dis Loss : 0.0006958929006941617\n",
      "Steps : 82000, \t Total Gen Loss : 25.67673110961914, \t Total Dis Loss : 7.208830356830731e-05\n",
      "Steps : 82100, \t Total Gen Loss : 25.902740478515625, \t Total Dis Loss : 0.00020783899526577443\n",
      "Steps : 82200, \t Total Gen Loss : 25.315303802490234, \t Total Dis Loss : 0.00011513134086271748\n",
      "Steps : 82300, \t Total Gen Loss : 27.94012451171875, \t Total Dis Loss : 8.736395830055699e-05\n",
      "Steps : 82400, \t Total Gen Loss : 24.7242488861084, \t Total Dis Loss : 0.0003068726509809494\n",
      "Steps : 82500, \t Total Gen Loss : 26.16853141784668, \t Total Dis Loss : 0.0008752281428314745\n",
      "Steps : 82600, \t Total Gen Loss : 27.523300170898438, \t Total Dis Loss : 4.055060344398953e-05\n",
      "Steps : 82700, \t Total Gen Loss : 23.25212860107422, \t Total Dis Loss : 0.0004900667699985206\n",
      "Steps : 82800, \t Total Gen Loss : 29.7598876953125, \t Total Dis Loss : 0.00020521393162198365\n",
      "Steps : 82900, \t Total Gen Loss : 21.708328247070312, \t Total Dis Loss : 0.00125812366604805\n",
      "Steps : 83000, \t Total Gen Loss : 22.731891632080078, \t Total Dis Loss : 0.00019639282254502177\n",
      "Steps : 83100, \t Total Gen Loss : 22.788009643554688, \t Total Dis Loss : 0.0007708616903983057\n",
      "Steps : 83200, \t Total Gen Loss : 24.023799896240234, \t Total Dis Loss : 0.000222016082261689\n",
      "Steps : 83300, \t Total Gen Loss : 26.304035186767578, \t Total Dis Loss : 0.00012229717685841024\n",
      "Steps : 83400, \t Total Gen Loss : 21.221023559570312, \t Total Dis Loss : 0.014056708663702011\n",
      "Steps : 83500, \t Total Gen Loss : 26.824337005615234, \t Total Dis Loss : 0.00036950770299881697\n",
      "Steps : 83600, \t Total Gen Loss : 22.63245964050293, \t Total Dis Loss : 0.00031929416581988335\n",
      "Steps : 83700, \t Total Gen Loss : 23.684921264648438, \t Total Dis Loss : 0.00011586127220652997\n",
      "Steps : 83800, \t Total Gen Loss : 28.706249237060547, \t Total Dis Loss : 0.0007949645514599979\n",
      "Steps : 83900, \t Total Gen Loss : 27.797164916992188, \t Total Dis Loss : 2.3939877792145126e-05\n",
      "Steps : 84000, \t Total Gen Loss : 23.88945770263672, \t Total Dis Loss : 0.00013254949590191245\n",
      "Steps : 84100, \t Total Gen Loss : 24.587661743164062, \t Total Dis Loss : 0.00022083484509494156\n",
      "Steps : 84200, \t Total Gen Loss : 27.055431365966797, \t Total Dis Loss : 0.00037015791167505085\n",
      "Steps : 84300, \t Total Gen Loss : 25.96914291381836, \t Total Dis Loss : 0.00013489281991496682\n",
      "Time for epoch 15 is 79.52019309997559 sec\n",
      "Steps : 84400, \t Total Gen Loss : 25.521778106689453, \t Total Dis Loss : 0.0001448563562007621\n",
      "Steps : 84500, \t Total Gen Loss : 23.121183395385742, \t Total Dis Loss : 5.98171645833645e-05\n",
      "Steps : 84600, \t Total Gen Loss : 26.737146377563477, \t Total Dis Loss : 8.628021168988198e-05\n",
      "Steps : 84700, \t Total Gen Loss : 25.124025344848633, \t Total Dis Loss : 3.328445745864883e-05\n",
      "Steps : 84800, \t Total Gen Loss : 22.785999298095703, \t Total Dis Loss : 0.0005043220589868724\n",
      "Steps : 84900, \t Total Gen Loss : 24.743709564208984, \t Total Dis Loss : 0.00018164851644542068\n",
      "Steps : 85000, \t Total Gen Loss : 24.819442749023438, \t Total Dis Loss : 0.00018677879415918142\n",
      "Steps : 85100, \t Total Gen Loss : 25.44831657409668, \t Total Dis Loss : 9.880659490590915e-05\n",
      "Steps : 85200, \t Total Gen Loss : 26.73031234741211, \t Total Dis Loss : 0.00015173810243140906\n",
      "Steps : 85300, \t Total Gen Loss : 28.776222229003906, \t Total Dis Loss : 2.7355674319551326e-05\n",
      "Steps : 85400, \t Total Gen Loss : 23.804203033447266, \t Total Dis Loss : 0.0021710782311856747\n",
      "Steps : 85500, \t Total Gen Loss : 26.733013153076172, \t Total Dis Loss : 7.428685057675466e-05\n",
      "Steps : 85600, \t Total Gen Loss : 25.451194763183594, \t Total Dis Loss : 3.650128928711638e-05\n",
      "Steps : 85700, \t Total Gen Loss : 27.039949417114258, \t Total Dis Loss : 5.6343233154620975e-05\n",
      "Steps : 85800, \t Total Gen Loss : 24.01691436767578, \t Total Dis Loss : 7.718594861216843e-05\n",
      "Steps : 85900, \t Total Gen Loss : 29.20746421813965, \t Total Dis Loss : 6.192587170517072e-05\n",
      "Steps : 86000, \t Total Gen Loss : 26.611785888671875, \t Total Dis Loss : 6.294468039413914e-05\n",
      "Steps : 86100, \t Total Gen Loss : 29.63241958618164, \t Total Dis Loss : 3.382208160473965e-05\n",
      "Steps : 86200, \t Total Gen Loss : 24.515026092529297, \t Total Dis Loss : 0.00017606999608688056\n",
      "Steps : 86300, \t Total Gen Loss : 27.572280883789062, \t Total Dis Loss : 4.548428478301503e-05\n",
      "Steps : 86400, \t Total Gen Loss : 26.980260848999023, \t Total Dis Loss : 0.0015760789392516017\n",
      "Steps : 86500, \t Total Gen Loss : 29.117359161376953, \t Total Dis Loss : 5.786128167528659e-05\n",
      "Steps : 86600, \t Total Gen Loss : 24.22112274169922, \t Total Dis Loss : 8.92759271664545e-05\n",
      "Steps : 86700, \t Total Gen Loss : 26.10189437866211, \t Total Dis Loss : 4.3773961806437e-05\n",
      "Steps : 86800, \t Total Gen Loss : 26.031036376953125, \t Total Dis Loss : 8.224717748817056e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 86900, \t Total Gen Loss : 24.498798370361328, \t Total Dis Loss : 0.00010081489745061845\n",
      "Steps : 87000, \t Total Gen Loss : 28.953510284423828, \t Total Dis Loss : 5.37983542017173e-05\n",
      "Steps : 87100, \t Total Gen Loss : 27.12322425842285, \t Total Dis Loss : 5.877159128431231e-05\n",
      "Steps : 87200, \t Total Gen Loss : 23.605804443359375, \t Total Dis Loss : 7.001868652878329e-05\n",
      "Steps : 87300, \t Total Gen Loss : 27.12704849243164, \t Total Dis Loss : 3.682522947201505e-05\n",
      "Steps : 87400, \t Total Gen Loss : 23.519561767578125, \t Total Dis Loss : 5.968709956505336e-05\n",
      "Steps : 87500, \t Total Gen Loss : 26.361562728881836, \t Total Dis Loss : 7.67618403187953e-05\n",
      "Steps : 87600, \t Total Gen Loss : 25.896785736083984, \t Total Dis Loss : 7.157542131608352e-05\n",
      "Steps : 87700, \t Total Gen Loss : 26.786659240722656, \t Total Dis Loss : 3.0196562875062227e-05\n",
      "Steps : 87800, \t Total Gen Loss : 25.26934242248535, \t Total Dis Loss : 3.563866994227283e-05\n",
      "Steps : 87900, \t Total Gen Loss : 29.312822341918945, \t Total Dis Loss : 0.0016372695099562407\n",
      "Steps : 88000, \t Total Gen Loss : 25.322986602783203, \t Total Dis Loss : 0.0002315585734322667\n",
      "Steps : 88100, \t Total Gen Loss : 26.204715728759766, \t Total Dis Loss : 9.127880912274122e-05\n",
      "Steps : 88200, \t Total Gen Loss : 29.505531311035156, \t Total Dis Loss : 8.67285780259408e-06\n",
      "Steps : 88300, \t Total Gen Loss : 25.566389083862305, \t Total Dis Loss : 1.7491662219981663e-05\n",
      "Steps : 88400, \t Total Gen Loss : 28.56975555419922, \t Total Dis Loss : 3.2685420592315495e-05\n",
      "Steps : 88500, \t Total Gen Loss : 26.475204467773438, \t Total Dis Loss : 4.223939686198719e-05\n",
      "Steps : 88600, \t Total Gen Loss : 24.105823516845703, \t Total Dis Loss : 9.195361781166866e-05\n",
      "Steps : 88700, \t Total Gen Loss : 24.774738311767578, \t Total Dis Loss : 0.00021005808957852423\n",
      "Steps : 88800, \t Total Gen Loss : 24.49338150024414, \t Total Dis Loss : 0.0001914278109325096\n",
      "Steps : 88900, \t Total Gen Loss : 24.19872283935547, \t Total Dis Loss : 0.00013760737783741206\n",
      "Steps : 89000, \t Total Gen Loss : 23.591384887695312, \t Total Dis Loss : 0.0002127278276020661\n",
      "Steps : 89100, \t Total Gen Loss : 26.02423667907715, \t Total Dis Loss : 0.00012818108370993286\n",
      "Steps : 89200, \t Total Gen Loss : 26.122926712036133, \t Total Dis Loss : 4.422414713189937e-05\n",
      "Steps : 89300, \t Total Gen Loss : 26.19369125366211, \t Total Dis Loss : 0.0001500953803770244\n",
      "Steps : 89400, \t Total Gen Loss : 25.684406280517578, \t Total Dis Loss : 0.0001017146569211036\n",
      "Steps : 89500, \t Total Gen Loss : 26.052396774291992, \t Total Dis Loss : 6.151683919597417e-05\n",
      "Steps : 89600, \t Total Gen Loss : 28.532976150512695, \t Total Dis Loss : 3.281877070548944e-05\n",
      "Steps : 89700, \t Total Gen Loss : 26.359947204589844, \t Total Dis Loss : 4.7951627493603155e-05\n",
      "Steps : 89800, \t Total Gen Loss : 28.69357681274414, \t Total Dis Loss : 0.018428215757012367\n",
      "Steps : 89900, \t Total Gen Loss : 35.26313781738281, \t Total Dis Loss : 0.011515365913510323\n",
      "Steps : 90000, \t Total Gen Loss : 34.07634353637695, \t Total Dis Loss : 0.019836708903312683\n",
      "Time for epoch 16 is 79.3688313961029 sec\n",
      "Steps : 90100, \t Total Gen Loss : 35.47174072265625, \t Total Dis Loss : 0.0004345712368376553\n",
      "Steps : 90200, \t Total Gen Loss : 32.52294158935547, \t Total Dis Loss : 0.00046681761159561574\n",
      "Steps : 90300, \t Total Gen Loss : 32.420902252197266, \t Total Dis Loss : 0.0019683227874338627\n",
      "Steps : 90400, \t Total Gen Loss : 28.977813720703125, \t Total Dis Loss : 0.0009963688207790256\n",
      "Steps : 90500, \t Total Gen Loss : 28.939498901367188, \t Total Dis Loss : 0.00038908893475309014\n",
      "Steps : 90600, \t Total Gen Loss : 29.188064575195312, \t Total Dis Loss : 0.00012445257743820548\n",
      "Steps : 90700, \t Total Gen Loss : 29.010894775390625, \t Total Dis Loss : 0.0006329885218292475\n",
      "Steps : 90800, \t Total Gen Loss : 32.51899337768555, \t Total Dis Loss : 0.0004119278455618769\n",
      "Steps : 90900, \t Total Gen Loss : 30.114131927490234, \t Total Dis Loss : 0.000506571086589247\n",
      "Steps : 91000, \t Total Gen Loss : 28.37874984741211, \t Total Dis Loss : 0.0007898186449892819\n",
      "Steps : 91100, \t Total Gen Loss : 25.33759117126465, \t Total Dis Loss : 0.0024079817812889814\n",
      "Steps : 91200, \t Total Gen Loss : 27.713577270507812, \t Total Dis Loss : 0.0001694464881438762\n",
      "Steps : 91300, \t Total Gen Loss : 31.282257080078125, \t Total Dis Loss : 6.199348717927933e-05\n",
      "Steps : 91400, \t Total Gen Loss : 27.946996688842773, \t Total Dis Loss : 3.837373878923245e-05\n",
      "Steps : 91500, \t Total Gen Loss : 24.987319946289062, \t Total Dis Loss : 5.4971635108813643e-05\n",
      "Steps : 91600, \t Total Gen Loss : 31.521236419677734, \t Total Dis Loss : 0.00071698147803545\n",
      "Steps : 91700, \t Total Gen Loss : 25.488628387451172, \t Total Dis Loss : 0.00010222117271041498\n",
      "Steps : 91800, \t Total Gen Loss : 27.259628295898438, \t Total Dis Loss : 0.0002923166030086577\n",
      "Steps : 91900, \t Total Gen Loss : 24.03160285949707, \t Total Dis Loss : 8.789610001258552e-05\n",
      "Steps : 92000, \t Total Gen Loss : 26.16136932373047, \t Total Dis Loss : 8.328688272740692e-05\n",
      "Steps : 92100, \t Total Gen Loss : 25.898834228515625, \t Total Dis Loss : 0.00016281600983347744\n",
      "Steps : 92200, \t Total Gen Loss : 31.30661964416504, \t Total Dis Loss : 0.0001522410602774471\n",
      "Steps : 92300, \t Total Gen Loss : 28.02655792236328, \t Total Dis Loss : 0.0007251095958054066\n",
      "Steps : 92400, \t Total Gen Loss : 28.139272689819336, \t Total Dis Loss : 6.188620318425819e-05\n",
      "Steps : 92500, \t Total Gen Loss : 25.779090881347656, \t Total Dis Loss : 7.645152800250798e-05\n",
      "Steps : 92600, \t Total Gen Loss : 26.488555908203125, \t Total Dis Loss : 0.00018047176126856357\n",
      "Steps : 92700, \t Total Gen Loss : 31.702449798583984, \t Total Dis Loss : 0.0001909566344693303\n",
      "Steps : 92800, \t Total Gen Loss : 29.789310455322266, \t Total Dis Loss : 5.198994040256366e-05\n",
      "Steps : 92900, \t Total Gen Loss : 28.257009506225586, \t Total Dis Loss : 6.76877680234611e-05\n",
      "Steps : 93000, \t Total Gen Loss : 28.04607391357422, \t Total Dis Loss : 2.3487333237426355e-05\n",
      "Steps : 93100, \t Total Gen Loss : 28.64448356628418, \t Total Dis Loss : 0.00011137616820633411\n",
      "Steps : 93200, \t Total Gen Loss : 33.998619079589844, \t Total Dis Loss : 2.5744684535311535e-05\n",
      "Steps : 93300, \t Total Gen Loss : 29.100284576416016, \t Total Dis Loss : 2.972195579786785e-05\n",
      "Steps : 93400, \t Total Gen Loss : 26.18059539794922, \t Total Dis Loss : 0.0007431700942106545\n",
      "Steps : 93500, \t Total Gen Loss : 30.105375289916992, \t Total Dis Loss : 0.000540153996553272\n",
      "Steps : 93600, \t Total Gen Loss : 28.42656707763672, \t Total Dis Loss : 0.0001030779822031036\n",
      "Steps : 93700, \t Total Gen Loss : 27.46642303466797, \t Total Dis Loss : 0.0005051561165601015\n",
      "Steps : 93800, \t Total Gen Loss : 25.32718276977539, \t Total Dis Loss : 0.00013406384096015245\n",
      "Steps : 93900, \t Total Gen Loss : 26.417964935302734, \t Total Dis Loss : 0.00010582167305983603\n",
      "Steps : 94000, \t Total Gen Loss : 28.333223342895508, \t Total Dis Loss : 8.247062942245975e-05\n",
      "Steps : 94100, \t Total Gen Loss : 25.000141143798828, \t Total Dis Loss : 0.00018668430857360363\n",
      "Steps : 94200, \t Total Gen Loss : 26.013874053955078, \t Total Dis Loss : 0.0022238274104893208\n",
      "Steps : 94300, \t Total Gen Loss : 27.492210388183594, \t Total Dis Loss : 0.00016967536066658795\n",
      "Steps : 94400, \t Total Gen Loss : 26.796363830566406, \t Total Dis Loss : 1.768050788086839e-05\n",
      "Steps : 94500, \t Total Gen Loss : 27.27940559387207, \t Total Dis Loss : 6.272191239986569e-05\n",
      "Steps : 94600, \t Total Gen Loss : 28.119117736816406, \t Total Dis Loss : 0.00011658030416583642\n",
      "Steps : 94700, \t Total Gen Loss : 24.944856643676758, \t Total Dis Loss : 0.0006359193939715624\n",
      "Steps : 94800, \t Total Gen Loss : 24.12631607055664, \t Total Dis Loss : 0.00014253499102778733\n",
      "Steps : 94900, \t Total Gen Loss : 25.41826820373535, \t Total Dis Loss : 0.00027580629102885723\n",
      "Steps : 95000, \t Total Gen Loss : 24.632423400878906, \t Total Dis Loss : 7.688468758715317e-05\n",
      "Steps : 95100, \t Total Gen Loss : 25.858509063720703, \t Total Dis Loss : 7.349170482484624e-05\n",
      "Steps : 95200, \t Total Gen Loss : 27.488576889038086, \t Total Dis Loss : 0.0002221215981990099\n",
      "Steps : 95300, \t Total Gen Loss : 25.469924926757812, \t Total Dis Loss : 8.936614904087037e-05\n",
      "Steps : 95400, \t Total Gen Loss : 28.673669815063477, \t Total Dis Loss : 0.00016571846208535135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 95500, \t Total Gen Loss : 21.964534759521484, \t Total Dis Loss : 0.00027523154858499765\n",
      "Steps : 95600, \t Total Gen Loss : 26.752155303955078, \t Total Dis Loss : 0.00031238459632731974\n",
      "Time for epoch 17 is 75.83030271530151 sec\n",
      "Steps : 95700, \t Total Gen Loss : 26.46480941772461, \t Total Dis Loss : 0.00010874750296352431\n",
      "Steps : 95800, \t Total Gen Loss : 24.604021072387695, \t Total Dis Loss : 0.0003061772440560162\n",
      "Steps : 95900, \t Total Gen Loss : 23.612581253051758, \t Total Dis Loss : 0.00035060953814536333\n",
      "Steps : 96000, \t Total Gen Loss : 24.672279357910156, \t Total Dis Loss : 0.00010309230128768831\n",
      "Steps : 96100, \t Total Gen Loss : 25.366622924804688, \t Total Dis Loss : 0.0001776084245648235\n",
      "Steps : 96200, \t Total Gen Loss : 25.21342658996582, \t Total Dis Loss : 0.00010397320875199512\n",
      "Steps : 96300, \t Total Gen Loss : 25.126502990722656, \t Total Dis Loss : 8.614541729912162e-05\n",
      "Steps : 96400, \t Total Gen Loss : 26.108253479003906, \t Total Dis Loss : 3.161979475407861e-05\n",
      "Steps : 96500, \t Total Gen Loss : 26.199752807617188, \t Total Dis Loss : 0.0005675128195434809\n",
      "Steps : 96600, \t Total Gen Loss : 29.46477699279785, \t Total Dis Loss : 0.0002158714341931045\n",
      "Steps : 96700, \t Total Gen Loss : 25.656417846679688, \t Total Dis Loss : 0.00023248279467225075\n",
      "Steps : 96800, \t Total Gen Loss : 22.84232521057129, \t Total Dis Loss : 0.00042636209400370717\n",
      "Steps : 96900, \t Total Gen Loss : 28.222599029541016, \t Total Dis Loss : 9.96081143966876e-05\n",
      "Steps : 97000, \t Total Gen Loss : 23.369327545166016, \t Total Dis Loss : 0.07132560014724731\n",
      "Steps : 97100, \t Total Gen Loss : 23.578357696533203, \t Total Dis Loss : 0.0002051870251307264\n",
      "Steps : 97200, \t Total Gen Loss : 27.927690505981445, \t Total Dis Loss : 4.4860400521429256e-05\n",
      "Steps : 97300, \t Total Gen Loss : 25.242767333984375, \t Total Dis Loss : 9.886507905321196e-05\n",
      "Steps : 97400, \t Total Gen Loss : 25.81966781616211, \t Total Dis Loss : 4.321174856158905e-05\n",
      "Steps : 97500, \t Total Gen Loss : 27.74352264404297, \t Total Dis Loss : 4.292322773835622e-05\n",
      "Steps : 97600, \t Total Gen Loss : 29.85622787475586, \t Total Dis Loss : 7.97760731074959e-05\n",
      "Steps : 97700, \t Total Gen Loss : 24.275169372558594, \t Total Dis Loss : 0.0001321330782957375\n",
      "Steps : 97800, \t Total Gen Loss : 26.496028900146484, \t Total Dis Loss : 0.00039706419920548797\n",
      "Steps : 97900, \t Total Gen Loss : 26.008155822753906, \t Total Dis Loss : 5.708637399948202e-05\n",
      "Steps : 98000, \t Total Gen Loss : 30.08072280883789, \t Total Dis Loss : 9.657753980718553e-05\n",
      "Steps : 98100, \t Total Gen Loss : 25.90786361694336, \t Total Dis Loss : 9.895917173707858e-05\n",
      "Steps : 98200, \t Total Gen Loss : 25.64017677307129, \t Total Dis Loss : 0.00015572147094644606\n",
      "Steps : 98300, \t Total Gen Loss : 26.577917098999023, \t Total Dis Loss : 6.658291385974735e-05\n",
      "Steps : 98400, \t Total Gen Loss : 27.95154571533203, \t Total Dis Loss : 9.472305828239769e-05\n",
      "Steps : 98500, \t Total Gen Loss : 26.225997924804688, \t Total Dis Loss : 0.00032387737883254886\n",
      "Steps : 98600, \t Total Gen Loss : 25.276535034179688, \t Total Dis Loss : 9.863419836619869e-05\n",
      "Steps : 98700, \t Total Gen Loss : 28.870670318603516, \t Total Dis Loss : 1.5994799468899146e-05\n",
      "Steps : 98800, \t Total Gen Loss : 24.229597091674805, \t Total Dis Loss : 3.538820237736218e-05\n",
      "Steps : 98900, \t Total Gen Loss : 22.749679565429688, \t Total Dis Loss : 0.0003061014285776764\n",
      "Steps : 99000, \t Total Gen Loss : 28.0952205657959, \t Total Dis Loss : 8.095747034531087e-05\n",
      "Steps : 99100, \t Total Gen Loss : 27.828964233398438, \t Total Dis Loss : 7.173415360739455e-05\n",
      "Steps : 99200, \t Total Gen Loss : 25.93297004699707, \t Total Dis Loss : 9.713215695228428e-05\n",
      "Steps : 99300, \t Total Gen Loss : 24.419692993164062, \t Total Dis Loss : 2.7479469281388447e-05\n",
      "Steps : 99400, \t Total Gen Loss : 24.52471923828125, \t Total Dis Loss : 8.424637780990452e-05\n",
      "Steps : 99500, \t Total Gen Loss : 25.174983978271484, \t Total Dis Loss : 3.579480471671559e-05\n",
      "Steps : 99600, \t Total Gen Loss : 28.763721466064453, \t Total Dis Loss : 1.8042859665001743e-05\n",
      "Steps : 99700, \t Total Gen Loss : 30.00323486328125, \t Total Dis Loss : 1.3537269296648446e-05\n",
      "Steps : 99800, \t Total Gen Loss : 31.599031448364258, \t Total Dis Loss : 2.749227496678941e-05\n",
      "Steps : 99900, \t Total Gen Loss : 26.979820251464844, \t Total Dis Loss : 3.0051583962631412e-05\n",
      "Steps : 100000, \t Total Gen Loss : 27.615245819091797, \t Total Dis Loss : 9.18112782528624e-05\n",
      "Steps : 100100, \t Total Gen Loss : 26.37370491027832, \t Total Dis Loss : 3.033836037502624e-05\n",
      "Steps : 100200, \t Total Gen Loss : 33.12347412109375, \t Total Dis Loss : 0.047735899686813354\n",
      "Steps : 100300, \t Total Gen Loss : 31.653911590576172, \t Total Dis Loss : 1.7442440366721712e-05\n",
      "Steps : 100400, \t Total Gen Loss : 33.558570861816406, \t Total Dis Loss : 1.6917576431296766e-05\n",
      "Steps : 100500, \t Total Gen Loss : 26.080402374267578, \t Total Dis Loss : 0.02779487892985344\n",
      "Steps : 100600, \t Total Gen Loss : 25.81870460510254, \t Total Dis Loss : 9.121452603721991e-05\n",
      "Steps : 100700, \t Total Gen Loss : 26.889497756958008, \t Total Dis Loss : 3.235578697058372e-05\n",
      "Steps : 100800, \t Total Gen Loss : 25.32855796813965, \t Total Dis Loss : 5.316870374372229e-05\n",
      "Steps : 100900, \t Total Gen Loss : 27.341339111328125, \t Total Dis Loss : 3.9300095522776246e-05\n",
      "Steps : 101000, \t Total Gen Loss : 29.361635208129883, \t Total Dis Loss : 2.2961194190429524e-05\n",
      "Steps : 101100, \t Total Gen Loss : 25.292572021484375, \t Total Dis Loss : 8.201995660783723e-05\n",
      "Steps : 101200, \t Total Gen Loss : 26.45746421813965, \t Total Dis Loss : 0.00014341599307954311\n",
      "Time for epoch 18 is 74.03360414505005 sec\n",
      "Steps : 101300, \t Total Gen Loss : 26.729785919189453, \t Total Dis Loss : 1.2102105756639503e-05\n",
      "Steps : 101400, \t Total Gen Loss : 29.850934982299805, \t Total Dis Loss : 5.988965494907461e-05\n",
      "Steps : 101500, \t Total Gen Loss : 28.946897506713867, \t Total Dis Loss : 1.807067746995017e-05\n",
      "Steps : 101600, \t Total Gen Loss : 24.95771026611328, \t Total Dis Loss : 2.639688682393171e-05\n",
      "Steps : 101700, \t Total Gen Loss : 28.84754180908203, \t Total Dis Loss : 7.563918188679963e-06\n",
      "Steps : 101800, \t Total Gen Loss : 24.76202392578125, \t Total Dis Loss : 9.857612167252228e-05\n",
      "Steps : 101900, \t Total Gen Loss : 24.454240798950195, \t Total Dis Loss : 7.238367106765509e-05\n",
      "Steps : 102000, \t Total Gen Loss : 26.189542770385742, \t Total Dis Loss : 0.00011221088789170608\n",
      "Steps : 102100, \t Total Gen Loss : 22.914291381835938, \t Total Dis Loss : 0.00016552787565160543\n",
      "Steps : 102200, \t Total Gen Loss : 28.000408172607422, \t Total Dis Loss : 2.6867030101129785e-05\n",
      "Steps : 102300, \t Total Gen Loss : 34.11799240112305, \t Total Dis Loss : 0.002768008504062891\n",
      "Steps : 102400, \t Total Gen Loss : 25.468524932861328, \t Total Dis Loss : 0.0013256208039820194\n",
      "Steps : 102500, \t Total Gen Loss : 22.562992095947266, \t Total Dis Loss : 0.00014127357280813158\n",
      "Steps : 102600, \t Total Gen Loss : 24.832763671875, \t Total Dis Loss : 0.0007520118961110711\n",
      "Steps : 102700, \t Total Gen Loss : 27.2464599609375, \t Total Dis Loss : 0.0004133226175326854\n",
      "Steps : 102800, \t Total Gen Loss : 28.079856872558594, \t Total Dis Loss : 7.266568718478084e-05\n",
      "Steps : 102900, \t Total Gen Loss : 22.6243953704834, \t Total Dis Loss : 0.002172754378989339\n",
      "Steps : 103000, \t Total Gen Loss : 25.523014068603516, \t Total Dis Loss : 0.0002527829783502966\n",
      "Steps : 103100, \t Total Gen Loss : 26.8104248046875, \t Total Dis Loss : 0.0001438510516891256\n",
      "Steps : 103200, \t Total Gen Loss : 24.707103729248047, \t Total Dis Loss : 0.0006632939912378788\n",
      "Steps : 103300, \t Total Gen Loss : 25.773635864257812, \t Total Dis Loss : 4.673019429901615e-05\n",
      "Steps : 103400, \t Total Gen Loss : 27.713973999023438, \t Total Dis Loss : 5.031964246882126e-05\n",
      "Steps : 103500, \t Total Gen Loss : 30.143436431884766, \t Total Dis Loss : 0.0013407162623479962\n",
      "Steps : 103600, \t Total Gen Loss : 26.492332458496094, \t Total Dis Loss : 0.00025768301566131413\n",
      "Steps : 103700, \t Total Gen Loss : 24.0416259765625, \t Total Dis Loss : 0.005734900012612343\n",
      "Steps : 103800, \t Total Gen Loss : 25.086585998535156, \t Total Dis Loss : 8.623423491371796e-05\n",
      "Steps : 103900, \t Total Gen Loss : 25.761632919311523, \t Total Dis Loss : 0.00010265366290695965\n",
      "Steps : 104000, \t Total Gen Loss : 25.88172721862793, \t Total Dis Loss : 9.350914479000494e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 104100, \t Total Gen Loss : 27.699710845947266, \t Total Dis Loss : 9.600708290236071e-05\n",
      "Steps : 104200, \t Total Gen Loss : 26.107105255126953, \t Total Dis Loss : 5.663034608005546e-05\n",
      "Steps : 104300, \t Total Gen Loss : 27.616878509521484, \t Total Dis Loss : 3.822216604021378e-05\n",
      "Steps : 104400, \t Total Gen Loss : 27.347991943359375, \t Total Dis Loss : 2.256574407510925e-05\n",
      "Steps : 104500, \t Total Gen Loss : 25.78097152709961, \t Total Dis Loss : 0.00011789999553002417\n",
      "Steps : 104600, \t Total Gen Loss : 24.45915985107422, \t Total Dis Loss : 5.56332161067985e-05\n",
      "Steps : 104700, \t Total Gen Loss : 28.57312774658203, \t Total Dis Loss : 7.160981476772577e-05\n",
      "Steps : 104800, \t Total Gen Loss : 25.868167877197266, \t Total Dis Loss : 5.6415836297674105e-05\n",
      "Steps : 104900, \t Total Gen Loss : 27.693065643310547, \t Total Dis Loss : 3.600942363846116e-05\n",
      "Steps : 105000, \t Total Gen Loss : 28.023271560668945, \t Total Dis Loss : 1.0769704203994479e-05\n",
      "Steps : 105100, \t Total Gen Loss : 33.11341857910156, \t Total Dis Loss : 0.00022197728685569018\n",
      "Steps : 105200, \t Total Gen Loss : 32.39186096191406, \t Total Dis Loss : 0.00010684067092370242\n",
      "Steps : 105300, \t Total Gen Loss : 30.750532150268555, \t Total Dis Loss : 1.1379854186088778e-05\n",
      "Steps : 105400, \t Total Gen Loss : 29.415674209594727, \t Total Dis Loss : 5.122922630107496e-06\n",
      "Steps : 105500, \t Total Gen Loss : 29.626556396484375, \t Total Dis Loss : 2.2840571546112187e-05\n",
      "Steps : 105600, \t Total Gen Loss : 26.36911392211914, \t Total Dis Loss : 6.843287701485679e-05\n",
      "Steps : 105700, \t Total Gen Loss : 27.612350463867188, \t Total Dis Loss : 2.0504387066466734e-05\n",
      "Steps : 105800, \t Total Gen Loss : 27.220256805419922, \t Total Dis Loss : 1.593533124832902e-05\n",
      "Steps : 105900, \t Total Gen Loss : 28.52619171142578, \t Total Dis Loss : 3.405886673135683e-05\n",
      "Steps : 106000, \t Total Gen Loss : 28.37271499633789, \t Total Dis Loss : 1.919736314448528e-05\n",
      "Steps : 106100, \t Total Gen Loss : 29.891897201538086, \t Total Dis Loss : 1.4451597053266596e-05\n",
      "Steps : 106200, \t Total Gen Loss : 29.65059471130371, \t Total Dis Loss : 1.909926322696265e-05\n",
      "Steps : 106300, \t Total Gen Loss : 28.502010345458984, \t Total Dis Loss : 6.349966042762389e-06\n",
      "Steps : 106400, \t Total Gen Loss : 26.222034454345703, \t Total Dis Loss : 1.9048900867346674e-05\n",
      "Steps : 106500, \t Total Gen Loss : 28.284170150756836, \t Total Dis Loss : 7.3212413553847e-06\n",
      "Steps : 106600, \t Total Gen Loss : 25.356292724609375, \t Total Dis Loss : 8.825445547699928e-06\n",
      "Steps : 106700, \t Total Gen Loss : 21.83141326904297, \t Total Dis Loss : 0.0005723360809497535\n",
      "Steps : 106800, \t Total Gen Loss : 28.475013732910156, \t Total Dis Loss : 2.8168449716758914e-05\n",
      "Time for epoch 19 is 78.05701398849487 sec\n",
      "Steps : 106900, \t Total Gen Loss : 24.712135314941406, \t Total Dis Loss : 7.771544915158302e-05\n",
      "Steps : 107000, \t Total Gen Loss : 26.767187118530273, \t Total Dis Loss : 0.000577777624130249\n",
      "Steps : 107100, \t Total Gen Loss : 28.884557723999023, \t Total Dis Loss : 0.00011003823601640761\n",
      "Steps : 107200, \t Total Gen Loss : 25.667991638183594, \t Total Dis Loss : 0.00102898757904768\n",
      "Steps : 107300, \t Total Gen Loss : 25.36102294921875, \t Total Dis Loss : 0.0005065393052063882\n",
      "Steps : 107400, \t Total Gen Loss : 24.038497924804688, \t Total Dis Loss : 0.00016696487728040665\n",
      "Steps : 107500, \t Total Gen Loss : 23.28590202331543, \t Total Dis Loss : 0.00277971220202744\n",
      "Steps : 107600, \t Total Gen Loss : 24.717756271362305, \t Total Dis Loss : 0.00011154029198223725\n",
      "Steps : 107700, \t Total Gen Loss : 27.828750610351562, \t Total Dis Loss : 6.678551289951429e-05\n",
      "Steps : 107800, \t Total Gen Loss : 23.68018341064453, \t Total Dis Loss : 0.00017786557145882398\n",
      "Steps : 107900, \t Total Gen Loss : 28.246200561523438, \t Total Dis Loss : 7.387938967440277e-05\n",
      "Steps : 108000, \t Total Gen Loss : 25.468772888183594, \t Total Dis Loss : 0.0002887963782995939\n",
      "Steps : 108100, \t Total Gen Loss : 24.709842681884766, \t Total Dis Loss : 0.003795756259933114\n",
      "Steps : 108200, \t Total Gen Loss : 26.558090209960938, \t Total Dis Loss : 1.4952798665035516e-05\n",
      "Steps : 108300, \t Total Gen Loss : 26.24068260192871, \t Total Dis Loss : 4.774369881488383e-05\n",
      "Steps : 108400, \t Total Gen Loss : 30.422592163085938, \t Total Dis Loss : 2.0657211280195042e-05\n",
      "Steps : 108500, \t Total Gen Loss : 26.4375, \t Total Dis Loss : 0.00012486819468904287\n",
      "Steps : 108600, \t Total Gen Loss : 28.057811737060547, \t Total Dis Loss : 6.57941636745818e-05\n",
      "Steps : 108700, \t Total Gen Loss : 25.123046875, \t Total Dis Loss : 0.0001580646785441786\n",
      "Steps : 108800, \t Total Gen Loss : 28.627864837646484, \t Total Dis Loss : 0.00017701523029245436\n",
      "Steps : 108900, \t Total Gen Loss : 29.187259674072266, \t Total Dis Loss : 6.196684262249619e-05\n",
      "Steps : 109000, \t Total Gen Loss : 26.877849578857422, \t Total Dis Loss : 6.463276804424822e-05\n",
      "Steps : 109100, \t Total Gen Loss : 25.22848129272461, \t Total Dis Loss : 0.0002792582381516695\n",
      "Steps : 109200, \t Total Gen Loss : 24.549802780151367, \t Total Dis Loss : 0.0002600876905489713\n",
      "Steps : 109300, \t Total Gen Loss : 26.159835815429688, \t Total Dis Loss : 4.742430610349402e-05\n",
      "Steps : 109400, \t Total Gen Loss : 25.079317092895508, \t Total Dis Loss : 0.00011572681250981987\n",
      "Steps : 109500, \t Total Gen Loss : 22.743732452392578, \t Total Dis Loss : 0.0018674497259780765\n",
      "Steps : 109600, \t Total Gen Loss : 30.063140869140625, \t Total Dis Loss : 8.952494681579992e-05\n",
      "Steps : 109700, \t Total Gen Loss : 25.756797790527344, \t Total Dis Loss : 0.00031634914921596646\n",
      "Steps : 109800, \t Total Gen Loss : 25.9874210357666, \t Total Dis Loss : 0.0003739930980373174\n",
      "Steps : 109900, \t Total Gen Loss : 29.275043487548828, \t Total Dis Loss : 5.480912659550086e-05\n",
      "Steps : 110000, \t Total Gen Loss : 25.870630264282227, \t Total Dis Loss : 7.664019358344376e-05\n",
      "Steps : 110100, \t Total Gen Loss : 28.536422729492188, \t Total Dis Loss : 2.791423503367696e-05\n",
      "Steps : 110200, \t Total Gen Loss : 27.310741424560547, \t Total Dis Loss : 4.997658834327012e-05\n",
      "Steps : 110300, \t Total Gen Loss : 26.98876190185547, \t Total Dis Loss : 2.692677480808925e-05\n",
      "Steps : 110400, \t Total Gen Loss : 25.404367446899414, \t Total Dis Loss : 0.0006251146551221609\n",
      "Steps : 110500, \t Total Gen Loss : 27.90882110595703, \t Total Dis Loss : 0.00510485889390111\n",
      "Steps : 110600, \t Total Gen Loss : 25.671735763549805, \t Total Dis Loss : 0.000136225251480937\n",
      "Steps : 110700, \t Total Gen Loss : 27.256122589111328, \t Total Dis Loss : 3.412760270293802e-05\n",
      "Steps : 110800, \t Total Gen Loss : 29.105506896972656, \t Total Dis Loss : 6.918661529198289e-05\n",
      "Steps : 110900, \t Total Gen Loss : 25.45700454711914, \t Total Dis Loss : 6.031738666933961e-05\n",
      "Steps : 111000, \t Total Gen Loss : 26.827205657958984, \t Total Dis Loss : 3.107866723439656e-05\n",
      "Steps : 111100, \t Total Gen Loss : 27.847030639648438, \t Total Dis Loss : 4.380004611448385e-05\n",
      "Steps : 111200, \t Total Gen Loss : 24.088150024414062, \t Total Dis Loss : 0.00046468491200357676\n",
      "Steps : 111300, \t Total Gen Loss : 33.740421295166016, \t Total Dis Loss : 0.0020632189698517323\n",
      "Steps : 111400, \t Total Gen Loss : 31.184650421142578, \t Total Dis Loss : 4.618080311047379e-06\n",
      "Steps : 111500, \t Total Gen Loss : 32.07079315185547, \t Total Dis Loss : 1.5704763427493162e-05\n",
      "Steps : 111600, \t Total Gen Loss : 31.33411407470703, \t Total Dis Loss : 2.6401825380162336e-05\n",
      "Steps : 111700, \t Total Gen Loss : 28.167949676513672, \t Total Dis Loss : 3.1147337722359225e-05\n",
      "Steps : 111800, \t Total Gen Loss : 33.017181396484375, \t Total Dis Loss : 9.834253432927653e-06\n",
      "Steps : 111900, \t Total Gen Loss : 28.537473678588867, \t Total Dis Loss : 1.1067108061979525e-05\n",
      "Steps : 112000, \t Total Gen Loss : 30.606021881103516, \t Total Dis Loss : 1.954336994458572e-06\n",
      "Steps : 112100, \t Total Gen Loss : 30.639257431030273, \t Total Dis Loss : 4.376731885713525e-05\n",
      "Steps : 112200, \t Total Gen Loss : 26.13740348815918, \t Total Dis Loss : 4.1333823901368305e-06\n",
      "Steps : 112300, \t Total Gen Loss : 25.922592163085938, \t Total Dis Loss : 0.019418805837631226\n",
      "Steps : 112400, \t Total Gen Loss : 24.746219635009766, \t Total Dis Loss : 0.00046443118480965495\n",
      "Steps : 112500, \t Total Gen Loss : 26.07137680053711, \t Total Dis Loss : 3.452609962550923e-05\n",
      "Time for epoch 20 is 80.35461521148682 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 112600, \t Total Gen Loss : 24.947324752807617, \t Total Dis Loss : 0.00012029088975396007\n",
      "Steps : 112700, \t Total Gen Loss : 30.08465576171875, \t Total Dis Loss : 4.454633381101303e-05\n",
      "Steps : 112800, \t Total Gen Loss : 27.834651947021484, \t Total Dis Loss : 8.828238787828013e-05\n",
      "Steps : 112900, \t Total Gen Loss : 27.2927188873291, \t Total Dis Loss : 3.4944871003972366e-05\n",
      "Steps : 113000, \t Total Gen Loss : 26.66472625732422, \t Total Dis Loss : 0.0002457398804835975\n",
      "Steps : 113100, \t Total Gen Loss : 23.64754295349121, \t Total Dis Loss : 0.000133609923068434\n",
      "Steps : 113200, \t Total Gen Loss : 28.773208618164062, \t Total Dis Loss : 4.571698809741065e-05\n",
      "Steps : 113300, \t Total Gen Loss : 26.99114227294922, \t Total Dis Loss : 6.192435103002936e-05\n",
      "Steps : 113400, \t Total Gen Loss : 25.89117431640625, \t Total Dis Loss : 4.9790291086537763e-05\n",
      "Steps : 113500, \t Total Gen Loss : 26.312238693237305, \t Total Dis Loss : 5.513928772415966e-05\n",
      "Steps : 113600, \t Total Gen Loss : 26.98012351989746, \t Total Dis Loss : 3.743239358300343e-05\n",
      "Steps : 113700, \t Total Gen Loss : 25.399734497070312, \t Total Dis Loss : 6.915663107065484e-05\n",
      "Steps : 113800, \t Total Gen Loss : 26.437297821044922, \t Total Dis Loss : 6.378197576850653e-05\n",
      "Steps : 113900, \t Total Gen Loss : 26.252826690673828, \t Total Dis Loss : 0.0001230343186762184\n",
      "Steps : 114000, \t Total Gen Loss : 23.499553680419922, \t Total Dis Loss : 0.0001158788800239563\n",
      "Steps : 114100, \t Total Gen Loss : 24.796764373779297, \t Total Dis Loss : 8.026065188460052e-05\n",
      "Steps : 114200, \t Total Gen Loss : 27.237985610961914, \t Total Dis Loss : 4.0244827687274665e-05\n",
      "Steps : 114300, \t Total Gen Loss : 26.148088455200195, \t Total Dis Loss : 5.7215500419260934e-05\n",
      "Steps : 114400, \t Total Gen Loss : 26.46994972229004, \t Total Dis Loss : 3.329214814584702e-05\n",
      "Steps : 114500, \t Total Gen Loss : 26.937602996826172, \t Total Dis Loss : 1.7143260265584104e-05\n",
      "Steps : 114600, \t Total Gen Loss : 25.308597564697266, \t Total Dis Loss : 7.232370262499899e-05\n",
      "Steps : 114700, \t Total Gen Loss : 26.55364990234375, \t Total Dis Loss : 6.148500688141212e-05\n",
      "Steps : 114800, \t Total Gen Loss : 25.509689331054688, \t Total Dis Loss : 6.727210711687803e-05\n",
      "Steps : 114900, \t Total Gen Loss : 28.248605728149414, \t Total Dis Loss : 5.310638516675681e-05\n",
      "Steps : 115000, \t Total Gen Loss : 27.622570037841797, \t Total Dis Loss : 0.00044086246634833515\n",
      "Steps : 115100, \t Total Gen Loss : 30.81735610961914, \t Total Dis Loss : 2.0228686480550095e-05\n",
      "Steps : 115200, \t Total Gen Loss : 28.209285736083984, \t Total Dis Loss : 0.002091360976919532\n",
      "Steps : 115300, \t Total Gen Loss : 26.167869567871094, \t Total Dis Loss : 2.9995870136190206e-05\n",
      "Steps : 115400, \t Total Gen Loss : 27.73348617553711, \t Total Dis Loss : 5.146058902028017e-05\n",
      "Steps : 115500, \t Total Gen Loss : 28.114967346191406, \t Total Dis Loss : 0.016550647094845772\n",
      "Steps : 115600, \t Total Gen Loss : 25.14422035217285, \t Total Dis Loss : 0.0007665216107852757\n",
      "Steps : 115700, \t Total Gen Loss : 23.680355072021484, \t Total Dis Loss : 0.0008042562985792756\n",
      "Steps : 115800, \t Total Gen Loss : 25.471725463867188, \t Total Dis Loss : 0.0007572289323434234\n",
      "Steps : 115900, \t Total Gen Loss : 26.30563735961914, \t Total Dis Loss : 0.0025458289310336113\n",
      "Steps : 116000, \t Total Gen Loss : 27.966033935546875, \t Total Dis Loss : 0.001340949209406972\n",
      "Steps : 116100, \t Total Gen Loss : 24.752607345581055, \t Total Dis Loss : 7.956126501085237e-05\n",
      "Steps : 116200, \t Total Gen Loss : 29.546648025512695, \t Total Dis Loss : 7.059600466163829e-05\n",
      "Steps : 116300, \t Total Gen Loss : 24.387981414794922, \t Total Dis Loss : 0.00012018565757898614\n",
      "Steps : 116400, \t Total Gen Loss : 26.2432804107666, \t Total Dis Loss : 0.00022178659855853766\n",
      "Steps : 116500, \t Total Gen Loss : 25.9791202545166, \t Total Dis Loss : 0.00020115991355851293\n",
      "Steps : 116600, \t Total Gen Loss : 27.25946807861328, \t Total Dis Loss : 0.0001514089817646891\n",
      "Steps : 116700, \t Total Gen Loss : 24.64425277709961, \t Total Dis Loss : 5.497152960742824e-05\n",
      "Steps : 116800, \t Total Gen Loss : 25.755298614501953, \t Total Dis Loss : 5.751090429839678e-05\n",
      "Steps : 116900, \t Total Gen Loss : 25.2542724609375, \t Total Dis Loss : 2.763389056781307e-05\n",
      "Steps : 117000, \t Total Gen Loss : 29.669084548950195, \t Total Dis Loss : 4.386494401842356e-05\n",
      "Steps : 117100, \t Total Gen Loss : 26.540075302124023, \t Total Dis Loss : 0.000912337563931942\n",
      "Steps : 117200, \t Total Gen Loss : 24.478111267089844, \t Total Dis Loss : 8.179818541975692e-05\n",
      "Steps : 117300, \t Total Gen Loss : 27.85150718688965, \t Total Dis Loss : 0.00014658081636298448\n",
      "Steps : 117400, \t Total Gen Loss : 25.453201293945312, \t Total Dis Loss : 5.2129824325675145e-05\n",
      "Steps : 117500, \t Total Gen Loss : 26.569496154785156, \t Total Dis Loss : 4.2125044274143875e-05\n",
      "Steps : 117600, \t Total Gen Loss : 25.57275390625, \t Total Dis Loss : 2.769352067844011e-05\n",
      "Steps : 117700, \t Total Gen Loss : 26.646163940429688, \t Total Dis Loss : 6.338772072922438e-05\n",
      "Steps : 117800, \t Total Gen Loss : 38.37903594970703, \t Total Dis Loss : 0.0015220287023112178\n",
      "Steps : 117900, \t Total Gen Loss : 22.207744598388672, \t Total Dis Loss : 0.10425757616758347\n",
      "Steps : 118000, \t Total Gen Loss : 38.55276870727539, \t Total Dis Loss : 0.00022178582730703056\n",
      "Steps : 118100, \t Total Gen Loss : 34.05516815185547, \t Total Dis Loss : 0.00024386506993323565\n",
      "Time for epoch 21 is 78.57607531547546 sec\n",
      "Steps : 118200, \t Total Gen Loss : 32.68943786621094, \t Total Dis Loss : 0.0007794824196025729\n",
      "Steps : 118300, \t Total Gen Loss : 30.091691970825195, \t Total Dis Loss : 0.0011418720241636038\n",
      "Steps : 118400, \t Total Gen Loss : 27.647659301757812, \t Total Dis Loss : 0.007298337295651436\n",
      "Steps : 118500, \t Total Gen Loss : 29.611278533935547, \t Total Dis Loss : 0.03587185963988304\n",
      "Steps : 118600, \t Total Gen Loss : 32.13648223876953, \t Total Dis Loss : 0.0003412365331314504\n",
      "Steps : 118700, \t Total Gen Loss : 30.632518768310547, \t Total Dis Loss : 0.0012849108316004276\n",
      "Steps : 118800, \t Total Gen Loss : 30.38898468017578, \t Total Dis Loss : 8.831196464598179e-05\n",
      "Steps : 118900, \t Total Gen Loss : 30.742847442626953, \t Total Dis Loss : 0.00012967846123501658\n",
      "Steps : 119000, \t Total Gen Loss : 30.91156768798828, \t Total Dis Loss : 0.00043301889672875404\n",
      "Steps : 119100, \t Total Gen Loss : 30.103008270263672, \t Total Dis Loss : 0.00011074227222707123\n",
      "Steps : 119200, \t Total Gen Loss : 29.911972045898438, \t Total Dis Loss : 0.00027059426065534353\n",
      "Steps : 119300, \t Total Gen Loss : 29.84209632873535, \t Total Dis Loss : 0.0010732390219345689\n",
      "Steps : 119400, \t Total Gen Loss : 32.88316345214844, \t Total Dis Loss : 2.3485805286327377e-05\n",
      "Steps : 119500, \t Total Gen Loss : 30.171146392822266, \t Total Dis Loss : 5.5055668781278655e-05\n",
      "Steps : 119600, \t Total Gen Loss : 31.279918670654297, \t Total Dis Loss : 0.0007559738587588072\n",
      "Steps : 119700, \t Total Gen Loss : 23.579296112060547, \t Total Dis Loss : 1.1251848936080933\n",
      "Steps : 119800, \t Total Gen Loss : 33.65190887451172, \t Total Dis Loss : 0.00011892821203218773\n",
      "Steps : 119900, \t Total Gen Loss : 30.92672348022461, \t Total Dis Loss : 0.010233213193714619\n",
      "Steps : 120000, \t Total Gen Loss : 29.94642448425293, \t Total Dis Loss : 0.000132908666273579\n",
      "Steps : 120100, \t Total Gen Loss : 32.119293212890625, \t Total Dis Loss : 0.00021891805226914585\n",
      "Steps : 120200, \t Total Gen Loss : 32.00865173339844, \t Total Dis Loss : 1.2768911801686045e-05\n",
      "Steps : 120300, \t Total Gen Loss : 26.939998626708984, \t Total Dis Loss : 0.00021308987925294787\n",
      "Steps : 120400, \t Total Gen Loss : 28.703872680664062, \t Total Dis Loss : 0.0002936144592240453\n",
      "Steps : 120500, \t Total Gen Loss : 31.925823211669922, \t Total Dis Loss : 0.0004462409997358918\n",
      "Steps : 120600, \t Total Gen Loss : 31.584854125976562, \t Total Dis Loss : 0.014940109103918076\n",
      "Steps : 120700, \t Total Gen Loss : 30.913583755493164, \t Total Dis Loss : 0.00020840315846726298\n",
      "Steps : 120800, \t Total Gen Loss : 33.029544830322266, \t Total Dis Loss : 8.53270375955617e-06\n",
      "Steps : 120900, \t Total Gen Loss : 29.731531143188477, \t Total Dis Loss : 0.0006382865831255913\n",
      "Steps : 121000, \t Total Gen Loss : 29.619585037231445, \t Total Dis Loss : 2.109464730892796e-05\n",
      "Steps : 121100, \t Total Gen Loss : 27.26202392578125, \t Total Dis Loss : 0.001611106563359499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 121200, \t Total Gen Loss : 27.75277328491211, \t Total Dis Loss : 0.03928148373961449\n",
      "Steps : 121300, \t Total Gen Loss : 32.20146942138672, \t Total Dis Loss : 0.0005575930117629468\n",
      "Steps : 121400, \t Total Gen Loss : 34.01935577392578, \t Total Dis Loss : 3.104167990386486e-05\n",
      "Steps : 121500, \t Total Gen Loss : 32.323394775390625, \t Total Dis Loss : 0.00015994359273463488\n",
      "Steps : 121600, \t Total Gen Loss : 29.695587158203125, \t Total Dis Loss : 0.0004865943919867277\n",
      "Steps : 121700, \t Total Gen Loss : 30.550689697265625, \t Total Dis Loss : 0.00014853064203634858\n",
      "Steps : 121800, \t Total Gen Loss : 33.447296142578125, \t Total Dis Loss : 0.0001268633350264281\n",
      "Steps : 121900, \t Total Gen Loss : 32.943443298339844, \t Total Dis Loss : 0.00015783261915203184\n",
      "Steps : 122000, \t Total Gen Loss : 30.753129959106445, \t Total Dis Loss : 0.006871331017464399\n",
      "Steps : 122100, \t Total Gen Loss : 28.55569076538086, \t Total Dis Loss : 0.0002452322223689407\n",
      "Steps : 122200, \t Total Gen Loss : 30.151899337768555, \t Total Dis Loss : 0.00019157640053890646\n",
      "Steps : 122300, \t Total Gen Loss : 33.389060974121094, \t Total Dis Loss : 0.00012843633885495365\n",
      "Steps : 122400, \t Total Gen Loss : 32.18812561035156, \t Total Dis Loss : 8.542420255253091e-05\n",
      "Steps : 122500, \t Total Gen Loss : 29.82480812072754, \t Total Dis Loss : 0.005185711197555065\n",
      "Steps : 122600, \t Total Gen Loss : 30.70132064819336, \t Total Dis Loss : 0.00020562976715154946\n",
      "Steps : 122700, \t Total Gen Loss : 31.750438690185547, \t Total Dis Loss : 2.7832702471641824e-05\n",
      "Steps : 122800, \t Total Gen Loss : 29.38677215576172, \t Total Dis Loss : 0.00028379468130879104\n",
      "Steps : 122900, \t Total Gen Loss : 32.34376525878906, \t Total Dis Loss : 0.00012253847671672702\n",
      "Steps : 123000, \t Total Gen Loss : 28.845115661621094, \t Total Dis Loss : 0.00037280944525264204\n",
      "Steps : 123100, \t Total Gen Loss : 31.38180160522461, \t Total Dis Loss : 5.308767140377313e-05\n",
      "Steps : 123200, \t Total Gen Loss : 30.89142608642578, \t Total Dis Loss : 0.00017789064440876245\n",
      "Steps : 123300, \t Total Gen Loss : 31.130123138427734, \t Total Dis Loss : 0.00038107146974653006\n",
      "Steps : 123400, \t Total Gen Loss : 27.77945899963379, \t Total Dis Loss : 8.831132436171174e-05\n",
      "Steps : 123500, \t Total Gen Loss : 32.13520812988281, \t Total Dis Loss : 0.0015204049414023757\n",
      "Steps : 123600, \t Total Gen Loss : 26.04379653930664, \t Total Dis Loss : 0.00016197195509448647\n",
      "Steps : 123700, \t Total Gen Loss : 30.643356323242188, \t Total Dis Loss : 0.00022656859073322266\n",
      "Time for epoch 22 is 77.7227873802185 sec\n",
      "Steps : 123800, \t Total Gen Loss : 31.123672485351562, \t Total Dis Loss : 0.0004669519839808345\n",
      "Steps : 123900, \t Total Gen Loss : 28.24083709716797, \t Total Dis Loss : 0.0001335127162747085\n",
      "Steps : 124000, \t Total Gen Loss : 27.520347595214844, \t Total Dis Loss : 0.0004941461957059801\n",
      "Steps : 124100, \t Total Gen Loss : 28.664613723754883, \t Total Dis Loss : 0.0031347188632935286\n",
      "Steps : 124200, \t Total Gen Loss : 32.212066650390625, \t Total Dis Loss : 0.00017725212092045695\n",
      "Steps : 124300, \t Total Gen Loss : 36.17938232421875, \t Total Dis Loss : 0.00014743457722943276\n",
      "Steps : 124400, \t Total Gen Loss : 31.2197265625, \t Total Dis Loss : 3.0388620871235617e-05\n",
      "Steps : 124500, \t Total Gen Loss : 28.28852081298828, \t Total Dis Loss : 0.0010657524690032005\n",
      "Steps : 124600, \t Total Gen Loss : 28.387939453125, \t Total Dis Loss : 0.00016576866619288921\n",
      "Steps : 124700, \t Total Gen Loss : 36.000633239746094, \t Total Dis Loss : 0.00012063018220942467\n",
      "Steps : 124800, \t Total Gen Loss : 33.38440704345703, \t Total Dis Loss : 0.0006670292350463569\n",
      "Steps : 124900, \t Total Gen Loss : 32.39278030395508, \t Total Dis Loss : 0.001944619813002646\n",
      "Steps : 125000, \t Total Gen Loss : 30.680694580078125, \t Total Dis Loss : 0.00011965509474975988\n",
      "Steps : 125100, \t Total Gen Loss : 30.657302856445312, \t Total Dis Loss : 0.0005151387304067612\n",
      "Steps : 125200, \t Total Gen Loss : 32.5426025390625, \t Total Dis Loss : 0.0620594397187233\n",
      "Steps : 125300, \t Total Gen Loss : 30.343368530273438, \t Total Dis Loss : 9.815330849960446e-05\n",
      "Steps : 125400, \t Total Gen Loss : 35.103118896484375, \t Total Dis Loss : 0.0002527839387767017\n",
      "Steps : 125500, \t Total Gen Loss : 34.527587890625, \t Total Dis Loss : 0.00012761770631186664\n",
      "Steps : 125600, \t Total Gen Loss : 31.754390716552734, \t Total Dis Loss : 0.0006214112509042025\n",
      "Steps : 125700, \t Total Gen Loss : 34.32398986816406, \t Total Dis Loss : 0.00014168763300403953\n",
      "Steps : 125800, \t Total Gen Loss : 31.88198471069336, \t Total Dis Loss : 6.707328429911286e-05\n",
      "Steps : 125900, \t Total Gen Loss : 28.874792098999023, \t Total Dis Loss : 0.00011390885629225522\n",
      "Steps : 126000, \t Total Gen Loss : 28.08322525024414, \t Total Dis Loss : 8.912196062738076e-05\n",
      "Steps : 126100, \t Total Gen Loss : 32.851318359375, \t Total Dis Loss : 0.00037653016624972224\n",
      "Steps : 126200, \t Total Gen Loss : 28.232242584228516, \t Total Dis Loss : 0.00018788408488035202\n",
      "Steps : 126300, \t Total Gen Loss : 26.873451232910156, \t Total Dis Loss : 0.001917804591357708\n",
      "Steps : 126400, \t Total Gen Loss : 29.92851448059082, \t Total Dis Loss : 3.463948814896867e-05\n",
      "Steps : 126500, \t Total Gen Loss : 36.23359680175781, \t Total Dis Loss : 0.0002701949269976467\n",
      "Steps : 126600, \t Total Gen Loss : 33.06976318359375, \t Total Dis Loss : 4.8654408601578325e-05\n",
      "Steps : 126700, \t Total Gen Loss : 33.035770416259766, \t Total Dis Loss : 0.00042118929559364915\n",
      "Steps : 126800, \t Total Gen Loss : 30.649160385131836, \t Total Dis Loss : 9.092730761040002e-05\n",
      "Steps : 126900, \t Total Gen Loss : 31.018205642700195, \t Total Dis Loss : 0.0014646914787590504\n",
      "Steps : 127000, \t Total Gen Loss : 40.052547454833984, \t Total Dis Loss : 2.8441205358831212e-05\n",
      "Steps : 127100, \t Total Gen Loss : 34.57581329345703, \t Total Dis Loss : 0.0011501680128276348\n",
      "Steps : 127200, \t Total Gen Loss : 30.7520694732666, \t Total Dis Loss : 0.0003034619730897248\n",
      "Steps : 127300, \t Total Gen Loss : 24.166465759277344, \t Total Dis Loss : 0.011225268244743347\n",
      "Steps : 127400, \t Total Gen Loss : 30.95577621459961, \t Total Dis Loss : 0.0007212565978989005\n",
      "Steps : 127500, \t Total Gen Loss : 32.82981872558594, \t Total Dis Loss : 0.0008961462299339473\n",
      "Steps : 127600, \t Total Gen Loss : 33.26736068725586, \t Total Dis Loss : 0.00028651271713897586\n",
      "Steps : 127700, \t Total Gen Loss : 35.870582580566406, \t Total Dis Loss : 0.00011323233047733083\n",
      "Steps : 127800, \t Total Gen Loss : 37.748470306396484, \t Total Dis Loss : 3.3760243240976706e-05\n",
      "Steps : 127900, \t Total Gen Loss : 34.808509826660156, \t Total Dis Loss : 6.0505029978230596e-05\n",
      "Steps : 128000, \t Total Gen Loss : 30.958648681640625, \t Total Dis Loss : 2.388542634434998e-05\n",
      "Steps : 128100, \t Total Gen Loss : 32.13054275512695, \t Total Dis Loss : 0.00010291170474374667\n",
      "Steps : 128200, \t Total Gen Loss : 28.76012420654297, \t Total Dis Loss : 4.601946420734748e-05\n",
      "Steps : 128300, \t Total Gen Loss : 28.660419464111328, \t Total Dis Loss : 3.3846608857857063e-05\n",
      "Steps : 128400, \t Total Gen Loss : 25.833845138549805, \t Total Dis Loss : 3.4470667742425576e-05\n",
      "Steps : 128500, \t Total Gen Loss : 32.25322341918945, \t Total Dis Loss : 0.00019943104416597635\n",
      "Steps : 128600, \t Total Gen Loss : 28.878307342529297, \t Total Dis Loss : 0.00010876783198909834\n",
      "Steps : 128700, \t Total Gen Loss : 31.69407081604004, \t Total Dis Loss : 9.328531450591981e-05\n",
      "Steps : 128800, \t Total Gen Loss : 29.516647338867188, \t Total Dis Loss : 0.00023406314721796662\n",
      "Steps : 128900, \t Total Gen Loss : 30.461406707763672, \t Total Dis Loss : 3.817238030023873e-05\n",
      "Steps : 129000, \t Total Gen Loss : 25.895275115966797, \t Total Dis Loss : 0.00010173889313591644\n",
      "Steps : 129100, \t Total Gen Loss : 28.023799896240234, \t Total Dis Loss : 0.00021148912492208183\n",
      "Steps : 129200, \t Total Gen Loss : 28.723064422607422, \t Total Dis Loss : 0.0001435217127436772\n",
      "Steps : 129300, \t Total Gen Loss : 29.0748291015625, \t Total Dis Loss : 0.0003190614515915513\n",
      "Time for epoch 23 is 78.87088251113892 sec\n",
      "Steps : 129400, \t Total Gen Loss : 29.08539581298828, \t Total Dis Loss : 7.091372390277684e-05\n",
      "Steps : 129500, \t Total Gen Loss : 32.30774688720703, \t Total Dis Loss : 3.530422327457927e-05\n",
      "Steps : 129600, \t Total Gen Loss : 31.650081634521484, \t Total Dis Loss : 0.00022908090613782406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 129700, \t Total Gen Loss : 29.52355194091797, \t Total Dis Loss : 0.0007136440835893154\n",
      "Steps : 129800, \t Total Gen Loss : 32.18408966064453, \t Total Dis Loss : 6.721117824781686e-05\n",
      "Steps : 129900, \t Total Gen Loss : 30.74532699584961, \t Total Dis Loss : 3.367270983289927e-05\n",
      "Steps : 130000, \t Total Gen Loss : 34.31655502319336, \t Total Dis Loss : 4.427533713169396e-05\n",
      "Steps : 130100, \t Total Gen Loss : 30.14887237548828, \t Total Dis Loss : 7.523340173065662e-05\n",
      "Steps : 130200, \t Total Gen Loss : 32.806671142578125, \t Total Dis Loss : 4.010702468804084e-05\n",
      "Steps : 130300, \t Total Gen Loss : 33.68292999267578, \t Total Dis Loss : 6.0144910094095394e-05\n",
      "Steps : 130400, \t Total Gen Loss : 31.64557456970215, \t Total Dis Loss : 6.705182750010863e-05\n",
      "Steps : 130500, \t Total Gen Loss : 28.66946029663086, \t Total Dis Loss : 0.0001512819726485759\n",
      "Steps : 130600, \t Total Gen Loss : 27.277103424072266, \t Total Dis Loss : 0.0054174247197806835\n",
      "Steps : 130700, \t Total Gen Loss : 32.33162307739258, \t Total Dis Loss : 0.00046336615923792124\n",
      "Steps : 130800, \t Total Gen Loss : 28.092870712280273, \t Total Dis Loss : 4.87128781969659e-05\n",
      "Steps : 130900, \t Total Gen Loss : 31.8428955078125, \t Total Dis Loss : 1.64795510499971e-05\n",
      "Steps : 131000, \t Total Gen Loss : 30.98370361328125, \t Total Dis Loss : 0.00011754980369005352\n",
      "Steps : 131100, \t Total Gen Loss : 29.395797729492188, \t Total Dis Loss : 8.240224269684404e-05\n",
      "Steps : 131200, \t Total Gen Loss : 28.618701934814453, \t Total Dis Loss : 5.559317651204765e-05\n",
      "Steps : 131300, \t Total Gen Loss : 35.11052703857422, \t Total Dis Loss : 0.0001995510538108647\n",
      "Steps : 131400, \t Total Gen Loss : 29.335521697998047, \t Total Dis Loss : 0.0001595754874870181\n",
      "Steps : 131500, \t Total Gen Loss : 31.614652633666992, \t Total Dis Loss : 0.0001118186628445983\n",
      "Steps : 131600, \t Total Gen Loss : 25.470495223999023, \t Total Dis Loss : 0.001567306462675333\n",
      "Steps : 131700, \t Total Gen Loss : 25.597591400146484, \t Total Dis Loss : 8.817701018415391e-05\n",
      "Steps : 131800, \t Total Gen Loss : 31.685317993164062, \t Total Dis Loss : 0.00013446247612591833\n",
      "Steps : 131900, \t Total Gen Loss : 24.91272735595703, \t Total Dis Loss : 0.00022034533321857452\n",
      "Steps : 132000, \t Total Gen Loss : 28.609743118286133, \t Total Dis Loss : 0.00025956385070458055\n",
      "Steps : 132100, \t Total Gen Loss : 28.592609405517578, \t Total Dis Loss : 8.680402970639989e-05\n",
      "Steps : 132200, \t Total Gen Loss : 25.634605407714844, \t Total Dis Loss : 0.0005596395931206644\n",
      "Steps : 132300, \t Total Gen Loss : 29.48050880432129, \t Total Dis Loss : 0.0001400684705004096\n",
      "Steps : 132400, \t Total Gen Loss : 25.00729751586914, \t Total Dis Loss : 0.0005381923983804882\n",
      "Steps : 132500, \t Total Gen Loss : 32.223480224609375, \t Total Dis Loss : 5.8211728173773736e-05\n",
      "Steps : 132600, \t Total Gen Loss : 34.11785888671875, \t Total Dis Loss : 2.2306659957394004e-05\n",
      "Steps : 132700, \t Total Gen Loss : 33.4873046875, \t Total Dis Loss : 5.890026386623504e-06\n",
      "Steps : 132800, \t Total Gen Loss : 31.247467041015625, \t Total Dis Loss : 5.7528086472302675e-05\n",
      "Steps : 132900, \t Total Gen Loss : 29.10866928100586, \t Total Dis Loss : 8.804335811873898e-05\n",
      "Steps : 133000, \t Total Gen Loss : 29.43552017211914, \t Total Dis Loss : 3.6453871871344745e-05\n",
      "Steps : 133100, \t Total Gen Loss : 27.61471939086914, \t Total Dis Loss : 7.309246575459838e-05\n",
      "Steps : 133200, \t Total Gen Loss : 28.767414093017578, \t Total Dis Loss : 4.496759356698021e-05\n",
      "Steps : 133300, \t Total Gen Loss : 34.81232452392578, \t Total Dis Loss : 0.0016112144803628325\n",
      "Steps : 133400, \t Total Gen Loss : 36.15870666503906, \t Total Dis Loss : 0.00071955710882321\n",
      "Steps : 133500, \t Total Gen Loss : 32.364620208740234, \t Total Dis Loss : 0.0005072135827504098\n",
      "Steps : 133600, \t Total Gen Loss : 27.396394729614258, \t Total Dis Loss : 0.020532691851258278\n",
      "Steps : 133700, \t Total Gen Loss : 33.726402282714844, \t Total Dis Loss : 0.000362387509085238\n",
      "Steps : 133800, \t Total Gen Loss : 32.656410217285156, \t Total Dis Loss : 0.002824455499649048\n",
      "Steps : 133900, \t Total Gen Loss : 31.957916259765625, \t Total Dis Loss : 0.0001252937945537269\n",
      "Steps : 134000, \t Total Gen Loss : 37.72980499267578, \t Total Dis Loss : 0.0010311653604730964\n",
      "Steps : 134100, \t Total Gen Loss : 32.32807922363281, \t Total Dis Loss : 9.694314940134063e-05\n",
      "Steps : 134200, \t Total Gen Loss : 30.32611083984375, \t Total Dis Loss : 0.0003364747972227633\n",
      "Steps : 134300, \t Total Gen Loss : 29.41309928894043, \t Total Dis Loss : 3.8210648199310526e-05\n",
      "Steps : 134400, \t Total Gen Loss : 30.64564323425293, \t Total Dis Loss : 2.8697839297819883e-05\n",
      "Steps : 134500, \t Total Gen Loss : 28.7296199798584, \t Total Dis Loss : 7.16203503543511e-05\n",
      "Steps : 134600, \t Total Gen Loss : 30.72673797607422, \t Total Dis Loss : 0.00023546327429357916\n",
      "Steps : 134700, \t Total Gen Loss : 26.320995330810547, \t Total Dis Loss : 9.028960630530491e-05\n",
      "Steps : 134800, \t Total Gen Loss : 31.05164909362793, \t Total Dis Loss : 0.00011287921370239928\n",
      "Steps : 134900, \t Total Gen Loss : 31.632673263549805, \t Total Dis Loss : 0.00013974597095511854\n",
      "Steps : 135000, \t Total Gen Loss : 35.66155242919922, \t Total Dis Loss : 0.00016556000628042966\n",
      "Time for epoch 24 is 78.2560408115387 sec\n",
      "Steps : 135100, \t Total Gen Loss : 33.568756103515625, \t Total Dis Loss : 9.046538252732717e-06\n",
      "Steps : 135200, \t Total Gen Loss : 29.018970489501953, \t Total Dis Loss : 6.471583037637174e-05\n",
      "Steps : 135300, \t Total Gen Loss : 27.335674285888672, \t Total Dis Loss : 6.373922224156559e-05\n",
      "Steps : 135400, \t Total Gen Loss : 30.102855682373047, \t Total Dis Loss : 1.9001638065674342e-05\n",
      "Steps : 135500, \t Total Gen Loss : 31.462087631225586, \t Total Dis Loss : 0.00023203645832836628\n",
      "Steps : 135600, \t Total Gen Loss : 30.156831741333008, \t Total Dis Loss : 2.9684437322430313e-05\n",
      "Steps : 135700, \t Total Gen Loss : 27.519954681396484, \t Total Dis Loss : 0.0002601623418740928\n",
      "Steps : 135800, \t Total Gen Loss : 28.05242156982422, \t Total Dis Loss : 0.00010831431427504867\n",
      "Steps : 135900, \t Total Gen Loss : 28.738788604736328, \t Total Dis Loss : 0.00016900728223845363\n",
      "Steps : 136000, \t Total Gen Loss : 30.310022354125977, \t Total Dis Loss : 4.304487447370775e-05\n",
      "Steps : 136100, \t Total Gen Loss : 30.0731258392334, \t Total Dis Loss : 2.9524646379286423e-05\n",
      "Steps : 136200, \t Total Gen Loss : 30.592689514160156, \t Total Dis Loss : 4.0307582821697e-05\n",
      "Steps : 136300, \t Total Gen Loss : 34.64385986328125, \t Total Dis Loss : 6.320950342342257e-05\n",
      "Steps : 136400, \t Total Gen Loss : 29.268165588378906, \t Total Dis Loss : 3.0052846341277473e-05\n",
      "Steps : 136500, \t Total Gen Loss : 31.407981872558594, \t Total Dis Loss : 2.2063912183512002e-05\n",
      "Steps : 136600, \t Total Gen Loss : 30.167287826538086, \t Total Dis Loss : 4.1703151509864256e-05\n",
      "Steps : 136700, \t Total Gen Loss : 31.9953556060791, \t Total Dis Loss : 3.3669715776341036e-05\n",
      "Steps : 136800, \t Total Gen Loss : 30.22451400756836, \t Total Dis Loss : 4.0159415220841765e-05\n",
      "Steps : 136900, \t Total Gen Loss : 26.654541015625, \t Total Dis Loss : 0.0016331267543137074\n",
      "Steps : 137000, \t Total Gen Loss : 25.705907821655273, \t Total Dis Loss : 0.0007733380189165473\n",
      "Steps : 137100, \t Total Gen Loss : 33.8841552734375, \t Total Dis Loss : 2.4363273041672073e-05\n",
      "Steps : 137200, \t Total Gen Loss : 32.808189392089844, \t Total Dis Loss : 0.00013208517339080572\n",
      "Steps : 137300, \t Total Gen Loss : 35.21448516845703, \t Total Dis Loss : 3.991459379903972e-05\n",
      "Steps : 137400, \t Total Gen Loss : 29.58156967163086, \t Total Dis Loss : 2.5526735043968074e-05\n",
      "Steps : 137500, \t Total Gen Loss : 29.990047454833984, \t Total Dis Loss : 3.0365335987880826e-05\n",
      "Steps : 137600, \t Total Gen Loss : 28.824295043945312, \t Total Dis Loss : 2.479193608451169e-05\n",
      "Steps : 137700, \t Total Gen Loss : 30.130252838134766, \t Total Dis Loss : 0.00013376071001403034\n",
      "Steps : 137800, \t Total Gen Loss : 34.54123306274414, \t Total Dis Loss : 0.00013518010382540524\n",
      "Steps : 137900, \t Total Gen Loss : 28.781843185424805, \t Total Dis Loss : 1.5096626157173887e-05\n",
      "Steps : 138000, \t Total Gen Loss : 30.673480987548828, \t Total Dis Loss : 1.230848556588171e-05\n",
      "Steps : 138100, \t Total Gen Loss : 28.793811798095703, \t Total Dis Loss : 0.0002455633657518774\n",
      "Steps : 138200, \t Total Gen Loss : 28.033687591552734, \t Total Dis Loss : 0.0005150224315002561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 138300, \t Total Gen Loss : 31.51563835144043, \t Total Dis Loss : 3.639006899902597e-05\n",
      "Steps : 138400, \t Total Gen Loss : 30.451236724853516, \t Total Dis Loss : 0.0001586871949257329\n",
      "Steps : 138500, \t Total Gen Loss : 31.15945053100586, \t Total Dis Loss : 0.0007723499438725412\n",
      "Steps : 138600, \t Total Gen Loss : 35.4501953125, \t Total Dis Loss : 6.385391316143796e-05\n",
      "Steps : 138700, \t Total Gen Loss : 41.971778869628906, \t Total Dis Loss : 0.0007629517931491137\n",
      "Steps : 138800, \t Total Gen Loss : 34.03407287597656, \t Total Dis Loss : 0.0001823774364311248\n",
      "Steps : 138900, \t Total Gen Loss : 27.78304672241211, \t Total Dis Loss : 7.456725143129006e-05\n",
      "Steps : 139000, \t Total Gen Loss : 27.01667022705078, \t Total Dis Loss : 0.0059233722276985645\n",
      "Steps : 139100, \t Total Gen Loss : 35.31381607055664, \t Total Dis Loss : 6.172155553940684e-05\n",
      "Steps : 139200, \t Total Gen Loss : 32.66620635986328, \t Total Dis Loss : 0.0004963437677361071\n",
      "Steps : 139300, \t Total Gen Loss : 32.035892486572266, \t Total Dis Loss : 6.40691869193688e-05\n",
      "Steps : 139400, \t Total Gen Loss : 30.65912437438965, \t Total Dis Loss : 0.0002615058037918061\n",
      "Steps : 139500, \t Total Gen Loss : 28.635725021362305, \t Total Dis Loss : 0.00021065378678031266\n",
      "Steps : 139600, \t Total Gen Loss : 28.775146484375, \t Total Dis Loss : 5.826281267218292e-05\n",
      "Steps : 139700, \t Total Gen Loss : 25.32955551147461, \t Total Dis Loss : 0.4677518606185913\n",
      "Steps : 139800, \t Total Gen Loss : 26.747562408447266, \t Total Dis Loss : 0.0002986585604958236\n",
      "Steps : 139900, \t Total Gen Loss : 30.381458282470703, \t Total Dis Loss : 6.121214391896501e-05\n",
      "Steps : 140000, \t Total Gen Loss : 29.361881256103516, \t Total Dis Loss : 6.518144073197618e-05\n",
      "Steps : 140100, \t Total Gen Loss : 26.202045440673828, \t Total Dis Loss : 5.843806138727814e-05\n",
      "Steps : 140200, \t Total Gen Loss : 27.975723266601562, \t Total Dis Loss : 6.627706170547754e-05\n",
      "Steps : 140300, \t Total Gen Loss : 27.574020385742188, \t Total Dis Loss : 5.133494778419845e-05\n",
      "Steps : 140400, \t Total Gen Loss : 27.22675323486328, \t Total Dis Loss : 4.1529601730871946e-05\n",
      "Steps : 140500, \t Total Gen Loss : 28.561538696289062, \t Total Dis Loss : 2.5078017642954364e-05\n",
      "Steps : 140600, \t Total Gen Loss : 25.221162796020508, \t Total Dis Loss : 4.396662552608177e-05\n",
      "Time for epoch 25 is 77.51449871063232 sec\n",
      "Steps : 140700, \t Total Gen Loss : 26.047910690307617, \t Total Dis Loss : 9.365699224872515e-05\n",
      "Steps : 140800, \t Total Gen Loss : 29.694368362426758, \t Total Dis Loss : 3.8991813198663294e-05\n",
      "Steps : 140900, \t Total Gen Loss : 28.42526626586914, \t Total Dis Loss : 4.060893115820363e-05\n",
      "Steps : 141000, \t Total Gen Loss : 26.265724182128906, \t Total Dis Loss : 1.9708140825969167e-05\n",
      "Steps : 141100, \t Total Gen Loss : 28.762542724609375, \t Total Dis Loss : 1.8195325537817553e-05\n",
      "Steps : 141200, \t Total Gen Loss : 26.329988479614258, \t Total Dis Loss : 0.00020560664415825158\n",
      "Steps : 141300, \t Total Gen Loss : 25.46110725402832, \t Total Dis Loss : 0.00013324599422048777\n",
      "Steps : 141400, \t Total Gen Loss : 26.580625534057617, \t Total Dis Loss : 0.00012011225771857426\n",
      "Steps : 141500, \t Total Gen Loss : 25.75176239013672, \t Total Dis Loss : 3.4603115636855364e-05\n",
      "Steps : 141600, \t Total Gen Loss : 26.06693458557129, \t Total Dis Loss : 1.2609166333277244e-05\n",
      "Steps : 141700, \t Total Gen Loss : 27.624916076660156, \t Total Dis Loss : 4.183231067145243e-05\n",
      "Steps : 141800, \t Total Gen Loss : 28.205280303955078, \t Total Dis Loss : 0.00014000055671203882\n",
      "Steps : 141900, \t Total Gen Loss : 29.29624366760254, \t Total Dis Loss : 5.467214577947743e-05\n",
      "Steps : 142000, \t Total Gen Loss : 28.15993309020996, \t Total Dis Loss : 9.945579949999228e-05\n",
      "Steps : 142100, \t Total Gen Loss : 29.90064239501953, \t Total Dis Loss : 2.192746069340501e-05\n",
      "Steps : 142200, \t Total Gen Loss : 28.800548553466797, \t Total Dis Loss : 3.667562486953102e-05\n",
      "Steps : 142300, \t Total Gen Loss : 26.31774139404297, \t Total Dis Loss : 0.00013063182996120304\n",
      "Steps : 142400, \t Total Gen Loss : 29.19997787475586, \t Total Dis Loss : 3.907323116436601e-05\n",
      "Steps : 142500, \t Total Gen Loss : 25.288349151611328, \t Total Dis Loss : 3.843234298983589e-05\n",
      "Steps : 142600, \t Total Gen Loss : 26.921756744384766, \t Total Dis Loss : 4.318900755606592e-05\n",
      "Steps : 142700, \t Total Gen Loss : 28.769458770751953, \t Total Dis Loss : 1.9858809537254274e-05\n",
      "Steps : 142800, \t Total Gen Loss : 27.20350456237793, \t Total Dis Loss : 0.000750810606405139\n",
      "Steps : 142900, \t Total Gen Loss : 27.29323959350586, \t Total Dis Loss : 3.573733920347877e-05\n",
      "Steps : 143000, \t Total Gen Loss : 27.140058517456055, \t Total Dis Loss : 0.00018229664419777691\n",
      "Steps : 143100, \t Total Gen Loss : 26.987939834594727, \t Total Dis Loss : 3.666343036456965e-05\n",
      "Steps : 143200, \t Total Gen Loss : 31.97614288330078, \t Total Dis Loss : 3.9060876588337123e-05\n",
      "Steps : 143300, \t Total Gen Loss : 29.181167602539062, \t Total Dis Loss : 2.082327046082355e-05\n",
      "Steps : 143400, \t Total Gen Loss : 27.18602180480957, \t Total Dis Loss : 1.10331357063842e-05\n",
      "Steps : 143500, \t Total Gen Loss : 29.046798706054688, \t Total Dis Loss : 3.489308073767461e-05\n",
      "Steps : 143600, \t Total Gen Loss : 28.61376190185547, \t Total Dis Loss : 4.503095260588452e-05\n",
      "Steps : 143700, \t Total Gen Loss : 27.280132293701172, \t Total Dis Loss : 1.0025150004366878e-05\n",
      "Steps : 143800, \t Total Gen Loss : 32.342323303222656, \t Total Dis Loss : 0.0017670835368335247\n",
      "Steps : 143900, \t Total Gen Loss : 29.32563018798828, \t Total Dis Loss : 0.0001285683101741597\n",
      "Steps : 144000, \t Total Gen Loss : 30.261058807373047, \t Total Dis Loss : 6.665050750598311e-05\n",
      "Steps : 144100, \t Total Gen Loss : 30.09746742248535, \t Total Dis Loss : 3.33850912284106e-05\n",
      "Steps : 144200, \t Total Gen Loss : 31.071449279785156, \t Total Dis Loss : 5.570903158513829e-05\n",
      "Steps : 144300, \t Total Gen Loss : 31.206192016601562, \t Total Dis Loss : 5.6337677960982546e-05\n",
      "Steps : 144400, \t Total Gen Loss : 28.655925750732422, \t Total Dis Loss : 2.429203777865041e-05\n",
      "Steps : 144500, \t Total Gen Loss : 31.391082763671875, \t Total Dis Loss : 0.00026940551470033824\n",
      "Steps : 144600, \t Total Gen Loss : 31.109329223632812, \t Total Dis Loss : 0.00017484162526670843\n",
      "Steps : 144700, \t Total Gen Loss : 28.976409912109375, \t Total Dis Loss : 0.001224373932927847\n",
      "Steps : 144800, \t Total Gen Loss : 31.491483688354492, \t Total Dis Loss : 3.406419273233041e-05\n",
      "Steps : 144900, \t Total Gen Loss : 28.056194305419922, \t Total Dis Loss : 5.3541702072834596e-05\n",
      "Steps : 145000, \t Total Gen Loss : 35.62065505981445, \t Total Dis Loss : 0.00025200843811035156\n",
      "Steps : 145100, \t Total Gen Loss : 31.752639770507812, \t Total Dis Loss : 4.0681516111362725e-05\n",
      "Steps : 145200, \t Total Gen Loss : 31.17243766784668, \t Total Dis Loss : 3.1930863769957796e-05\n",
      "Steps : 145300, \t Total Gen Loss : 24.589340209960938, \t Total Dis Loss : 5.30858087586239e-05\n",
      "Steps : 145400, \t Total Gen Loss : 27.754024505615234, \t Total Dis Loss : 4.446199818630703e-05\n",
      "Steps : 145500, \t Total Gen Loss : 26.593547821044922, \t Total Dis Loss : 3.5926968848798424e-05\n",
      "Steps : 145600, \t Total Gen Loss : 28.413211822509766, \t Total Dis Loss : 9.168183169094846e-05\n",
      "Steps : 145700, \t Total Gen Loss : 26.650100708007812, \t Total Dis Loss : 9.999921167036518e-05\n",
      "Steps : 145800, \t Total Gen Loss : 27.49840545654297, \t Total Dis Loss : 0.00018829968757927418\n",
      "Steps : 145900, \t Total Gen Loss : 26.543968200683594, \t Total Dis Loss : 0.00012783458805643022\n",
      "Steps : 146000, \t Total Gen Loss : 31.447675704956055, \t Total Dis Loss : 0.00010587926226435229\n",
      "Steps : 146100, \t Total Gen Loss : 28.761093139648438, \t Total Dis Loss : 9.268841677112505e-05\n",
      "Steps : 146200, \t Total Gen Loss : 26.151771545410156, \t Total Dis Loss : 0.00012075147242285311\n",
      "Time for epoch 26 is 73.95269107818604 sec\n",
      "Steps : 146300, \t Total Gen Loss : 29.46713638305664, \t Total Dis Loss : 0.00013163985568098724\n",
      "Steps : 146400, \t Total Gen Loss : 26.57442855834961, \t Total Dis Loss : 0.00011851316958200186\n",
      "Steps : 146500, \t Total Gen Loss : 27.827106475830078, \t Total Dis Loss : 0.0002979565761052072\n",
      "Steps : 146600, \t Total Gen Loss : 24.811840057373047, \t Total Dis Loss : 0.0001779011799953878\n",
      "Steps : 146700, \t Total Gen Loss : 29.679344177246094, \t Total Dis Loss : 4.748328501591459e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 146800, \t Total Gen Loss : 28.97926902770996, \t Total Dis Loss : 8.02168287918903e-05\n",
      "Steps : 146900, \t Total Gen Loss : 26.236896514892578, \t Total Dis Loss : 8.652210817672312e-05\n",
      "Steps : 147000, \t Total Gen Loss : 30.317237854003906, \t Total Dis Loss : 6.943523476365954e-05\n",
      "Steps : 147100, \t Total Gen Loss : 28.322742462158203, \t Total Dis Loss : 0.0001463009393773973\n",
      "Steps : 147200, \t Total Gen Loss : 28.169353485107422, \t Total Dis Loss : 0.00012916812556795776\n",
      "Steps : 147300, \t Total Gen Loss : 25.999523162841797, \t Total Dis Loss : 8.729847468202934e-05\n",
      "Steps : 147400, \t Total Gen Loss : 28.418907165527344, \t Total Dis Loss : 4.344258559285663e-05\n",
      "Steps : 147500, \t Total Gen Loss : 27.58492660522461, \t Total Dis Loss : 0.00017376757750753313\n",
      "Steps : 147600, \t Total Gen Loss : 28.28854751586914, \t Total Dis Loss : 9.374768706038594e-05\n",
      "Steps : 147700, \t Total Gen Loss : 28.48363494873047, \t Total Dis Loss : 6.734527414664626e-05\n",
      "Steps : 147800, \t Total Gen Loss : 25.58478546142578, \t Total Dis Loss : 4.3786665628431365e-05\n",
      "Steps : 147900, \t Total Gen Loss : 30.86100196838379, \t Total Dis Loss : 7.365772944467608e-06\n",
      "Steps : 148000, \t Total Gen Loss : 26.222936630249023, \t Total Dis Loss : 0.00014653801918029785\n",
      "Steps : 148100, \t Total Gen Loss : 29.555068969726562, \t Total Dis Loss : 1.6657926607877016e-05\n",
      "Steps : 148200, \t Total Gen Loss : 28.065916061401367, \t Total Dis Loss : 2.4526656488887966e-05\n",
      "Steps : 148300, \t Total Gen Loss : 27.340770721435547, \t Total Dis Loss : 3.8867910916451365e-05\n",
      "Steps : 148400, \t Total Gen Loss : 29.230815887451172, \t Total Dis Loss : 3.94945454900153e-05\n",
      "Steps : 148500, \t Total Gen Loss : 27.4917049407959, \t Total Dis Loss : 2.9051163437543437e-05\n",
      "Steps : 148600, \t Total Gen Loss : 28.827672958374023, \t Total Dis Loss : 1.419812542735599e-05\n",
      "Steps : 148700, \t Total Gen Loss : 28.342069625854492, \t Total Dis Loss : 2.1358748199418187e-05\n",
      "Steps : 148800, \t Total Gen Loss : 28.368854522705078, \t Total Dis Loss : 2.21278805838665e-05\n",
      "Steps : 148900, \t Total Gen Loss : 30.32895278930664, \t Total Dis Loss : 2.3463453544536605e-05\n",
      "Steps : 149000, \t Total Gen Loss : 28.048925399780273, \t Total Dis Loss : 2.0921750547131523e-05\n",
      "Steps : 149100, \t Total Gen Loss : 30.115005493164062, \t Total Dis Loss : 1.9833678379654884e-05\n",
      "Steps : 149200, \t Total Gen Loss : 29.296798706054688, \t Total Dis Loss : 1.6319487258442678e-05\n",
      "Steps : 149300, \t Total Gen Loss : 30.273454666137695, \t Total Dis Loss : 4.1001872887136415e-05\n",
      "Steps : 149400, \t Total Gen Loss : 28.951759338378906, \t Total Dis Loss : 1.4912813639966771e-05\n",
      "Steps : 149500, \t Total Gen Loss : 25.53775405883789, \t Total Dis Loss : 3.536209624144249e-05\n",
      "Steps : 149600, \t Total Gen Loss : 28.626283645629883, \t Total Dis Loss : 3.322363045299426e-05\n",
      "Steps : 149700, \t Total Gen Loss : 28.38950538635254, \t Total Dis Loss : 1.9118120690109208e-05\n",
      "Steps : 149800, \t Total Gen Loss : 30.520362854003906, \t Total Dis Loss : 1.5510111552430317e-05\n",
      "Steps : 149900, \t Total Gen Loss : 28.8229923248291, \t Total Dis Loss : 2.2838217773824e-05\n",
      "Steps : 150000, \t Total Gen Loss : 32.82018280029297, \t Total Dis Loss : 2.8736370950355195e-05\n",
      "Steps : 150100, \t Total Gen Loss : 32.25897979736328, \t Total Dis Loss : 0.0005619524163194001\n",
      "Steps : 150200, \t Total Gen Loss : 28.70047378540039, \t Total Dis Loss : 0.00010868800745811313\n",
      "Steps : 150300, \t Total Gen Loss : 30.073091506958008, \t Total Dis Loss : 7.13745248503983e-05\n",
      "Steps : 150400, \t Total Gen Loss : 34.71133041381836, \t Total Dis Loss : 0.0001387318770866841\n",
      "Steps : 150500, \t Total Gen Loss : 30.372821807861328, \t Total Dis Loss : 9.514421981293708e-05\n",
      "Steps : 150600, \t Total Gen Loss : 31.579668045043945, \t Total Dis Loss : 2.4956043489510193e-05\n",
      "Steps : 150700, \t Total Gen Loss : 31.08663558959961, \t Total Dis Loss : 3.5261226003058255e-05\n",
      "Steps : 150800, \t Total Gen Loss : 32.689937591552734, \t Total Dis Loss : 7.924041710793972e-05\n",
      "Steps : 150900, \t Total Gen Loss : 34.07401657104492, \t Total Dis Loss : 1.3703532204090152e-05\n",
      "Steps : 151000, \t Total Gen Loss : 35.2202262878418, \t Total Dis Loss : 9.116637374972925e-05\n",
      "Steps : 151100, \t Total Gen Loss : 24.861492156982422, \t Total Dis Loss : 0.0005927926977165043\n",
      "Steps : 151200, \t Total Gen Loss : 31.262863159179688, \t Total Dis Loss : 0.0003787624300457537\n",
      "Steps : 151300, \t Total Gen Loss : 25.75687026977539, \t Total Dis Loss : 0.0010061061475425959\n",
      "Steps : 151400, \t Total Gen Loss : 24.295856475830078, \t Total Dis Loss : 0.00014330630074255168\n",
      "Steps : 151500, \t Total Gen Loss : 27.246315002441406, \t Total Dis Loss : 0.0004599177045747638\n",
      "Steps : 151600, \t Total Gen Loss : 23.311182022094727, \t Total Dis Loss : 0.0014144526794552803\n",
      "Steps : 151700, \t Total Gen Loss : 23.166202545166016, \t Total Dis Loss : 0.0006559424218721688\n",
      "Steps : 151800, \t Total Gen Loss : 29.71758460998535, \t Total Dis Loss : 0.00010039186599897221\n",
      "Time for epoch 27 is 73.91569566726685 sec\n",
      "Steps : 151900, \t Total Gen Loss : 33.34575271606445, \t Total Dis Loss : 0.00027190378750674427\n",
      "Steps : 152000, \t Total Gen Loss : 31.501367568969727, \t Total Dis Loss : 7.118445500964299e-05\n",
      "Steps : 152100, \t Total Gen Loss : 25.003238677978516, \t Total Dis Loss : 0.00012230713036842644\n",
      "Steps : 152200, \t Total Gen Loss : 26.730464935302734, \t Total Dis Loss : 6.860107532702386e-05\n",
      "Steps : 152300, \t Total Gen Loss : 25.676095962524414, \t Total Dis Loss : 0.0002614593249745667\n",
      "Steps : 152400, \t Total Gen Loss : 26.330549240112305, \t Total Dis Loss : 6.98257063049823e-05\n",
      "Steps : 152500, \t Total Gen Loss : 30.309402465820312, \t Total Dis Loss : 7.358295260928571e-05\n",
      "Steps : 152600, \t Total Gen Loss : 28.670215606689453, \t Total Dis Loss : 7.440873741870746e-05\n",
      "Steps : 152700, \t Total Gen Loss : 28.848594665527344, \t Total Dis Loss : 9.589637193130329e-05\n",
      "Steps : 152800, \t Total Gen Loss : 26.601163864135742, \t Total Dis Loss : 0.0002646267239470035\n",
      "Steps : 152900, \t Total Gen Loss : 22.229297637939453, \t Total Dis Loss : 0.0009609992266632617\n",
      "Steps : 153000, \t Total Gen Loss : 24.849224090576172, \t Total Dis Loss : 0.0002340880164410919\n",
      "Steps : 153100, \t Total Gen Loss : 25.093393325805664, \t Total Dis Loss : 0.00013137383211869746\n",
      "Steps : 153200, \t Total Gen Loss : 28.29607391357422, \t Total Dis Loss : 6.848681368865073e-05\n",
      "Steps : 153300, \t Total Gen Loss : 26.433692932128906, \t Total Dis Loss : 0.00010275424574501812\n",
      "Steps : 153400, \t Total Gen Loss : 29.782928466796875, \t Total Dis Loss : 3.0925413739169016e-05\n",
      "Steps : 153500, \t Total Gen Loss : 26.73440170288086, \t Total Dis Loss : 9.071754902834073e-05\n",
      "Steps : 153600, \t Total Gen Loss : 23.853073120117188, \t Total Dis Loss : 0.00015532811812590808\n",
      "Steps : 153700, \t Total Gen Loss : 29.658939361572266, \t Total Dis Loss : 7.005418592598289e-05\n",
      "Steps : 153800, \t Total Gen Loss : 30.867746353149414, \t Total Dis Loss : 0.0001902573276311159\n",
      "Steps : 153900, \t Total Gen Loss : 29.56494140625, \t Total Dis Loss : 0.00036403295234777033\n",
      "Steps : 154000, \t Total Gen Loss : 29.14059829711914, \t Total Dis Loss : 0.00019649203750304878\n",
      "Steps : 154100, \t Total Gen Loss : 28.417190551757812, \t Total Dis Loss : 0.0002409821463515982\n",
      "Steps : 154200, \t Total Gen Loss : 27.48031234741211, \t Total Dis Loss : 8.623843314126134e-05\n",
      "Steps : 154300, \t Total Gen Loss : 25.4827880859375, \t Total Dis Loss : 0.00021581960027106106\n",
      "Steps : 154400, \t Total Gen Loss : 24.19912338256836, \t Total Dis Loss : 4.12117296946235e-05\n",
      "Steps : 154500, \t Total Gen Loss : 27.350046157836914, \t Total Dis Loss : 8.030730532482266e-05\n",
      "Steps : 154600, \t Total Gen Loss : 29.21622085571289, \t Total Dis Loss : 5.559603596339002e-05\n",
      "Steps : 154700, \t Total Gen Loss : 26.426860809326172, \t Total Dis Loss : 2.7469035558169708e-05\n",
      "Steps : 154800, \t Total Gen Loss : 25.54393768310547, \t Total Dis Loss : 3.9005470171105117e-05\n",
      "Steps : 154900, \t Total Gen Loss : 29.995094299316406, \t Total Dis Loss : 6.131080590421334e-05\n",
      "Steps : 155000, \t Total Gen Loss : 28.941944122314453, \t Total Dis Loss : 5.177413549972698e-05\n",
      "Steps : 155100, \t Total Gen Loss : 27.415990829467773, \t Total Dis Loss : 6.208307604538277e-05\n",
      "Steps : 155200, \t Total Gen Loss : 28.199371337890625, \t Total Dis Loss : 5.1804927352350205e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 155300, \t Total Gen Loss : 27.708280563354492, \t Total Dis Loss : 7.173552876338363e-05\n",
      "Steps : 155400, \t Total Gen Loss : 27.333866119384766, \t Total Dis Loss : 3.0610299290856346e-05\n",
      "Steps : 155500, \t Total Gen Loss : 30.036640167236328, \t Total Dis Loss : 2.846400639100466e-05\n",
      "Steps : 155600, \t Total Gen Loss : 27.730457305908203, \t Total Dis Loss : 0.00016169450827874243\n",
      "Steps : 155700, \t Total Gen Loss : 26.776124954223633, \t Total Dis Loss : 0.00010349266813136637\n",
      "Steps : 155800, \t Total Gen Loss : 27.08024024963379, \t Total Dis Loss : 7.79017063905485e-05\n",
      "Steps : 155900, \t Total Gen Loss : 31.82501983642578, \t Total Dis Loss : 7.1919598667591345e-06\n",
      "Steps : 156000, \t Total Gen Loss : 29.0744571685791, \t Total Dis Loss : 3.561652920325287e-05\n",
      "Steps : 156100, \t Total Gen Loss : 25.38955307006836, \t Total Dis Loss : 7.19714371371083e-05\n",
      "Steps : 156200, \t Total Gen Loss : 26.76422119140625, \t Total Dis Loss : 5.2358813263708726e-05\n",
      "Steps : 156300, \t Total Gen Loss : 26.107593536376953, \t Total Dis Loss : 3.625554381869733e-05\n",
      "Steps : 156400, \t Total Gen Loss : 25.207441329956055, \t Total Dis Loss : 3.393127553863451e-05\n",
      "Steps : 156500, \t Total Gen Loss : 29.804534912109375, \t Total Dis Loss : 0.00018950362573377788\n",
      "Steps : 156600, \t Total Gen Loss : 27.334625244140625, \t Total Dis Loss : 6.906303315190598e-05\n",
      "Steps : 156700, \t Total Gen Loss : 26.054399490356445, \t Total Dis Loss : 5.248949310043827e-05\n",
      "Steps : 156800, \t Total Gen Loss : 25.736080169677734, \t Total Dis Loss : 0.0007316761766560376\n",
      "Steps : 156900, \t Total Gen Loss : 26.308021545410156, \t Total Dis Loss : 0.00011424812691984698\n",
      "Steps : 157000, \t Total Gen Loss : 26.255502700805664, \t Total Dis Loss : 0.0004215790540911257\n",
      "Steps : 157100, \t Total Gen Loss : 23.97097396850586, \t Total Dis Loss : 0.00010243058204650879\n",
      "Steps : 157200, \t Total Gen Loss : 26.433368682861328, \t Total Dis Loss : 3.95730348827783e-05\n",
      "Steps : 157300, \t Total Gen Loss : 23.090173721313477, \t Total Dis Loss : 0.004174191039055586\n",
      "Steps : 157400, \t Total Gen Loss : 26.182125091552734, \t Total Dis Loss : 6.824474985478446e-05\n",
      "Steps : 157500, \t Total Gen Loss : 29.505542755126953, \t Total Dis Loss : 6.026407208992168e-05\n",
      "Time for epoch 28 is 74.34328770637512 sec\n",
      "Steps : 157600, \t Total Gen Loss : 25.932575225830078, \t Total Dis Loss : 4.617897138814442e-05\n",
      "Steps : 157700, \t Total Gen Loss : 28.789356231689453, \t Total Dis Loss : 0.00014821103832218796\n",
      "Steps : 157800, \t Total Gen Loss : 24.996070861816406, \t Total Dis Loss : 4.540481313597411e-05\n",
      "Steps : 157900, \t Total Gen Loss : 25.87396240234375, \t Total Dis Loss : 9.773871715879068e-05\n",
      "Steps : 158000, \t Total Gen Loss : 24.63711929321289, \t Total Dis Loss : 0.00026042296667583287\n",
      "Steps : 158100, \t Total Gen Loss : 27.859161376953125, \t Total Dis Loss : 4.510708822635934e-05\n",
      "Steps : 158200, \t Total Gen Loss : 26.21097183227539, \t Total Dis Loss : 3.83031256205868e-05\n",
      "Steps : 158300, \t Total Gen Loss : 26.082988739013672, \t Total Dis Loss : 3.0155990316416137e-05\n",
      "Steps : 158400, \t Total Gen Loss : 31.153194427490234, \t Total Dis Loss : 3.854600072372705e-05\n",
      "Steps : 158500, \t Total Gen Loss : 30.427942276000977, \t Total Dis Loss : 1.3975476576888468e-05\n",
      "Steps : 158600, \t Total Gen Loss : 28.231569290161133, \t Total Dis Loss : 2.62914109043777e-05\n",
      "Steps : 158700, \t Total Gen Loss : 30.984630584716797, \t Total Dis Loss : 0.0001721288717817515\n",
      "Steps : 158800, \t Total Gen Loss : 27.071636199951172, \t Total Dis Loss : 6.55611656839028e-05\n",
      "Steps : 158900, \t Total Gen Loss : 28.039588928222656, \t Total Dis Loss : 6.453371315728873e-05\n",
      "Steps : 159000, \t Total Gen Loss : 25.553314208984375, \t Total Dis Loss : 0.00022761055151931942\n",
      "Steps : 159100, \t Total Gen Loss : 24.66226577758789, \t Total Dis Loss : 0.00013117787602823228\n",
      "Steps : 159200, \t Total Gen Loss : 26.809829711914062, \t Total Dis Loss : 0.000100003628176637\n",
      "Steps : 159300, \t Total Gen Loss : 26.07654571533203, \t Total Dis Loss : 2.7701244107447565e-05\n",
      "Steps : 159400, \t Total Gen Loss : 23.980785369873047, \t Total Dis Loss : 0.00031207295251078904\n",
      "Steps : 159500, \t Total Gen Loss : 27.314395904541016, \t Total Dis Loss : 4.4619489926844835e-05\n",
      "Steps : 159600, \t Total Gen Loss : 26.949626922607422, \t Total Dis Loss : 2.1629881302942522e-05\n",
      "Steps : 159700, \t Total Gen Loss : 29.45760154724121, \t Total Dis Loss : 1.243865153810475e-05\n",
      "Steps : 159800, \t Total Gen Loss : 26.38202667236328, \t Total Dis Loss : 1.1205913324374706e-05\n",
      "Steps : 159900, \t Total Gen Loss : 27.954605102539062, \t Total Dis Loss : 7.361327789112693e-06\n",
      "Steps : 160000, \t Total Gen Loss : 32.02954864501953, \t Total Dis Loss : 1.4001293493492994e-05\n",
      "Steps : 160100, \t Total Gen Loss : 29.89995765686035, \t Total Dis Loss : 1.817502561607398e-05\n",
      "Steps : 160200, \t Total Gen Loss : 30.062570571899414, \t Total Dis Loss : 6.482422122644493e-06\n",
      "Steps : 160300, \t Total Gen Loss : 26.588626861572266, \t Total Dis Loss : 7.595233910251409e-05\n",
      "Steps : 160400, \t Total Gen Loss : 24.988502502441406, \t Total Dis Loss : 1.5968174920999445e-05\n",
      "Steps : 160500, \t Total Gen Loss : 29.671730041503906, \t Total Dis Loss : 4.8898033128352836e-05\n",
      "Steps : 160600, \t Total Gen Loss : 28.705585479736328, \t Total Dis Loss : 2.263873648189474e-05\n",
      "Steps : 160700, \t Total Gen Loss : 31.709333419799805, \t Total Dis Loss : 3.098943125223741e-05\n",
      "Steps : 160800, \t Total Gen Loss : 26.820037841796875, \t Total Dis Loss : 1.923325180541724e-05\n",
      "Steps : 160900, \t Total Gen Loss : 28.392549514770508, \t Total Dis Loss : 7.130688118195394e-06\n",
      "Steps : 161000, \t Total Gen Loss : 27.10763931274414, \t Total Dis Loss : 1.1875354175572284e-05\n",
      "Steps : 161100, \t Total Gen Loss : 27.915817260742188, \t Total Dis Loss : 5.62773766432656e-06\n",
      "Steps : 161200, \t Total Gen Loss : 29.775785446166992, \t Total Dis Loss : 5.503432021214394e-06\n",
      "Steps : 161300, \t Total Gen Loss : 29.807933807373047, \t Total Dis Loss : 5.141843303135829e-06\n",
      "Steps : 161400, \t Total Gen Loss : 32.187416076660156, \t Total Dis Loss : 4.806064680451527e-06\n",
      "Steps : 161500, \t Total Gen Loss : 31.22732162475586, \t Total Dis Loss : 4.468286533665378e-06\n",
      "Steps : 161600, \t Total Gen Loss : 29.141273498535156, \t Total Dis Loss : 7.690410711802542e-06\n",
      "Steps : 161700, \t Total Gen Loss : 31.942428588867188, \t Total Dis Loss : 7.378372629318619e-06\n",
      "Steps : 161800, \t Total Gen Loss : 26.64742660522461, \t Total Dis Loss : 3.19058817694895e-05\n",
      "Steps : 161900, \t Total Gen Loss : 27.075349807739258, \t Total Dis Loss : 1.4841078154859133e-05\n",
      "Steps : 162000, \t Total Gen Loss : 27.660987854003906, \t Total Dis Loss : 0.00024011422647163272\n",
      "Steps : 162100, \t Total Gen Loss : 26.280710220336914, \t Total Dis Loss : 2.0595309251802973e-05\n",
      "Steps : 162200, \t Total Gen Loss : 24.44255256652832, \t Total Dis Loss : 0.00023977966338861734\n",
      "Steps : 162300, \t Total Gen Loss : 31.86075782775879, \t Total Dis Loss : 7.702297807554714e-06\n",
      "Steps : 162400, \t Total Gen Loss : 28.223487854003906, \t Total Dis Loss : 0.00012460692960303277\n",
      "Steps : 162500, \t Total Gen Loss : 30.40880012512207, \t Total Dis Loss : 3.033942675756407e-06\n",
      "Steps : 162600, \t Total Gen Loss : 32.939476013183594, \t Total Dis Loss : 0.00010028312681242824\n",
      "Steps : 162700, \t Total Gen Loss : 29.241714477539062, \t Total Dis Loss : 0.00013090274296700954\n",
      "Steps : 162800, \t Total Gen Loss : 31.57982635498047, \t Total Dis Loss : 2.3995346055016853e-05\n",
      "Steps : 162900, \t Total Gen Loss : 28.509811401367188, \t Total Dis Loss : 5.897920345887542e-05\n",
      "Steps : 163000, \t Total Gen Loss : 27.46990966796875, \t Total Dis Loss : 0.00019344186875969172\n",
      "Steps : 163100, \t Total Gen Loss : 27.725160598754883, \t Total Dis Loss : 0.00010195727372774854\n",
      "Time for epoch 29 is 77.4232087135315 sec\n",
      "Steps : 163200, \t Total Gen Loss : 24.322471618652344, \t Total Dis Loss : 0.0002777718473225832\n",
      "Steps : 163300, \t Total Gen Loss : 26.5447998046875, \t Total Dis Loss : 0.00029860969516448677\n",
      "Steps : 163400, \t Total Gen Loss : 28.301475524902344, \t Total Dis Loss : 4.176282891421579e-05\n",
      "Steps : 163500, \t Total Gen Loss : 39.68894958496094, \t Total Dis Loss : 1.238761433342006e-05\n",
      "Steps : 163600, \t Total Gen Loss : 27.213455200195312, \t Total Dis Loss : 9.473471436649561e-05\n",
      "Steps : 163700, \t Total Gen Loss : 28.10855484008789, \t Total Dis Loss : 0.00032851489959284663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 163800, \t Total Gen Loss : 33.5372314453125, \t Total Dis Loss : 0.00016347634664271027\n",
      "Steps : 163900, \t Total Gen Loss : 34.412925720214844, \t Total Dis Loss : 1.7725647921906784e-05\n",
      "Steps : 164000, \t Total Gen Loss : 34.251380920410156, \t Total Dis Loss : 4.9591908464208245e-06\n",
      "Steps : 164100, \t Total Gen Loss : 31.917438507080078, \t Total Dis Loss : 7.366794307017699e-05\n",
      "Steps : 164200, \t Total Gen Loss : 30.439788818359375, \t Total Dis Loss : 0.00011065163562307134\n",
      "Steps : 164300, \t Total Gen Loss : 34.15964126586914, \t Total Dis Loss : 0.0005144287133589387\n",
      "Steps : 164400, \t Total Gen Loss : 34.958160400390625, \t Total Dis Loss : 0.000276709150057286\n",
      "Steps : 164500, \t Total Gen Loss : 30.957027435302734, \t Total Dis Loss : 0.00035271281376481056\n",
      "Steps : 164600, \t Total Gen Loss : 28.422386169433594, \t Total Dis Loss : 0.0002463949203956872\n",
      "Steps : 164700, \t Total Gen Loss : 32.260711669921875, \t Total Dis Loss : 0.0003612938744481653\n",
      "Steps : 164800, \t Total Gen Loss : 31.166717529296875, \t Total Dis Loss : 8.09736957307905e-05\n",
      "Steps : 164900, \t Total Gen Loss : 36.296302795410156, \t Total Dis Loss : 4.189480750937946e-05\n",
      "Steps : 165000, \t Total Gen Loss : 32.99562072753906, \t Total Dis Loss : 3.443797322688624e-05\n",
      "Steps : 165100, \t Total Gen Loss : 31.311248779296875, \t Total Dis Loss : 7.511188596254215e-05\n",
      "Steps : 165200, \t Total Gen Loss : 30.955081939697266, \t Total Dis Loss : 3.94013368349988e-05\n",
      "Steps : 165300, \t Total Gen Loss : 32.28479766845703, \t Total Dis Loss : 3.9052341890055686e-05\n",
      "Steps : 165400, \t Total Gen Loss : 30.403820037841797, \t Total Dis Loss : 3.6257231840863824e-05\n",
      "Steps : 165500, \t Total Gen Loss : 36.16511535644531, \t Total Dis Loss : 2.0746938389493152e-05\n",
      "Steps : 165600, \t Total Gen Loss : 30.785642623901367, \t Total Dis Loss : 1.3601656974060461e-05\n",
      "Steps : 165700, \t Total Gen Loss : 31.96048355102539, \t Total Dis Loss : 5.5794036597944796e-05\n",
      "Steps : 165800, \t Total Gen Loss : 31.87215805053711, \t Total Dis Loss : 0.4665650427341461\n",
      "Steps : 165900, \t Total Gen Loss : 28.738292694091797, \t Total Dis Loss : 0.02916349098086357\n",
      "Steps : 166000, \t Total Gen Loss : 33.99103546142578, \t Total Dis Loss : 0.0001720626896712929\n",
      "Steps : 166100, \t Total Gen Loss : 32.515113830566406, \t Total Dis Loss : 0.0004937208141200244\n",
      "Steps : 166200, \t Total Gen Loss : 32.6755485534668, \t Total Dis Loss : 0.00020144329755567014\n",
      "Steps : 166300, \t Total Gen Loss : 26.416996002197266, \t Total Dis Loss : 0.0005113876541145146\n",
      "Steps : 166400, \t Total Gen Loss : 32.7972297668457, \t Total Dis Loss : 0.00018121757602784783\n",
      "Steps : 166500, \t Total Gen Loss : 30.622238159179688, \t Total Dis Loss : 4.05414066335652e-05\n",
      "Steps : 166600, \t Total Gen Loss : 31.25531005859375, \t Total Dis Loss : 0.000409220956498757\n",
      "Steps : 166700, \t Total Gen Loss : 33.906890869140625, \t Total Dis Loss : 4.674233787227422e-05\n",
      "Steps : 166800, \t Total Gen Loss : 31.91468048095703, \t Total Dis Loss : 9.926327038556337e-06\n",
      "Steps : 166900, \t Total Gen Loss : 31.632247924804688, \t Total Dis Loss : 3.853623638860881e-05\n",
      "Steps : 167000, \t Total Gen Loss : 32.8931884765625, \t Total Dis Loss : 9.409723861608654e-05\n",
      "Steps : 167100, \t Total Gen Loss : 34.33551025390625, \t Total Dis Loss : 0.00011855617049150169\n",
      "Steps : 167200, \t Total Gen Loss : 34.449337005615234, \t Total Dis Loss : 0.00011050173634430394\n",
      "Steps : 167300, \t Total Gen Loss : 34.20706558227539, \t Total Dis Loss : 8.072578202700242e-05\n",
      "Steps : 167400, \t Total Gen Loss : 37.65350341796875, \t Total Dis Loss : 0.00010662907152436674\n",
      "Steps : 167500, \t Total Gen Loss : 33.15986251831055, \t Total Dis Loss : 0.00012598917237482965\n",
      "Steps : 167600, \t Total Gen Loss : 34.99878692626953, \t Total Dis Loss : 4.105089828954078e-05\n",
      "Steps : 167700, \t Total Gen Loss : 37.318077087402344, \t Total Dis Loss : 0.012768913991749287\n",
      "Steps : 167800, \t Total Gen Loss : 39.83990478515625, \t Total Dis Loss : 0.00097115826793015\n",
      "Steps : 167900, \t Total Gen Loss : 36.1175651550293, \t Total Dis Loss : 0.00023004694958217442\n",
      "Steps : 168000, \t Total Gen Loss : 38.84620666503906, \t Total Dis Loss : 0.00010066804679809138\n",
      "Steps : 168100, \t Total Gen Loss : 35.31111145019531, \t Total Dis Loss : 0.0002074001240544021\n",
      "Steps : 168200, \t Total Gen Loss : 36.66495132446289, \t Total Dis Loss : 0.0004989413428120315\n",
      "Steps : 168300, \t Total Gen Loss : 30.029163360595703, \t Total Dis Loss : 0.0009222872322425246\n",
      "Steps : 168400, \t Total Gen Loss : 32.11605453491211, \t Total Dis Loss : 8.902874105842784e-05\n",
      "Steps : 168500, \t Total Gen Loss : 31.458091735839844, \t Total Dis Loss : 0.00014502365957014263\n",
      "Steps : 168600, \t Total Gen Loss : 35.76751708984375, \t Total Dis Loss : 3.427566116442904e-05\n",
      "Steps : 168700, \t Total Gen Loss : 43.165489196777344, \t Total Dis Loss : 0.0002776150358840823\n",
      "Time for epoch 30 is 77.56779623031616 sec\n",
      "Steps : 168800, \t Total Gen Loss : 33.764610290527344, \t Total Dis Loss : 0.00014266706421039999\n",
      "Steps : 168900, \t Total Gen Loss : 33.737876892089844, \t Total Dis Loss : 6.109647802077234e-05\n",
      "Steps : 169000, \t Total Gen Loss : 33.00083923339844, \t Total Dis Loss : 1.16484161480912e-05\n",
      "Steps : 169100, \t Total Gen Loss : 35.81501770019531, \t Total Dis Loss : 3.848410415230319e-05\n",
      "Steps : 169200, \t Total Gen Loss : 33.09624481201172, \t Total Dis Loss : 8.128755143843591e-05\n",
      "Steps : 169300, \t Total Gen Loss : 32.070125579833984, \t Total Dis Loss : 2.199968002969399e-05\n",
      "Steps : 169400, \t Total Gen Loss : 34.180137634277344, \t Total Dis Loss : 4.114474904781673e-06\n",
      "Steps : 169500, \t Total Gen Loss : 33.10624694824219, \t Total Dis Loss : 0.00016130316362250596\n",
      "Steps : 169600, \t Total Gen Loss : 31.697269439697266, \t Total Dis Loss : 0.00011576271208468825\n",
      "Steps : 169700, \t Total Gen Loss : 30.404560089111328, \t Total Dis Loss : 0.00015388407337013632\n",
      "Steps : 169800, \t Total Gen Loss : 31.28807830810547, \t Total Dis Loss : 1.836498086049687e-05\n",
      "Steps : 169900, \t Total Gen Loss : 28.707977294921875, \t Total Dis Loss : 0.00019315205281600356\n",
      "Steps : 170000, \t Total Gen Loss : 28.66852569580078, \t Total Dis Loss : 0.0011643655598163605\n",
      "Steps : 170100, \t Total Gen Loss : 31.442970275878906, \t Total Dis Loss : 1.4061486581340432e-05\n",
      "Steps : 170200, \t Total Gen Loss : 28.767091751098633, \t Total Dis Loss : 0.00014251016546040773\n",
      "Steps : 170300, \t Total Gen Loss : 29.647598266601562, \t Total Dis Loss : 1.300838812312577e-05\n",
      "Steps : 170400, \t Total Gen Loss : 32.80430603027344, \t Total Dis Loss : 1.4769128029001877e-05\n",
      "Steps : 170500, \t Total Gen Loss : 28.463228225708008, \t Total Dis Loss : 2.0080558897461742e-05\n",
      "Steps : 170600, \t Total Gen Loss : 26.580413818359375, \t Total Dis Loss : 9.001666330732405e-05\n",
      "Steps : 170700, \t Total Gen Loss : 26.458065032958984, \t Total Dis Loss : 5.3575164201902226e-05\n",
      "Steps : 170800, \t Total Gen Loss : 31.162254333496094, \t Total Dis Loss : 4.035343954456039e-05\n",
      "Steps : 170900, \t Total Gen Loss : 34.731597900390625, \t Total Dis Loss : 2.7503712772158906e-05\n",
      "Steps : 171000, \t Total Gen Loss : 31.001049041748047, \t Total Dis Loss : 4.2139155993936583e-05\n",
      "Steps : 171100, \t Total Gen Loss : 27.309345245361328, \t Total Dis Loss : 0.0001930668659042567\n",
      "Steps : 171200, \t Total Gen Loss : 27.333026885986328, \t Total Dis Loss : 0.0001278248819289729\n",
      "Steps : 171300, \t Total Gen Loss : 26.726444244384766, \t Total Dis Loss : 8.359488128917292e-05\n",
      "Steps : 171400, \t Total Gen Loss : 29.004283905029297, \t Total Dis Loss : 2.31990561587736e-05\n",
      "Steps : 171500, \t Total Gen Loss : 27.31473159790039, \t Total Dis Loss : 5.763260560343042e-05\n",
      "Steps : 171600, \t Total Gen Loss : 31.254905700683594, \t Total Dis Loss : 4.820664798899088e-06\n",
      "Steps : 171700, \t Total Gen Loss : 28.7872257232666, \t Total Dis Loss : 0.00013754841347690672\n",
      "Steps : 171800, \t Total Gen Loss : 26.204944610595703, \t Total Dis Loss : 2.4288916392833926e-05\n",
      "Steps : 171900, \t Total Gen Loss : 28.86162567138672, \t Total Dis Loss : 2.7000582122127526e-05\n",
      "Steps : 172000, \t Total Gen Loss : 25.226055145263672, \t Total Dis Loss : 1.677803993516136e-05\n",
      "Steps : 172100, \t Total Gen Loss : 31.46933364868164, \t Total Dis Loss : 4.585872375173494e-06\n",
      "Steps : 172200, \t Total Gen Loss : 24.83493423461914, \t Total Dis Loss : 5.15290885232389e-05\n",
      "Steps : 172300, \t Total Gen Loss : 26.826793670654297, \t Total Dis Loss : 4.6140274207573384e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 172400, \t Total Gen Loss : 30.83892822265625, \t Total Dis Loss : 6.689438305329531e-05\n",
      "Steps : 172500, \t Total Gen Loss : 26.347583770751953, \t Total Dis Loss : 6.319405656540766e-05\n",
      "Steps : 172600, \t Total Gen Loss : 29.117422103881836, \t Total Dis Loss : 1.5162342606345192e-05\n",
      "Steps : 172700, \t Total Gen Loss : 24.514915466308594, \t Total Dis Loss : 3.8252906961133704e-05\n",
      "Steps : 172800, \t Total Gen Loss : 29.49356460571289, \t Total Dis Loss : 1.1173030543432105e-05\n",
      "Steps : 172900, \t Total Gen Loss : 27.900957107543945, \t Total Dis Loss : 3.3538359275553375e-05\n",
      "Steps : 173000, \t Total Gen Loss : 27.340126037597656, \t Total Dis Loss : 2.1594696590909734e-05\n",
      "Steps : 173100, \t Total Gen Loss : 29.526845932006836, \t Total Dis Loss : 1.2700708794000093e-05\n",
      "Steps : 173200, \t Total Gen Loss : 29.18081283569336, \t Total Dis Loss : 1.2372789569781162e-05\n",
      "Steps : 173300, \t Total Gen Loss : 29.438318252563477, \t Total Dis Loss : 1.4003665455675218e-05\n",
      "Steps : 173400, \t Total Gen Loss : 27.799766540527344, \t Total Dis Loss : 1.815462383092381e-05\n",
      "Steps : 173500, \t Total Gen Loss : 28.605602264404297, \t Total Dis Loss : 4.4703938328893855e-06\n",
      "Steps : 173600, \t Total Gen Loss : 30.895980834960938, \t Total Dis Loss : 4.669920599553734e-06\n",
      "Steps : 173700, \t Total Gen Loss : 29.241241455078125, \t Total Dis Loss : 3.0101919037406333e-05\n",
      "Steps : 173800, \t Total Gen Loss : 31.408248901367188, \t Total Dis Loss : 2.6903820980805904e-05\n",
      "Steps : 173900, \t Total Gen Loss : 29.659950256347656, \t Total Dis Loss : 1.0940999345621094e-05\n",
      "Steps : 174000, \t Total Gen Loss : 30.2689266204834, \t Total Dis Loss : 4.760059528052807e-05\n",
      "Steps : 174100, \t Total Gen Loss : 28.80474853515625, \t Total Dis Loss : 9.955139103112742e-05\n",
      "Steps : 174200, \t Total Gen Loss : 25.82140350341797, \t Total Dis Loss : 2.7866390155395493e-05\n",
      "Steps : 174300, \t Total Gen Loss : 24.90481948852539, \t Total Dis Loss : 0.0001945031835930422\n",
      "Time for epoch 31 is 77.44337964057922 sec\n",
      "Steps : 174400, \t Total Gen Loss : 27.546602249145508, \t Total Dis Loss : 0.0006980663747526705\n",
      "Steps : 174500, \t Total Gen Loss : 29.903409957885742, \t Total Dis Loss : 9.058461000677198e-05\n",
      "Steps : 174600, \t Total Gen Loss : 27.038036346435547, \t Total Dis Loss : 0.00012164097279310226\n",
      "Steps : 174700, \t Total Gen Loss : 28.018356323242188, \t Total Dis Loss : 6.71458401484415e-05\n",
      "Steps : 174800, \t Total Gen Loss : 27.907699584960938, \t Total Dis Loss : 0.00028244126588106155\n",
      "Steps : 174900, \t Total Gen Loss : 32.588619232177734, \t Total Dis Loss : 4.4350021198624745e-05\n",
      "Steps : 175000, \t Total Gen Loss : 29.565567016601562, \t Total Dis Loss : 5.111423888592981e-05\n",
      "Steps : 175100, \t Total Gen Loss : 26.818965911865234, \t Total Dis Loss : 7.406866643577814e-05\n",
      "Steps : 175200, \t Total Gen Loss : 28.178146362304688, \t Total Dis Loss : 9.418861009180546e-05\n",
      "Steps : 175300, \t Total Gen Loss : 29.834962844848633, \t Total Dis Loss : 2.7447031243355013e-05\n",
      "Steps : 175400, \t Total Gen Loss : 27.92755699157715, \t Total Dis Loss : 0.00011850239388877526\n",
      "Steps : 175500, \t Total Gen Loss : 24.720787048339844, \t Total Dis Loss : 0.00012143817002652213\n",
      "Steps : 175600, \t Total Gen Loss : 27.63850975036621, \t Total Dis Loss : 0.00010087285772897303\n",
      "Steps : 175700, \t Total Gen Loss : 26.936477661132812, \t Total Dis Loss : 3.826704778475687e-05\n",
      "Steps : 175800, \t Total Gen Loss : 25.15313720703125, \t Total Dis Loss : 0.00031084398506209254\n",
      "Steps : 175900, \t Total Gen Loss : 29.25720977783203, \t Total Dis Loss : 0.00013291288632899523\n",
      "Steps : 176000, \t Total Gen Loss : 29.27926254272461, \t Total Dis Loss : 7.569778244942427e-05\n",
      "Steps : 176100, \t Total Gen Loss : 28.34882354736328, \t Total Dis Loss : 4.9886297347256914e-05\n",
      "Steps : 176200, \t Total Gen Loss : 27.320350646972656, \t Total Dis Loss : 0.00030306520056910813\n",
      "Steps : 176300, \t Total Gen Loss : 26.74715805053711, \t Total Dis Loss : 0.00012862270523328334\n",
      "Steps : 176400, \t Total Gen Loss : 27.91579246520996, \t Total Dis Loss : 1.965578667295631e-05\n",
      "Steps : 176500, \t Total Gen Loss : 29.3658447265625, \t Total Dis Loss : 2.3216629415401258e-05\n",
      "Steps : 176600, \t Total Gen Loss : 28.408344268798828, \t Total Dis Loss : 0.00011554455704754218\n",
      "Steps : 176700, \t Total Gen Loss : 31.98508071899414, \t Total Dis Loss : 1.5871328287175857e-05\n",
      "Steps : 176800, \t Total Gen Loss : 23.530662536621094, \t Total Dis Loss : 0.5503315329551697\n",
      "Steps : 176900, \t Total Gen Loss : 31.205177307128906, \t Total Dis Loss : 0.000214960309676826\n",
      "Steps : 177000, \t Total Gen Loss : 33.930442810058594, \t Total Dis Loss : 1.3696130736207124e-05\n",
      "Steps : 177100, \t Total Gen Loss : 33.18687438964844, \t Total Dis Loss : 3.8095599848020356e-06\n",
      "Steps : 177200, \t Total Gen Loss : 29.006439208984375, \t Total Dis Loss : 8.822997187962756e-06\n",
      "Steps : 177300, \t Total Gen Loss : 30.525083541870117, \t Total Dis Loss : 0.0007790971430949867\n",
      "Steps : 177400, \t Total Gen Loss : 29.789562225341797, \t Total Dis Loss : 6.17189198237611e-06\n",
      "Steps : 177500, \t Total Gen Loss : 26.204679489135742, \t Total Dis Loss : 3.188331538694911e-05\n",
      "Steps : 177600, \t Total Gen Loss : 30.20458984375, \t Total Dis Loss : 2.29811885219533e-05\n",
      "Steps : 177700, \t Total Gen Loss : 31.171749114990234, \t Total Dis Loss : 1.6506791098436224e-06\n",
      "Steps : 177800, \t Total Gen Loss : 29.336864471435547, \t Total Dis Loss : 2.3720425815554336e-05\n",
      "Steps : 177900, \t Total Gen Loss : 24.64056396484375, \t Total Dis Loss : 0.0002643117040861398\n",
      "Steps : 178000, \t Total Gen Loss : 30.39031982421875, \t Total Dis Loss : 1.4660462511528749e-05\n",
      "Steps : 178100, \t Total Gen Loss : 29.166536331176758, \t Total Dis Loss : 1.3724509699386545e-05\n",
      "Steps : 178200, \t Total Gen Loss : 28.11539077758789, \t Total Dis Loss : 2.075397969747428e-05\n",
      "Steps : 178300, \t Total Gen Loss : 25.90070915222168, \t Total Dis Loss : 0.00017566836322657764\n",
      "Steps : 178400, \t Total Gen Loss : 29.501739501953125, \t Total Dis Loss : 1.846968734753318e-05\n",
      "Steps : 178500, \t Total Gen Loss : 29.77035140991211, \t Total Dis Loss : 6.528657650051173e-06\n",
      "Steps : 178600, \t Total Gen Loss : 36.524803161621094, \t Total Dis Loss : 2.8987393307033926e-05\n",
      "Steps : 178700, \t Total Gen Loss : 31.583053588867188, \t Total Dis Loss : 0.0001140517633757554\n",
      "Steps : 178800, \t Total Gen Loss : 31.69158172607422, \t Total Dis Loss : 5.584892278420739e-06\n",
      "Steps : 178900, \t Total Gen Loss : 33.03432083129883, \t Total Dis Loss : 0.00014208894572220743\n",
      "Steps : 179000, \t Total Gen Loss : 32.81079864501953, \t Total Dis Loss : 4.740415533888154e-05\n",
      "Steps : 179100, \t Total Gen Loss : 30.114660263061523, \t Total Dis Loss : 7.596933755849022e-06\n",
      "Steps : 179200, \t Total Gen Loss : 25.192617416381836, \t Total Dis Loss : 7.54736247472465e-05\n",
      "Steps : 179300, \t Total Gen Loss : 28.716943740844727, \t Total Dis Loss : 5.325341044226661e-05\n",
      "Steps : 179400, \t Total Gen Loss : 28.037918090820312, \t Total Dis Loss : 1.4276141882874072e-05\n",
      "Steps : 179500, \t Total Gen Loss : 28.1580810546875, \t Total Dis Loss : 1.8600054318085313e-05\n",
      "Steps : 179600, \t Total Gen Loss : 27.14004135131836, \t Total Dis Loss : 1.6200419850065373e-05\n",
      "Steps : 179700, \t Total Gen Loss : 27.5721378326416, \t Total Dis Loss : 9.913075700751506e-06\n",
      "Steps : 179800, \t Total Gen Loss : 27.529354095458984, \t Total Dis Loss : 1.4644745533587411e-05\n",
      "Steps : 179900, \t Total Gen Loss : 26.961536407470703, \t Total Dis Loss : 1.0105376532010268e-05\n",
      "Steps : 180000, \t Total Gen Loss : 27.34343719482422, \t Total Dis Loss : 1.482495827076491e-05\n",
      "Time for epoch 32 is 77.41474866867065 sec\n",
      "Steps : 180100, \t Total Gen Loss : 29.00865364074707, \t Total Dis Loss : 9.466226401855238e-06\n",
      "Steps : 180200, \t Total Gen Loss : 28.705276489257812, \t Total Dis Loss : 9.375871741212904e-06\n",
      "Steps : 180300, \t Total Gen Loss : 27.92160415649414, \t Total Dis Loss : 9.721665264805779e-06\n",
      "Steps : 180400, \t Total Gen Loss : 30.02761459350586, \t Total Dis Loss : 5.2995992518845014e-06\n",
      "Steps : 180500, \t Total Gen Loss : 29.6009464263916, \t Total Dis Loss : 4.8973770390148275e-06\n",
      "Steps : 180600, \t Total Gen Loss : 28.855792999267578, \t Total Dis Loss : 1.710393735265825e-05\n",
      "Steps : 180700, \t Total Gen Loss : 28.48193359375, \t Total Dis Loss : 1.6425638023065403e-05\n",
      "Steps : 180800, \t Total Gen Loss : 26.747074127197266, \t Total Dis Loss : 0.0008187323692254722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 180900, \t Total Gen Loss : 27.111949920654297, \t Total Dis Loss : 7.056833419483155e-05\n",
      "Steps : 181000, \t Total Gen Loss : 29.989364624023438, \t Total Dis Loss : 2.7298023269395344e-05\n",
      "Steps : 181100, \t Total Gen Loss : 24.568302154541016, \t Total Dis Loss : 0.0001293832465307787\n",
      "Steps : 181200, \t Total Gen Loss : 27.7406005859375, \t Total Dis Loss : 3.802403443842195e-05\n",
      "Steps : 181300, \t Total Gen Loss : 26.716493606567383, \t Total Dis Loss : 4.887279646936804e-05\n",
      "Steps : 181400, \t Total Gen Loss : 26.835981369018555, \t Total Dis Loss : 3.342628770042211e-05\n",
      "Steps : 181500, \t Total Gen Loss : 27.475383758544922, \t Total Dis Loss : 5.0742601160891354e-05\n",
      "Steps : 181600, \t Total Gen Loss : 30.619077682495117, \t Total Dis Loss : 9.433194645680487e-05\n",
      "Steps : 181700, \t Total Gen Loss : 23.68819808959961, \t Total Dis Loss : 0.0002922466374002397\n",
      "Steps : 181800, \t Total Gen Loss : 27.808258056640625, \t Total Dis Loss : 8.152181544573978e-05\n",
      "Steps : 181900, \t Total Gen Loss : 26.807615280151367, \t Total Dis Loss : 4.118063225178048e-05\n",
      "Steps : 182000, \t Total Gen Loss : 28.10456085205078, \t Total Dis Loss : 7.856098818592727e-05\n",
      "Steps : 182100, \t Total Gen Loss : 28.091148376464844, \t Total Dis Loss : 4.0565297240391374e-05\n",
      "Steps : 182200, \t Total Gen Loss : 28.283798217773438, \t Total Dis Loss : 2.0218894860590808e-05\n",
      "Steps : 182300, \t Total Gen Loss : 27.44207000732422, \t Total Dis Loss : 2.9501023163902573e-05\n",
      "Steps : 182400, \t Total Gen Loss : 27.404850006103516, \t Total Dis Loss : 1.3399177987594157e-05\n",
      "Steps : 182500, \t Total Gen Loss : 26.3203182220459, \t Total Dis Loss : 9.682322342996486e-06\n",
      "Steps : 182600, \t Total Gen Loss : 32.38348388671875, \t Total Dis Loss : 4.148261268710485e-06\n",
      "Steps : 182700, \t Total Gen Loss : 26.522607803344727, \t Total Dis Loss : 1.0382338587078266e-05\n",
      "Steps : 182800, \t Total Gen Loss : 29.576181411743164, \t Total Dis Loss : 7.151661066018278e-06\n",
      "Steps : 182900, \t Total Gen Loss : 28.07113265991211, \t Total Dis Loss : 1.115180566557683e-05\n",
      "Steps : 183000, \t Total Gen Loss : 32.062171936035156, \t Total Dis Loss : 6.345477231661789e-06\n",
      "Steps : 183100, \t Total Gen Loss : 30.47762680053711, \t Total Dis Loss : 1.4537607967213262e-05\n",
      "Steps : 183200, \t Total Gen Loss : 27.536808013916016, \t Total Dis Loss : 8.21628054836765e-06\n",
      "Steps : 183300, \t Total Gen Loss : 26.737903594970703, \t Total Dis Loss : 1.2808823157683946e-05\n",
      "Steps : 183400, \t Total Gen Loss : 29.702823638916016, \t Total Dis Loss : 9.635813512431923e-06\n",
      "Steps : 183500, \t Total Gen Loss : 30.00321388244629, \t Total Dis Loss : 4.8392398639407475e-06\n",
      "Steps : 183600, \t Total Gen Loss : 28.227523803710938, \t Total Dis Loss : 6.1677878875343595e-06\n",
      "Steps : 183700, \t Total Gen Loss : 30.990068435668945, \t Total Dis Loss : 5.370258350012591e-06\n",
      "Steps : 183800, \t Total Gen Loss : 31.989795684814453, \t Total Dis Loss : 5.0603334784682374e-06\n",
      "Steps : 183900, \t Total Gen Loss : 28.51322364807129, \t Total Dis Loss : 4.63146989204688e-06\n",
      "Steps : 184000, \t Total Gen Loss : 31.448577880859375, \t Total Dis Loss : 5.866058018000331e-06\n",
      "Steps : 184100, \t Total Gen Loss : 27.45582389831543, \t Total Dis Loss : 0.0012581739574670792\n",
      "Steps : 184200, \t Total Gen Loss : 25.854358673095703, \t Total Dis Loss : 0.002144737634807825\n",
      "Steps : 184300, \t Total Gen Loss : 30.222707748413086, \t Total Dis Loss : 8.837760105961934e-06\n",
      "Steps : 184400, \t Total Gen Loss : 31.676986694335938, \t Total Dis Loss : 1.7742642739904113e-05\n",
      "Steps : 184500, \t Total Gen Loss : 28.751590728759766, \t Total Dis Loss : 0.00013285705063026398\n",
      "Steps : 184600, \t Total Gen Loss : 26.647613525390625, \t Total Dis Loss : 8.880329551175237e-05\n",
      "Steps : 184700, \t Total Gen Loss : 27.240203857421875, \t Total Dis Loss : 4.0770279156276956e-05\n",
      "Steps : 184800, \t Total Gen Loss : 27.47963523864746, \t Total Dis Loss : 1.9882212654920295e-05\n",
      "Steps : 184900, \t Total Gen Loss : 29.99890899658203, \t Total Dis Loss : 1.43952729558805e-05\n",
      "Steps : 185000, \t Total Gen Loss : 30.65412712097168, \t Total Dis Loss : 1.6203093764488585e-05\n",
      "Steps : 185100, \t Total Gen Loss : 23.59421157836914, \t Total Dis Loss : 6.925126217538491e-05\n",
      "Steps : 185200, \t Total Gen Loss : 28.734046936035156, \t Total Dis Loss : 2.7023734219255857e-05\n",
      "Steps : 185300, \t Total Gen Loss : 26.103199005126953, \t Total Dis Loss : 4.135103881708346e-05\n",
      "Steps : 185400, \t Total Gen Loss : 29.356395721435547, \t Total Dis Loss : 7.150495548557956e-06\n",
      "Steps : 185500, \t Total Gen Loss : 30.865135192871094, \t Total Dis Loss : 1.1414267646614462e-05\n",
      "Steps : 185600, \t Total Gen Loss : 27.524364471435547, \t Total Dis Loss : 0.0003373352810740471\n",
      "Time for epoch 33 is 78.08388352394104 sec\n",
      "Steps : 185700, \t Total Gen Loss : 26.384876251220703, \t Total Dis Loss : 0.001907811500132084\n",
      "Steps : 185800, \t Total Gen Loss : 30.706157684326172, \t Total Dis Loss : 0.00011125235323561355\n",
      "Steps : 185900, \t Total Gen Loss : 33.20217514038086, \t Total Dis Loss : 0.04479080066084862\n",
      "Steps : 186000, \t Total Gen Loss : 31.883548736572266, \t Total Dis Loss : 0.0002561514265835285\n",
      "Steps : 186100, \t Total Gen Loss : 32.32600021362305, \t Total Dis Loss : 0.00023372331634163857\n",
      "Steps : 186200, \t Total Gen Loss : 37.45269775390625, \t Total Dis Loss : 8.45392878545681e-06\n",
      "Steps : 186300, \t Total Gen Loss : 36.88565444946289, \t Total Dis Loss : 0.0008356606122106314\n",
      "Steps : 186400, \t Total Gen Loss : 34.17352294921875, \t Total Dis Loss : 0.00028076922171749175\n",
      "Steps : 186500, \t Total Gen Loss : 31.947834014892578, \t Total Dis Loss : 7.633346103830263e-05\n",
      "Steps : 186600, \t Total Gen Loss : 30.948013305664062, \t Total Dis Loss : 0.0001353813277091831\n",
      "Steps : 186700, \t Total Gen Loss : 30.803808212280273, \t Total Dis Loss : 4.780840390594676e-05\n",
      "Steps : 186800, \t Total Gen Loss : 30.42901611328125, \t Total Dis Loss : 5.576857438427396e-05\n",
      "Steps : 186900, \t Total Gen Loss : 32.74820327758789, \t Total Dis Loss : 5.3718955314252526e-05\n",
      "Steps : 187000, \t Total Gen Loss : 31.318687438964844, \t Total Dis Loss : 2.6858902856474742e-05\n",
      "Steps : 187100, \t Total Gen Loss : 28.998491287231445, \t Total Dis Loss : 4.2902691347990185e-05\n",
      "Steps : 187200, \t Total Gen Loss : 29.76665687561035, \t Total Dis Loss : 0.00021664770611096174\n",
      "Steps : 187300, \t Total Gen Loss : 29.122207641601562, \t Total Dis Loss : 7.930707943160087e-05\n",
      "Steps : 187400, \t Total Gen Loss : 30.29833221435547, \t Total Dis Loss : 0.0001888079132186249\n",
      "Steps : 187500, \t Total Gen Loss : 28.914241790771484, \t Total Dis Loss : 4.90721758978907e-05\n",
      "Steps : 187600, \t Total Gen Loss : 29.883071899414062, \t Total Dis Loss : 4.5549561036750674e-05\n",
      "Steps : 187700, \t Total Gen Loss : 31.177892684936523, \t Total Dis Loss : 2.4399418180109933e-05\n",
      "Steps : 187800, \t Total Gen Loss : 30.169235229492188, \t Total Dis Loss : 1.1976732821494807e-05\n",
      "Steps : 187900, \t Total Gen Loss : 28.61667251586914, \t Total Dis Loss : 6.818084511905909e-05\n",
      "Steps : 188000, \t Total Gen Loss : 29.11736297607422, \t Total Dis Loss : 2.6010216970462352e-05\n",
      "Steps : 188100, \t Total Gen Loss : 30.44428062438965, \t Total Dis Loss : 1.5167765013757162e-05\n",
      "Steps : 188200, \t Total Gen Loss : 31.81165313720703, \t Total Dis Loss : 1.747351052472368e-05\n",
      "Steps : 188300, \t Total Gen Loss : 31.714338302612305, \t Total Dis Loss : 2.4605655198683962e-05\n",
      "Steps : 188400, \t Total Gen Loss : 34.16203689575195, \t Total Dis Loss : 1.9830740711768158e-05\n",
      "Steps : 188500, \t Total Gen Loss : 32.30925369262695, \t Total Dis Loss : 8.76516514836112e-06\n",
      "Steps : 188600, \t Total Gen Loss : 28.260059356689453, \t Total Dis Loss : 4.048068876727484e-05\n",
      "Steps : 188700, \t Total Gen Loss : 31.70037841796875, \t Total Dis Loss : 7.998888031579554e-05\n",
      "Steps : 188800, \t Total Gen Loss : 30.44685935974121, \t Total Dis Loss : 4.932501178700477e-05\n",
      "Steps : 188900, \t Total Gen Loss : 31.691631317138672, \t Total Dis Loss : 5.877498915651813e-05\n",
      "Steps : 189000, \t Total Gen Loss : 34.40003967285156, \t Total Dis Loss : 1.985535527637694e-05\n",
      "Steps : 189100, \t Total Gen Loss : 31.655197143554688, \t Total Dis Loss : 2.4873423171811737e-05\n",
      "Steps : 189200, \t Total Gen Loss : 31.669795989990234, \t Total Dis Loss : 0.00026713451370596886\n",
      "Steps : 189300, \t Total Gen Loss : 31.63538360595703, \t Total Dis Loss : 2.4200544430641457e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 189400, \t Total Gen Loss : 27.219554901123047, \t Total Dis Loss : 0.010478921234607697\n",
      "Steps : 189500, \t Total Gen Loss : 34.704105377197266, \t Total Dis Loss : 1.353102288703667e-05\n",
      "Steps : 189600, \t Total Gen Loss : 32.94737243652344, \t Total Dis Loss : 2.5047731469385326e-05\n",
      "Steps : 189700, \t Total Gen Loss : 32.16856384277344, \t Total Dis Loss : 1.65127421496436e-05\n",
      "Steps : 189800, \t Total Gen Loss : 32.43590545654297, \t Total Dis Loss : 3.814033334492706e-05\n",
      "Steps : 189900, \t Total Gen Loss : 28.48406982421875, \t Total Dis Loss : 0.00022721900313626975\n",
      "Steps : 190000, \t Total Gen Loss : 32.01054000854492, \t Total Dis Loss : 1.4305506738310214e-05\n",
      "Steps : 190100, \t Total Gen Loss : 30.695781707763672, \t Total Dis Loss : 5.681598122464493e-05\n",
      "Steps : 190200, \t Total Gen Loss : 33.73627471923828, \t Total Dis Loss : 8.306129893753678e-05\n",
      "Steps : 190300, \t Total Gen Loss : 31.31879425048828, \t Total Dis Loss : 4.733667446998879e-05\n",
      "Steps : 190400, \t Total Gen Loss : 30.973377227783203, \t Total Dis Loss : 1.6518961274414323e-05\n",
      "Steps : 190500, \t Total Gen Loss : 28.79703140258789, \t Total Dis Loss : 9.729710654937662e-06\n",
      "Steps : 190600, \t Total Gen Loss : 36.692996978759766, \t Total Dis Loss : 8.788811101112515e-05\n",
      "Steps : 190700, \t Total Gen Loss : 34.23896789550781, \t Total Dis Loss : 0.00039072646177373827\n",
      "Steps : 190800, \t Total Gen Loss : 35.310367584228516, \t Total Dis Loss : 4.3384225136833265e-05\n",
      "Steps : 190900, \t Total Gen Loss : 32.87799072265625, \t Total Dis Loss : 0.000122389494208619\n",
      "Steps : 191000, \t Total Gen Loss : 31.97703742980957, \t Total Dis Loss : 5.460127067635767e-05\n",
      "Steps : 191100, \t Total Gen Loss : 30.21871566772461, \t Total Dis Loss : 0.00017074588686227798\n",
      "Steps : 191200, \t Total Gen Loss : 31.73634910583496, \t Total Dis Loss : 0.00011653628462227061\n",
      "Time for epoch 34 is 77.61341524124146 sec\n",
      "Steps : 191300, \t Total Gen Loss : 34.874366760253906, \t Total Dis Loss : 0.00014108253526501358\n",
      "Steps : 191400, \t Total Gen Loss : 31.573162078857422, \t Total Dis Loss : 2.966826650663279e-05\n",
      "Steps : 191500, \t Total Gen Loss : 27.965763092041016, \t Total Dis Loss : 3.128324533463456e-05\n",
      "Steps : 191600, \t Total Gen Loss : 24.799991607666016, \t Total Dis Loss : 0.0015275388723239303\n",
      "Steps : 191700, \t Total Gen Loss : 31.265235900878906, \t Total Dis Loss : 3.814908268395811e-06\n",
      "Steps : 191800, \t Total Gen Loss : 26.76535415649414, \t Total Dis Loss : 6.944847700651735e-05\n",
      "Steps : 191900, \t Total Gen Loss : 27.407913208007812, \t Total Dis Loss : 6.851687794551253e-05\n",
      "Steps : 192000, \t Total Gen Loss : 26.06498146057129, \t Total Dis Loss : 6.021093940944411e-05\n",
      "Steps : 192100, \t Total Gen Loss : 28.23042869567871, \t Total Dis Loss : 8.545529271941632e-05\n",
      "Steps : 192200, \t Total Gen Loss : 26.15699005126953, \t Total Dis Loss : 0.0007394791464321315\n",
      "Steps : 192300, \t Total Gen Loss : 29.311960220336914, \t Total Dis Loss : 5.9576527746685315e-06\n",
      "Steps : 192400, \t Total Gen Loss : 31.943498611450195, \t Total Dis Loss : 4.2492181819397956e-05\n",
      "Steps : 192500, \t Total Gen Loss : 25.30803871154785, \t Total Dis Loss : 0.00022039547911845148\n",
      "Steps : 192600, \t Total Gen Loss : 27.999095916748047, \t Total Dis Loss : 3.1151779694482684e-05\n",
      "Steps : 192700, \t Total Gen Loss : 26.180953979492188, \t Total Dis Loss : 0.0002218205772805959\n",
      "Steps : 192800, \t Total Gen Loss : 29.84842300415039, \t Total Dis Loss : 1.8045071556116454e-05\n",
      "Steps : 192900, \t Total Gen Loss : 28.49401092529297, \t Total Dis Loss : 0.0001093189712264575\n",
      "Steps : 193000, \t Total Gen Loss : 24.536170959472656, \t Total Dis Loss : 8.566110773244873e-05\n",
      "Steps : 193100, \t Total Gen Loss : 26.095415115356445, \t Total Dis Loss : 8.722572238184512e-05\n",
      "Steps : 193200, \t Total Gen Loss : 27.164409637451172, \t Total Dis Loss : 0.00012993918790016323\n",
      "Steps : 193300, \t Total Gen Loss : 24.699764251708984, \t Total Dis Loss : 0.0009423833689652383\n",
      "Steps : 193400, \t Total Gen Loss : 25.37375259399414, \t Total Dis Loss : 0.000135556110762991\n",
      "Steps : 193500, \t Total Gen Loss : 22.893373489379883, \t Total Dis Loss : 0.0007267592591233552\n",
      "Steps : 193600, \t Total Gen Loss : 30.459800720214844, \t Total Dis Loss : 6.22665902483277e-05\n",
      "Steps : 193700, \t Total Gen Loss : 27.169151306152344, \t Total Dis Loss : 5.1757418987108395e-05\n",
      "Steps : 193800, \t Total Gen Loss : 25.4163818359375, \t Total Dis Loss : 3.583935176720843e-05\n",
      "Steps : 193900, \t Total Gen Loss : 26.89022445678711, \t Total Dis Loss : 3.830347486655228e-05\n",
      "Steps : 194000, \t Total Gen Loss : 27.19961929321289, \t Total Dis Loss : 6.381782441167161e-05\n",
      "Steps : 194100, \t Total Gen Loss : 26.738468170166016, \t Total Dis Loss : 3.1625269912183285e-05\n",
      "Steps : 194200, \t Total Gen Loss : 27.9949951171875, \t Total Dis Loss : 7.69389298511669e-05\n",
      "Steps : 194300, \t Total Gen Loss : 27.77236557006836, \t Total Dis Loss : 1.7858728824649006e-05\n",
      "Steps : 194400, \t Total Gen Loss : 28.501949310302734, \t Total Dis Loss : 1.8034939785138704e-05\n",
      "Steps : 194500, \t Total Gen Loss : 26.70730209350586, \t Total Dis Loss : 0.0007153765764087439\n",
      "Steps : 194600, \t Total Gen Loss : 27.136009216308594, \t Total Dis Loss : 0.00014461168029811233\n",
      "Steps : 194700, \t Total Gen Loss : 24.63454818725586, \t Total Dis Loss : 4.375394928501919e-05\n",
      "Steps : 194800, \t Total Gen Loss : 27.228836059570312, \t Total Dis Loss : 4.966859705746174e-05\n",
      "Steps : 194900, \t Total Gen Loss : 31.318859100341797, \t Total Dis Loss : 4.342922693467699e-05\n",
      "Steps : 195000, \t Total Gen Loss : 29.23276138305664, \t Total Dis Loss : 5.5371383496094495e-05\n",
      "Steps : 195100, \t Total Gen Loss : 26.93368148803711, \t Total Dis Loss : 0.0001177136437036097\n",
      "Steps : 195200, \t Total Gen Loss : 24.78856658935547, \t Total Dis Loss : 0.00037718378007411957\n",
      "Steps : 195300, \t Total Gen Loss : 29.781780242919922, \t Total Dis Loss : 1.3735328138864134e-05\n",
      "Steps : 195400, \t Total Gen Loss : 26.929656982421875, \t Total Dis Loss : 3.8657683035125956e-05\n",
      "Steps : 195500, \t Total Gen Loss : 24.603900909423828, \t Total Dis Loss : 6.533254781970754e-05\n",
      "Steps : 195600, \t Total Gen Loss : 27.325634002685547, \t Total Dis Loss : 4.8252833948936313e-05\n",
      "Steps : 195700, \t Total Gen Loss : 27.102706909179688, \t Total Dis Loss : 1.713467099762056e-05\n",
      "Steps : 195800, \t Total Gen Loss : 28.265352249145508, \t Total Dis Loss : 2.3968477762537077e-05\n",
      "Steps : 195900, \t Total Gen Loss : 33.589080810546875, \t Total Dis Loss : 9.700251393951476e-05\n",
      "Steps : 196000, \t Total Gen Loss : 29.73505973815918, \t Total Dis Loss : 2.396415402472485e-05\n",
      "Steps : 196100, \t Total Gen Loss : 29.226234436035156, \t Total Dis Loss : 5.52160527149681e-05\n",
      "Steps : 196200, \t Total Gen Loss : 32.87131881713867, \t Total Dis Loss : 1.0817629117809702e-05\n",
      "Steps : 196300, \t Total Gen Loss : 31.683696746826172, \t Total Dis Loss : 1.891533611342311e-05\n",
      "Steps : 196400, \t Total Gen Loss : 31.19502067565918, \t Total Dis Loss : 3.4320889881200856e-06\n",
      "Steps : 196500, \t Total Gen Loss : 31.211029052734375, \t Total Dis Loss : 1.8565464415587485e-05\n",
      "Steps : 196600, \t Total Gen Loss : 28.2474308013916, \t Total Dis Loss : 3.0318155040731654e-05\n",
      "Steps : 196700, \t Total Gen Loss : 30.359506607055664, \t Total Dis Loss : 9.91420674836263e-05\n",
      "Steps : 196800, \t Total Gen Loss : 27.219236373901367, \t Total Dis Loss : 3.313310298835859e-05\n",
      "Time for epoch 35 is 76.91993856430054 sec\n",
      "Steps : 196900, \t Total Gen Loss : 34.19554901123047, \t Total Dis Loss : 1.1593627277761698e-05\n",
      "Steps : 197000, \t Total Gen Loss : 28.48352813720703, \t Total Dis Loss : 8.861672540660948e-06\n",
      "Steps : 197100, \t Total Gen Loss : 32.197261810302734, \t Total Dis Loss : 4.127469765080605e-06\n",
      "Steps : 197200, \t Total Gen Loss : 28.6093807220459, \t Total Dis Loss : 6.52480866847327e-06\n",
      "Steps : 197300, \t Total Gen Loss : 31.826656341552734, \t Total Dis Loss : 1.1791764336521737e-05\n",
      "Steps : 197400, \t Total Gen Loss : 30.41153335571289, \t Total Dis Loss : 1.2525236343208235e-05\n",
      "Steps : 197500, \t Total Gen Loss : 34.526920318603516, \t Total Dis Loss : 4.5163351387600414e-06\n",
      "Steps : 197600, \t Total Gen Loss : 26.572368621826172, \t Total Dis Loss : 0.0009913478279486299\n",
      "Steps : 197700, \t Total Gen Loss : 36.077301025390625, \t Total Dis Loss : 0.00022074556909501553\n",
      "Steps : 197800, \t Total Gen Loss : 25.887622833251953, \t Total Dis Loss : 0.0002562437148299068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 197900, \t Total Gen Loss : 29.146520614624023, \t Total Dis Loss : 0.00041286839405074716\n",
      "Steps : 198000, \t Total Gen Loss : 33.50325012207031, \t Total Dis Loss : 0.00010252988431602716\n",
      "Steps : 198100, \t Total Gen Loss : 30.5123348236084, \t Total Dis Loss : 9.205430978909135e-05\n",
      "Steps : 198200, \t Total Gen Loss : 29.275968551635742, \t Total Dis Loss : 7.24988931324333e-05\n",
      "Steps : 198300, \t Total Gen Loss : 27.24555015563965, \t Total Dis Loss : 0.00025489143445156515\n",
      "Steps : 198400, \t Total Gen Loss : 24.920547485351562, \t Total Dis Loss : 0.0001374878193018958\n",
      "Steps : 198500, \t Total Gen Loss : 25.12252426147461, \t Total Dis Loss : 0.00019794337276834995\n",
      "Steps : 198600, \t Total Gen Loss : 28.359676361083984, \t Total Dis Loss : 0.00014467997243627906\n",
      "Steps : 198700, \t Total Gen Loss : 29.487024307250977, \t Total Dis Loss : 4.815300417249091e-05\n",
      "Steps : 198800, \t Total Gen Loss : 28.71018409729004, \t Total Dis Loss : 0.00022295858070719987\n",
      "Steps : 198900, \t Total Gen Loss : 27.397571563720703, \t Total Dis Loss : 8.915069338399917e-05\n",
      "Steps : 199000, \t Total Gen Loss : 25.74390411376953, \t Total Dis Loss : 0.0001088223434635438\n",
      "Steps : 199100, \t Total Gen Loss : 29.616565704345703, \t Total Dis Loss : 1.8623810319695622e-05\n",
      "Steps : 199200, \t Total Gen Loss : 27.93449592590332, \t Total Dis Loss : 0.00027580198366194963\n",
      "Steps : 199300, \t Total Gen Loss : 27.535484313964844, \t Total Dis Loss : 9.010872599901631e-05\n",
      "Steps : 199400, \t Total Gen Loss : 28.952882766723633, \t Total Dis Loss : 9.154848521575332e-05\n",
      "Steps : 199500, \t Total Gen Loss : 27.686052322387695, \t Total Dis Loss : 8.119822450680658e-05\n",
      "Steps : 199600, \t Total Gen Loss : 30.7916316986084, \t Total Dis Loss : 8.462205005344003e-05\n",
      "Steps : 199700, \t Total Gen Loss : 28.123477935791016, \t Total Dis Loss : 2.0802004655706696e-05\n",
      "Steps : 199800, \t Total Gen Loss : 25.717754364013672, \t Total Dis Loss : 5.3061721700942144e-05\n",
      "Steps : 199900, \t Total Gen Loss : 32.37355041503906, \t Total Dis Loss : 2.5341632863273844e-05\n",
      "Steps : 200000, \t Total Gen Loss : 33.977840423583984, \t Total Dis Loss : 1.9601673557190225e-05\n",
      "Steps : 200100, \t Total Gen Loss : 35.9411735534668, \t Total Dis Loss : 3.4491495171096176e-05\n",
      "Steps : 200200, \t Total Gen Loss : 29.860095977783203, \t Total Dis Loss : 7.937331247376278e-05\n",
      "Steps : 200300, \t Total Gen Loss : 24.820425033569336, \t Total Dis Loss : 0.005541997496038675\n",
      "Steps : 200400, \t Total Gen Loss : 29.157461166381836, \t Total Dis Loss : 0.0001597181981196627\n",
      "Steps : 200500, \t Total Gen Loss : 25.992565155029297, \t Total Dis Loss : 9.850510105025023e-05\n",
      "Steps : 200600, \t Total Gen Loss : 26.49409294128418, \t Total Dis Loss : 0.0006881522131152451\n",
      "Steps : 200700, \t Total Gen Loss : 33.305931091308594, \t Total Dis Loss : 3.575180744519457e-05\n",
      "Steps : 200800, \t Total Gen Loss : 30.080677032470703, \t Total Dis Loss : 0.000261044770013541\n",
      "Steps : 200900, \t Total Gen Loss : 27.76524543762207, \t Total Dis Loss : 4.1776631405809894e-05\n",
      "Steps : 201000, \t Total Gen Loss : 33.77125549316406, \t Total Dis Loss : 0.00017631606897339225\n",
      "Steps : 201100, \t Total Gen Loss : 35.69188690185547, \t Total Dis Loss : 6.227276753634214e-05\n",
      "Steps : 201200, \t Total Gen Loss : 32.14994812011719, \t Total Dis Loss : 0.000583319109864533\n",
      "Steps : 201300, \t Total Gen Loss : 32.11147689819336, \t Total Dis Loss : 7.156920764828101e-05\n",
      "Steps : 201400, \t Total Gen Loss : 33.42815017700195, \t Total Dis Loss : 6.27627014182508e-06\n",
      "Steps : 201500, \t Total Gen Loss : 32.53917694091797, \t Total Dis Loss : 1.7648595530772582e-05\n",
      "Steps : 201600, \t Total Gen Loss : 34.436134338378906, \t Total Dis Loss : 2.0846673578489572e-05\n",
      "Steps : 201700, \t Total Gen Loss : 32.29828643798828, \t Total Dis Loss : 2.152141860278789e-06\n",
      "Steps : 201800, \t Total Gen Loss : 30.677688598632812, \t Total Dis Loss : 9.50977027969202e-06\n",
      "Steps : 201900, \t Total Gen Loss : 32.60468292236328, \t Total Dis Loss : 1.965971569006797e-05\n",
      "Steps : 202000, \t Total Gen Loss : 27.01293182373047, \t Total Dis Loss : 5.2286137361079454e-05\n",
      "Steps : 202100, \t Total Gen Loss : 33.020545959472656, \t Total Dis Loss : 7.103643838490825e-06\n",
      "Steps : 202200, \t Total Gen Loss : 32.1196403503418, \t Total Dis Loss : 5.239838355919346e-05\n",
      "Steps : 202300, \t Total Gen Loss : 33.66697692871094, \t Total Dis Loss : 5.533473995456006e-06\n",
      "Steps : 202400, \t Total Gen Loss : 29.41764259338379, \t Total Dis Loss : 9.123168638325296e-06\n",
      "Steps : 202500, \t Total Gen Loss : 30.033315658569336, \t Total Dis Loss : 5.47128092875937e-06\n",
      "Time for epoch 36 is 76.81068348884583 sec\n",
      "Steps : 202600, \t Total Gen Loss : 30.872440338134766, \t Total Dis Loss : 2.664654493855778e-05\n",
      "Steps : 202700, \t Total Gen Loss : 30.93523406982422, \t Total Dis Loss : 1.2539138879219536e-05\n",
      "Steps : 202800, \t Total Gen Loss : 34.30520248413086, \t Total Dis Loss : 1.1992469808319584e-05\n",
      "Steps : 202900, \t Total Gen Loss : 36.58325958251953, \t Total Dis Loss : 6.195207788550761e-06\n",
      "Steps : 203000, \t Total Gen Loss : 30.619964599609375, \t Total Dis Loss : 1.972820973605849e-05\n",
      "Steps : 203100, \t Total Gen Loss : 33.279396057128906, \t Total Dis Loss : 3.253024942750926e-06\n",
      "Steps : 203200, \t Total Gen Loss : 27.825292587280273, \t Total Dis Loss : 4.2298747757740784e-06\n",
      "Steps : 203300, \t Total Gen Loss : 28.811737060546875, \t Total Dis Loss : 3.953839041059837e-05\n",
      "Steps : 203400, \t Total Gen Loss : 30.381637573242188, \t Total Dis Loss : 8.169582542905118e-06\n",
      "Steps : 203500, \t Total Gen Loss : 32.962318420410156, \t Total Dis Loss : 3.296680733910762e-05\n",
      "Steps : 203600, \t Total Gen Loss : 31.99591064453125, \t Total Dis Loss : 4.8468351451447234e-05\n",
      "Steps : 203700, \t Total Gen Loss : 31.17042350769043, \t Total Dis Loss : 3.0565170163754374e-06\n",
      "Steps : 203800, \t Total Gen Loss : 30.521936416625977, \t Total Dis Loss : 6.507836133096134e-06\n",
      "Steps : 203900, \t Total Gen Loss : 34.2850456237793, \t Total Dis Loss : 8.37554398458451e-06\n",
      "Steps : 204000, \t Total Gen Loss : 30.429466247558594, \t Total Dis Loss : 2.5088320398936048e-05\n",
      "Steps : 204100, \t Total Gen Loss : 31.235065460205078, \t Total Dis Loss : 7.595573606522521e-06\n",
      "Steps : 204200, \t Total Gen Loss : 34.016151428222656, \t Total Dis Loss : 8.178900316124782e-06\n",
      "Steps : 204300, \t Total Gen Loss : 30.33150863647461, \t Total Dis Loss : 6.617078724957537e-06\n",
      "Steps : 204400, \t Total Gen Loss : 28.00173568725586, \t Total Dis Loss : 2.7637845050776377e-05\n",
      "Steps : 204500, \t Total Gen Loss : 30.920917510986328, \t Total Dis Loss : 1.4737431229150388e-05\n",
      "Steps : 204600, \t Total Gen Loss : 29.51498794555664, \t Total Dis Loss : 3.8618894905084744e-05\n",
      "Steps : 204700, \t Total Gen Loss : 29.084150314331055, \t Total Dis Loss : 1.0490152817510534e-05\n",
      "Steps : 204800, \t Total Gen Loss : 30.06978416442871, \t Total Dis Loss : 6.941212177480338e-06\n",
      "Steps : 204900, \t Total Gen Loss : 32.809391021728516, \t Total Dis Loss : 7.413058938254835e-06\n",
      "Steps : 205000, \t Total Gen Loss : 25.838356018066406, \t Total Dis Loss : 2.4512748495908454e-05\n",
      "Steps : 205100, \t Total Gen Loss : 29.43117332458496, \t Total Dis Loss : 2.690326437004842e-05\n",
      "Steps : 205200, \t Total Gen Loss : 29.746767044067383, \t Total Dis Loss : 2.2904421712155454e-05\n",
      "Steps : 205300, \t Total Gen Loss : 30.882293701171875, \t Total Dis Loss : 0.00021369710157159716\n",
      "Steps : 205400, \t Total Gen Loss : 32.139976501464844, \t Total Dis Loss : 4.734356843982823e-05\n",
      "Steps : 205500, \t Total Gen Loss : 32.171878814697266, \t Total Dis Loss : 1.4920087778591551e-05\n",
      "Steps : 205600, \t Total Gen Loss : 30.832210540771484, \t Total Dis Loss : 5.9169869928155094e-06\n",
      "Steps : 205700, \t Total Gen Loss : 30.01694107055664, \t Total Dis Loss : 3.6739853385370225e-05\n",
      "Steps : 205800, \t Total Gen Loss : 28.454160690307617, \t Total Dis Loss : 7.402448682114482e-05\n",
      "Steps : 205900, \t Total Gen Loss : 29.57180404663086, \t Total Dis Loss : 0.0001631473278393969\n",
      "Steps : 206000, \t Total Gen Loss : 26.51625633239746, \t Total Dis Loss : 7.826895307516679e-05\n",
      "Steps : 206100, \t Total Gen Loss : 27.21591567993164, \t Total Dis Loss : 0.00011016494681825861\n",
      "Steps : 206200, \t Total Gen Loss : 28.31859016418457, \t Total Dis Loss : 0.00011714926222339272\n",
      "Steps : 206300, \t Total Gen Loss : 25.307397842407227, \t Total Dis Loss : 5.951841376372613e-05\n",
      "Steps : 206400, \t Total Gen Loss : 27.820600509643555, \t Total Dis Loss : 0.00013898764154873788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 206500, \t Total Gen Loss : 28.085878372192383, \t Total Dis Loss : 5.194775803829543e-05\n",
      "Steps : 206600, \t Total Gen Loss : 26.270492553710938, \t Total Dis Loss : 0.00020860358199570328\n",
      "Steps : 206700, \t Total Gen Loss : 27.58285903930664, \t Total Dis Loss : 5.944946678937413e-05\n",
      "Steps : 206800, \t Total Gen Loss : 29.0974063873291, \t Total Dis Loss : 8.130564674502239e-05\n",
      "Steps : 206900, \t Total Gen Loss : 27.66107940673828, \t Total Dis Loss : 0.0020802775397896767\n",
      "Steps : 207000, \t Total Gen Loss : 29.626323699951172, \t Total Dis Loss : 2.8315782401477918e-05\n",
      "Steps : 207100, \t Total Gen Loss : 28.42858123779297, \t Total Dis Loss : 9.506769856670871e-06\n",
      "Steps : 207200, \t Total Gen Loss : 30.406009674072266, \t Total Dis Loss : 3.0126169804134406e-05\n",
      "Steps : 207300, \t Total Gen Loss : 26.223751068115234, \t Total Dis Loss : 1.8085947885992937e-05\n",
      "Steps : 207400, \t Total Gen Loss : 28.18924331665039, \t Total Dis Loss : 3.535683572408743e-05\n",
      "Steps : 207500, \t Total Gen Loss : 26.996747970581055, \t Total Dis Loss : 1.6041614799178205e-05\n",
      "Steps : 207600, \t Total Gen Loss : 24.713882446289062, \t Total Dis Loss : 4.275011087884195e-05\n",
      "Steps : 207700, \t Total Gen Loss : 30.535110473632812, \t Total Dis Loss : 8.693396921444219e-06\n",
      "Steps : 207800, \t Total Gen Loss : 26.50796890258789, \t Total Dis Loss : 1.0714205018302891e-05\n",
      "Steps : 207900, \t Total Gen Loss : 30.82139015197754, \t Total Dis Loss : 0.00034036103170365095\n",
      "Steps : 208000, \t Total Gen Loss : 26.473264694213867, \t Total Dis Loss : 4.48913051513955e-05\n",
      "Steps : 208100, \t Total Gen Loss : 28.851999282836914, \t Total Dis Loss : 4.886426904704422e-05\n",
      "Time for epoch 37 is 76.7973279953003 sec\n",
      "Steps : 208200, \t Total Gen Loss : 27.62033462524414, \t Total Dis Loss : 7.665962766623124e-05\n",
      "Steps : 208300, \t Total Gen Loss : 29.346145629882812, \t Total Dis Loss : 1.7893115000333637e-05\n",
      "Steps : 208400, \t Total Gen Loss : 24.978822708129883, \t Total Dis Loss : 0.013596816919744015\n",
      "Steps : 208500, \t Total Gen Loss : 26.631031036376953, \t Total Dis Loss : 6.589521944988519e-05\n",
      "Steps : 208600, \t Total Gen Loss : 28.614791870117188, \t Total Dis Loss : 0.00019122062076348811\n",
      "Steps : 208700, \t Total Gen Loss : 28.164073944091797, \t Total Dis Loss : 7.097081834217533e-05\n",
      "Steps : 208800, \t Total Gen Loss : 26.385967254638672, \t Total Dis Loss : 0.00013294556993059814\n",
      "Steps : 208900, \t Total Gen Loss : 27.958078384399414, \t Total Dis Loss : 3.619252674980089e-05\n",
      "Steps : 209000, \t Total Gen Loss : 26.361995697021484, \t Total Dis Loss : 0.00013830108218826354\n",
      "Steps : 209100, \t Total Gen Loss : 26.689411163330078, \t Total Dis Loss : 0.08188124746084213\n",
      "Steps : 209200, \t Total Gen Loss : 29.911266326904297, \t Total Dis Loss : 8.425558917224407e-05\n",
      "Steps : 209300, \t Total Gen Loss : 26.895465850830078, \t Total Dis Loss : 7.070832361932844e-05\n",
      "Steps : 209400, \t Total Gen Loss : 31.419639587402344, \t Total Dis Loss : 1.3770988516625948e-05\n",
      "Steps : 209500, \t Total Gen Loss : 29.832717895507812, \t Total Dis Loss : 5.010498716728762e-05\n",
      "Steps : 209600, \t Total Gen Loss : 27.766578674316406, \t Total Dis Loss : 0.00017029506852850318\n",
      "Steps : 209700, \t Total Gen Loss : 28.567501068115234, \t Total Dis Loss : 4.637351230485365e-05\n",
      "Steps : 209800, \t Total Gen Loss : 30.519926071166992, \t Total Dis Loss : 0.00025264694704674184\n",
      "Steps : 209900, \t Total Gen Loss : 29.829843521118164, \t Total Dis Loss : 0.0002516784006729722\n",
      "Steps : 210000, \t Total Gen Loss : 26.054336547851562, \t Total Dis Loss : 0.00015875652024988085\n",
      "Steps : 210100, \t Total Gen Loss : 31.137483596801758, \t Total Dis Loss : 2.3762029741192237e-05\n",
      "Steps : 210200, \t Total Gen Loss : 27.678579330444336, \t Total Dis Loss : 4.108325083507225e-05\n",
      "Steps : 210300, \t Total Gen Loss : 25.117816925048828, \t Total Dis Loss : 0.3438771665096283\n",
      "Steps : 210400, \t Total Gen Loss : 26.524600982666016, \t Total Dis Loss : 0.0009718101937323809\n",
      "Steps : 210500, \t Total Gen Loss : 32.707313537597656, \t Total Dis Loss : 0.00010833394480869174\n",
      "Steps : 210600, \t Total Gen Loss : 28.908416748046875, \t Total Dis Loss : 8.46320326672867e-05\n",
      "Steps : 210700, \t Total Gen Loss : 27.597261428833008, \t Total Dis Loss : 6.931566167622805e-05\n",
      "Steps : 210800, \t Total Gen Loss : 29.171611785888672, \t Total Dis Loss : 7.379753515124321e-05\n",
      "Steps : 210900, \t Total Gen Loss : 32.20009231567383, \t Total Dis Loss : 6.908216164447367e-06\n",
      "Steps : 211000, \t Total Gen Loss : 27.24197006225586, \t Total Dis Loss : 0.0003483140026219189\n",
      "Steps : 211100, \t Total Gen Loss : 26.239227294921875, \t Total Dis Loss : 0.00011791459110099822\n",
      "Steps : 211200, \t Total Gen Loss : 27.059661865234375, \t Total Dis Loss : 0.0003573044668883085\n",
      "Steps : 211300, \t Total Gen Loss : 23.89229965209961, \t Total Dis Loss : 0.00024332734756171703\n",
      "Steps : 211400, \t Total Gen Loss : 26.896602630615234, \t Total Dis Loss : 0.00021333948825486004\n",
      "Steps : 211500, \t Total Gen Loss : 30.357872009277344, \t Total Dis Loss : 8.944889486883767e-06\n",
      "Steps : 211600, \t Total Gen Loss : 28.54254913330078, \t Total Dis Loss : 0.0001011380591080524\n",
      "Steps : 211700, \t Total Gen Loss : 30.556621551513672, \t Total Dis Loss : 0.0005483516724780202\n",
      "Steps : 211800, \t Total Gen Loss : 33.777862548828125, \t Total Dis Loss : 9.534759556117933e-06\n",
      "Steps : 211900, \t Total Gen Loss : 25.93758773803711, \t Total Dis Loss : 2.6571762646199204e-05\n",
      "Steps : 212000, \t Total Gen Loss : 28.761943817138672, \t Total Dis Loss : 4.627409725799225e-05\n",
      "Steps : 212100, \t Total Gen Loss : 28.0185604095459, \t Total Dis Loss : 1.0280064088874497e-05\n",
      "Steps : 212200, \t Total Gen Loss : 23.421676635742188, \t Total Dis Loss : 9.004220919450745e-05\n",
      "Steps : 212300, \t Total Gen Loss : 26.96826171875, \t Total Dis Loss : 9.374418732477352e-05\n",
      "Steps : 212400, \t Total Gen Loss : 24.19320297241211, \t Total Dis Loss : 0.0001026135723805055\n",
      "Steps : 212500, \t Total Gen Loss : 27.518253326416016, \t Total Dis Loss : 1.8753116819425486e-05\n",
      "Steps : 212600, \t Total Gen Loss : 27.379215240478516, \t Total Dis Loss : 0.00020210904767736793\n",
      "Steps : 212700, \t Total Gen Loss : 27.274471282958984, \t Total Dis Loss : 5.3742111049359664e-05\n",
      "Steps : 212800, \t Total Gen Loss : 25.603496551513672, \t Total Dis Loss : 0.00010906251554843038\n",
      "Steps : 212900, \t Total Gen Loss : 26.589031219482422, \t Total Dis Loss : 8.416904893238097e-05\n",
      "Steps : 213000, \t Total Gen Loss : 25.954566955566406, \t Total Dis Loss : 4.642398198484443e-05\n",
      "Steps : 213100, \t Total Gen Loss : 24.791793823242188, \t Total Dis Loss : 2.4931337975431234e-05\n",
      "Steps : 213200, \t Total Gen Loss : 25.030277252197266, \t Total Dis Loss : 3.8675945688737556e-05\n",
      "Steps : 213300, \t Total Gen Loss : 28.0657901763916, \t Total Dis Loss : 1.813372909964528e-05\n",
      "Steps : 213400, \t Total Gen Loss : 28.436050415039062, \t Total Dis Loss : 4.0164919482776895e-05\n",
      "Steps : 213500, \t Total Gen Loss : 27.366973876953125, \t Total Dis Loss : 1.6738686099415645e-05\n",
      "Steps : 213600, \t Total Gen Loss : 28.868139266967773, \t Total Dis Loss : 0.0007843127241358161\n",
      "Steps : 213700, \t Total Gen Loss : 32.9635124206543, \t Total Dis Loss : 2.2291910681815352e-06\n",
      "Time for epoch 38 is 74.00084090232849 sec\n",
      "Steps : 213800, \t Total Gen Loss : 30.32274055480957, \t Total Dis Loss : 5.144134775036946e-05\n",
      "Steps : 213900, \t Total Gen Loss : 29.05467414855957, \t Total Dis Loss : 0.0004161967954132706\n",
      "Steps : 214000, \t Total Gen Loss : 28.390533447265625, \t Total Dis Loss : 5.950136255705729e-05\n",
      "Steps : 214100, \t Total Gen Loss : 26.833772659301758, \t Total Dis Loss : 2.166112062695902e-05\n",
      "Steps : 214200, \t Total Gen Loss : 28.396066665649414, \t Total Dis Loss : 1.585400059411768e-05\n",
      "Steps : 214300, \t Total Gen Loss : 29.513912200927734, \t Total Dis Loss : 1.5355475625256076e-05\n",
      "Steps : 214400, \t Total Gen Loss : 27.210391998291016, \t Total Dis Loss : 0.00015104864723980427\n",
      "Steps : 214500, \t Total Gen Loss : 28.482650756835938, \t Total Dis Loss : 1.7019710867316462e-05\n",
      "Steps : 214600, \t Total Gen Loss : 27.23194122314453, \t Total Dis Loss : 0.00012900964065920562\n",
      "Steps : 214700, \t Total Gen Loss : 29.50908660888672, \t Total Dis Loss : 1.5532024917774834e-05\n",
      "Steps : 214800, \t Total Gen Loss : 27.64561653137207, \t Total Dis Loss : 3.872701927321032e-05\n",
      "Steps : 214900, \t Total Gen Loss : 29.679941177368164, \t Total Dis Loss : 8.543417607143056e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 215000, \t Total Gen Loss : 28.385181427001953, \t Total Dis Loss : 2.1744906916865148e-05\n",
      "Steps : 215100, \t Total Gen Loss : 27.45614242553711, \t Total Dis Loss : 9.352073902846314e-06\n",
      "Steps : 215200, \t Total Gen Loss : 28.343551635742188, \t Total Dis Loss : 4.430670742294751e-05\n",
      "Steps : 215300, \t Total Gen Loss : 29.605754852294922, \t Total Dis Loss : 4.171848559053615e-05\n",
      "Steps : 215400, \t Total Gen Loss : 23.445903778076172, \t Total Dis Loss : 0.003490730654448271\n",
      "Steps : 215500, \t Total Gen Loss : 27.41509246826172, \t Total Dis Loss : 0.00016679713735356927\n",
      "Steps : 215600, \t Total Gen Loss : 28.820308685302734, \t Total Dis Loss : 0.00015539002197328955\n",
      "Steps : 215700, \t Total Gen Loss : 26.1893310546875, \t Total Dis Loss : 8.187552157323807e-05\n",
      "Steps : 215800, \t Total Gen Loss : 31.077896118164062, \t Total Dis Loss : 1.0918983207375277e-05\n",
      "Steps : 215900, \t Total Gen Loss : 27.75933837890625, \t Total Dis Loss : 2.0632091036532074e-05\n",
      "Steps : 216000, \t Total Gen Loss : 31.979467391967773, \t Total Dis Loss : 5.783284450444626e-06\n",
      "Steps : 216100, \t Total Gen Loss : 23.673080444335938, \t Total Dis Loss : 0.0029363837093114853\n",
      "Steps : 216200, \t Total Gen Loss : 26.143461227416992, \t Total Dis Loss : 8.348725532414392e-05\n",
      "Steps : 216300, \t Total Gen Loss : 32.99089813232422, \t Total Dis Loss : 0.0001498877682024613\n",
      "Steps : 216400, \t Total Gen Loss : 27.876171112060547, \t Total Dis Loss : 2.078274337691255e-05\n",
      "Steps : 216500, \t Total Gen Loss : 29.907588958740234, \t Total Dis Loss : 0.0023362988140434027\n",
      "Steps : 216600, \t Total Gen Loss : 23.399871826171875, \t Total Dis Loss : 0.0013306466862559319\n",
      "Steps : 216700, \t Total Gen Loss : 26.2105770111084, \t Total Dis Loss : 0.0006697572534903884\n",
      "Steps : 216800, \t Total Gen Loss : 24.989620208740234, \t Total Dis Loss : 4.172431363258511e-05\n",
      "Steps : 216900, \t Total Gen Loss : 28.99863624572754, \t Total Dis Loss : 5.596450864686631e-05\n",
      "Steps : 217000, \t Total Gen Loss : 29.783897399902344, \t Total Dis Loss : 7.276679389178753e-05\n",
      "Steps : 217100, \t Total Gen Loss : 26.693906784057617, \t Total Dis Loss : 0.00032068483415059745\n",
      "Steps : 217200, \t Total Gen Loss : 26.227313995361328, \t Total Dis Loss : 0.00011548352631507441\n",
      "Steps : 217300, \t Total Gen Loss : 26.278179168701172, \t Total Dis Loss : 2.4002913050935604e-05\n",
      "Steps : 217400, \t Total Gen Loss : 26.86547088623047, \t Total Dis Loss : 6.912052776897326e-05\n",
      "Steps : 217500, \t Total Gen Loss : 28.920642852783203, \t Total Dis Loss : 4.351067400421016e-05\n",
      "Steps : 217600, \t Total Gen Loss : 26.543209075927734, \t Total Dis Loss : 1.9299615814816207e-05\n",
      "Steps : 217700, \t Total Gen Loss : 23.542789459228516, \t Total Dis Loss : 0.0003085750504396856\n",
      "Steps : 217800, \t Total Gen Loss : 24.620695114135742, \t Total Dis Loss : 0.0001644748990656808\n",
      "Steps : 217900, \t Total Gen Loss : 28.046506881713867, \t Total Dis Loss : 3.3212036214536056e-05\n",
      "Steps : 218000, \t Total Gen Loss : 28.995590209960938, \t Total Dis Loss : 5.689902536687441e-05\n",
      "Steps : 218100, \t Total Gen Loss : 29.050983428955078, \t Total Dis Loss : 2.6595551389618777e-05\n",
      "Steps : 218200, \t Total Gen Loss : 28.119213104248047, \t Total Dis Loss : 1.8289361833012663e-05\n",
      "Steps : 218300, \t Total Gen Loss : 25.1748046875, \t Total Dis Loss : 5.048599268775433e-05\n",
      "Steps : 218400, \t Total Gen Loss : 28.194698333740234, \t Total Dis Loss : 2.032941483776085e-05\n",
      "Steps : 218500, \t Total Gen Loss : 26.011554718017578, \t Total Dis Loss : 0.00013226267765276134\n",
      "Steps : 218600, \t Total Gen Loss : 31.94153594970703, \t Total Dis Loss : 0.0004021715431008488\n",
      "Steps : 218700, \t Total Gen Loss : 30.277013778686523, \t Total Dis Loss : 8.030372555367649e-05\n",
      "Steps : 218800, \t Total Gen Loss : 30.904563903808594, \t Total Dis Loss : 1.260335739061702e-05\n",
      "Steps : 218900, \t Total Gen Loss : 30.943527221679688, \t Total Dis Loss : 1.3025795851717703e-05\n",
      "Steps : 219000, \t Total Gen Loss : 31.69554328918457, \t Total Dis Loss : 1.6049503756221384e-05\n",
      "Steps : 219100, \t Total Gen Loss : 28.78227424621582, \t Total Dis Loss : 7.846677362977061e-06\n",
      "Steps : 219200, \t Total Gen Loss : 26.362218856811523, \t Total Dis Loss : 2.31892063311534e-05\n",
      "Steps : 219300, \t Total Gen Loss : 31.103275299072266, \t Total Dis Loss : 7.003298833296867e-06\n",
      "Time for epoch 39 is 73.91662812232971 sec\n",
      "Steps : 219400, \t Total Gen Loss : 27.22930335998535, \t Total Dis Loss : 1.8313734472030774e-05\n",
      "Steps : 219500, \t Total Gen Loss : 30.707019805908203, \t Total Dis Loss : 0.00015248649287968874\n",
      "Steps : 219600, \t Total Gen Loss : 27.526264190673828, \t Total Dis Loss : 3.958425440941937e-05\n",
      "Steps : 219700, \t Total Gen Loss : 31.40827178955078, \t Total Dis Loss : 3.546859443304129e-05\n",
      "Steps : 219800, \t Total Gen Loss : 24.53464126586914, \t Total Dis Loss : 0.00012711489398498088\n",
      "Steps : 219900, \t Total Gen Loss : 29.033084869384766, \t Total Dis Loss : 3.11929288727697e-05\n",
      "Steps : 220000, \t Total Gen Loss : 33.01529312133789, \t Total Dis Loss : 4.063152300659567e-05\n",
      "Steps : 220100, \t Total Gen Loss : 26.182388305664062, \t Total Dis Loss : 0.00015336234355345368\n",
      "Steps : 220200, \t Total Gen Loss : 28.520633697509766, \t Total Dis Loss : 3.5816301533486694e-05\n",
      "Steps : 220300, \t Total Gen Loss : 27.56855583190918, \t Total Dis Loss : 1.3337565178517252e-05\n",
      "Steps : 220400, \t Total Gen Loss : 26.566757202148438, \t Total Dis Loss : 1.4219813238014467e-05\n",
      "Steps : 220500, \t Total Gen Loss : 34.5477294921875, \t Total Dis Loss : 7.7261956903385e-06\n",
      "Steps : 220600, \t Total Gen Loss : 28.127729415893555, \t Total Dis Loss : 8.726682608539704e-06\n",
      "Steps : 220700, \t Total Gen Loss : 28.815675735473633, \t Total Dis Loss : 7.171316610765643e-06\n",
      "Steps : 220800, \t Total Gen Loss : 31.265764236450195, \t Total Dis Loss : 8.674227865412831e-05\n",
      "Steps : 220900, \t Total Gen Loss : 27.777923583984375, \t Total Dis Loss : 7.780831219861284e-05\n",
      "Steps : 221000, \t Total Gen Loss : 30.59040069580078, \t Total Dis Loss : 1.926945287777926e-06\n",
      "Steps : 221100, \t Total Gen Loss : 32.630855560302734, \t Total Dis Loss : 3.5932762330048718e-06\n",
      "Steps : 221200, \t Total Gen Loss : 28.239734649658203, \t Total Dis Loss : 1.7018965081661008e-05\n",
      "Steps : 221300, \t Total Gen Loss : 35.850135803222656, \t Total Dis Loss : 8.393644748139195e-06\n",
      "Steps : 221400, \t Total Gen Loss : 31.249439239501953, \t Total Dis Loss : 6.132559065008536e-05\n",
      "Steps : 221500, \t Total Gen Loss : 28.20447540283203, \t Total Dis Loss : 1.5545410860795528e-05\n",
      "Steps : 221600, \t Total Gen Loss : 30.93547248840332, \t Total Dis Loss : 2.1966503481962718e-05\n",
      "Steps : 221700, \t Total Gen Loss : 31.939517974853516, \t Total Dis Loss : 5.575953400693834e-05\n",
      "Steps : 221800, \t Total Gen Loss : 30.300357818603516, \t Total Dis Loss : 2.8588929126271978e-05\n",
      "Steps : 221900, \t Total Gen Loss : 26.78472328186035, \t Total Dis Loss : 0.0001789198286132887\n",
      "Steps : 222000, \t Total Gen Loss : 26.015926361083984, \t Total Dis Loss : 0.0016097599873319268\n",
      "Steps : 222100, \t Total Gen Loss : 27.23556137084961, \t Total Dis Loss : 0.0006991855916567147\n",
      "Steps : 222200, \t Total Gen Loss : 30.79894256591797, \t Total Dis Loss : 2.7195570510230027e-05\n",
      "Steps : 222300, \t Total Gen Loss : 25.862407684326172, \t Total Dis Loss : 6.793910142732784e-05\n",
      "Steps : 222400, \t Total Gen Loss : 30.01899528503418, \t Total Dis Loss : 2.4330981887032976e-06\n",
      "Steps : 222500, \t Total Gen Loss : 29.153852462768555, \t Total Dis Loss : 5.43456117156893e-05\n",
      "Steps : 222600, \t Total Gen Loss : 28.85009765625, \t Total Dis Loss : 7.422042835969478e-05\n",
      "Steps : 222700, \t Total Gen Loss : 24.06645965576172, \t Total Dis Loss : 0.00011916264338651672\n",
      "Steps : 222800, \t Total Gen Loss : 25.68915557861328, \t Total Dis Loss : 0.00011728609388228506\n",
      "Steps : 222900, \t Total Gen Loss : 26.131507873535156, \t Total Dis Loss : 1.8346185242990032e-05\n",
      "Steps : 223000, \t Total Gen Loss : 30.43982696533203, \t Total Dis Loss : 4.38611687059165e-06\n",
      "Steps : 223100, \t Total Gen Loss : 30.485593795776367, \t Total Dis Loss : 3.0365592920134077e-06\n",
      "Steps : 223200, \t Total Gen Loss : 31.465885162353516, \t Total Dis Loss : 2.51819415097998e-06\n",
      "Steps : 223300, \t Total Gen Loss : 27.763973236083984, \t Total Dis Loss : 7.32073340259376e-06\n",
      "Steps : 223400, \t Total Gen Loss : 28.181068420410156, \t Total Dis Loss : 0.00024278277123812586\n",
      "Steps : 223500, \t Total Gen Loss : 31.852333068847656, \t Total Dis Loss : 0.00015338478260673583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 223600, \t Total Gen Loss : 31.429241180419922, \t Total Dis Loss : 3.8818525354145095e-06\n",
      "Steps : 223700, \t Total Gen Loss : 31.58095932006836, \t Total Dis Loss : 4.919114871881902e-06\n",
      "Steps : 223800, \t Total Gen Loss : 29.856090545654297, \t Total Dis Loss : 1.2843057447753381e-05\n",
      "Steps : 223900, \t Total Gen Loss : 27.563575744628906, \t Total Dis Loss : 0.0002562719164416194\n",
      "Steps : 224000, \t Total Gen Loss : 33.691123962402344, \t Total Dis Loss : 5.038741437601857e-05\n",
      "Steps : 224100, \t Total Gen Loss : 29.285688400268555, \t Total Dis Loss : 2.9661503504030406e-05\n",
      "Steps : 224200, \t Total Gen Loss : 26.830886840820312, \t Total Dis Loss : 2.6133104256587103e-05\n",
      "Steps : 224300, \t Total Gen Loss : 28.89905548095703, \t Total Dis Loss : 4.183179771644063e-05\n",
      "Steps : 224400, \t Total Gen Loss : 28.06057357788086, \t Total Dis Loss : 3.866018232656643e-05\n",
      "Steps : 224500, \t Total Gen Loss : 28.218854904174805, \t Total Dis Loss : 0.00013372873945627362\n",
      "Steps : 224600, \t Total Gen Loss : 25.551088333129883, \t Total Dis Loss : 0.0001861327764345333\n",
      "Steps : 224700, \t Total Gen Loss : 29.552873611450195, \t Total Dis Loss : 5.927002348471433e-05\n",
      "Steps : 224800, \t Total Gen Loss : 27.36224365234375, \t Total Dis Loss : 5.073919965070672e-05\n",
      "Steps : 224900, \t Total Gen Loss : 27.62788963317871, \t Total Dis Loss : 9.346871593152173e-06\n",
      "Steps : 225000, \t Total Gen Loss : 26.09516143798828, \t Total Dis Loss : 0.00029981567058712244\n",
      "Time for epoch 40 is 73.96848750114441 sec\n",
      "Steps : 225100, \t Total Gen Loss : 29.853256225585938, \t Total Dis Loss : 2.356530058023054e-05\n",
      "Steps : 225200, \t Total Gen Loss : 29.52953338623047, \t Total Dis Loss : 9.778962521522772e-06\n",
      "Steps : 225300, \t Total Gen Loss : 24.57428741455078, \t Total Dis Loss : 0.00019205818534828722\n",
      "Steps : 225400, \t Total Gen Loss : 29.789600372314453, \t Total Dis Loss : 4.4718075514538214e-05\n",
      "Steps : 225500, \t Total Gen Loss : 26.152099609375, \t Total Dis Loss : 0.0001319680013693869\n",
      "Steps : 225600, \t Total Gen Loss : 24.75516128540039, \t Total Dis Loss : 0.00013053299335297197\n",
      "Steps : 225700, \t Total Gen Loss : 28.61628532409668, \t Total Dis Loss : 1.5227259609673638e-05\n",
      "Steps : 225800, \t Total Gen Loss : 30.682071685791016, \t Total Dis Loss : 5.723334652429912e-06\n",
      "Steps : 225900, \t Total Gen Loss : 31.95266342163086, \t Total Dis Loss : 1.8194250515080057e-05\n",
      "Steps : 226000, \t Total Gen Loss : 33.784027099609375, \t Total Dis Loss : 3.4692857298068702e-06\n",
      "Steps : 226100, \t Total Gen Loss : 26.074783325195312, \t Total Dis Loss : 4.346058267401531e-05\n",
      "Steps : 226200, \t Total Gen Loss : 30.22449493408203, \t Total Dis Loss : 4.441083365236409e-05\n",
      "Steps : 226300, \t Total Gen Loss : 29.456069946289062, \t Total Dis Loss : 0.00015231141878757626\n",
      "Steps : 226400, \t Total Gen Loss : 29.19482421875, \t Total Dis Loss : 9.035118637257256e-06\n",
      "Steps : 226500, \t Total Gen Loss : 27.949195861816406, \t Total Dis Loss : 2.0086064978386275e-05\n",
      "Steps : 226600, \t Total Gen Loss : 28.647945404052734, \t Total Dis Loss : 0.001541060279123485\n",
      "Steps : 226700, \t Total Gen Loss : 28.749479293823242, \t Total Dis Loss : 4.080264989170246e-05\n",
      "Steps : 226800, \t Total Gen Loss : 27.364477157592773, \t Total Dis Loss : 0.00010526344703976065\n",
      "Steps : 226900, \t Total Gen Loss : 32.51127243041992, \t Total Dis Loss : 2.8838333037128905e-06\n",
      "Steps : 227000, \t Total Gen Loss : 32.415748596191406, \t Total Dis Loss : 8.958985745266546e-06\n",
      "Steps : 227100, \t Total Gen Loss : 26.03810691833496, \t Total Dis Loss : 0.0009049340733326972\n",
      "Steps : 227200, \t Total Gen Loss : 30.598960876464844, \t Total Dis Loss : 7.944870048959274e-06\n",
      "Steps : 227300, \t Total Gen Loss : 33.03710174560547, \t Total Dis Loss : 8.748457912588492e-05\n",
      "Steps : 227400, \t Total Gen Loss : 33.24712371826172, \t Total Dis Loss : 7.0971418608678505e-06\n",
      "Steps : 227500, \t Total Gen Loss : 31.916105270385742, \t Total Dis Loss : 0.001393582089804113\n",
      "Steps : 227600, \t Total Gen Loss : 35.32371520996094, \t Total Dis Loss : 3.335502469781204e-06\n",
      "Steps : 227700, \t Total Gen Loss : 32.99029541015625, \t Total Dis Loss : 2.4361579562537372e-05\n",
      "Steps : 227800, \t Total Gen Loss : 34.34805679321289, \t Total Dis Loss : 0.00013301729632075876\n",
      "Steps : 227900, \t Total Gen Loss : 29.8337345123291, \t Total Dis Loss : 6.792438216507435e-05\n",
      "Steps : 228000, \t Total Gen Loss : 40.3961181640625, \t Total Dis Loss : 0.00021725293481722474\n",
      "Steps : 228100, \t Total Gen Loss : 40.37213134765625, \t Total Dis Loss : 0.000130961969261989\n",
      "Steps : 228200, \t Total Gen Loss : 42.83623123168945, \t Total Dis Loss : 0.0002566165931057185\n",
      "Steps : 228300, \t Total Gen Loss : 35.590003967285156, \t Total Dis Loss : 2.1508396457647905e-05\n",
      "Steps : 228400, \t Total Gen Loss : 35.527442932128906, \t Total Dis Loss : 6.120232137618586e-05\n",
      "Steps : 228500, \t Total Gen Loss : 33.04432678222656, \t Total Dis Loss : 1.159102794190403e-06\n",
      "Steps : 228600, \t Total Gen Loss : 31.60971450805664, \t Total Dis Loss : 4.042404270876432e-06\n",
      "Steps : 228700, \t Total Gen Loss : 37.20343017578125, \t Total Dis Loss : 4.089986305189086e-06\n",
      "Steps : 228800, \t Total Gen Loss : 35.468284606933594, \t Total Dis Loss : 6.115341420809273e-06\n",
      "Steps : 228900, \t Total Gen Loss : 38.591007232666016, \t Total Dis Loss : 1.4461281352851074e-05\n",
      "Steps : 229000, \t Total Gen Loss : 31.80510711669922, \t Total Dis Loss : 5.913332279305905e-05\n",
      "Steps : 229100, \t Total Gen Loss : 33.89830780029297, \t Total Dis Loss : 1.7625194232095964e-05\n",
      "Steps : 229200, \t Total Gen Loss : 33.44965744018555, \t Total Dis Loss : 2.292030148964841e-05\n",
      "Steps : 229300, \t Total Gen Loss : 29.841934204101562, \t Total Dis Loss : 2.1788493540952913e-05\n",
      "Steps : 229400, \t Total Gen Loss : 36.12913131713867, \t Total Dis Loss : 8.879495908331592e-06\n",
      "Steps : 229500, \t Total Gen Loss : 32.107391357421875, \t Total Dis Loss : 2.1944200852885842e-05\n",
      "Steps : 229600, \t Total Gen Loss : 34.189964294433594, \t Total Dis Loss : 3.8231464714044705e-05\n",
      "Steps : 229700, \t Total Gen Loss : 32.94633483886719, \t Total Dis Loss : 4.741318207379663e-06\n",
      "Steps : 229800, \t Total Gen Loss : 29.413589477539062, \t Total Dis Loss : 4.2992207454517484e-05\n",
      "Steps : 229900, \t Total Gen Loss : 29.802330017089844, \t Total Dis Loss : 1.3686436659554602e-06\n",
      "Steps : 230000, \t Total Gen Loss : 28.52263832092285, \t Total Dis Loss : 3.0265246095950715e-06\n",
      "Steps : 230100, \t Total Gen Loss : 30.005041122436523, \t Total Dis Loss : 5.267736196401529e-05\n",
      "Steps : 230200, \t Total Gen Loss : 30.662961959838867, \t Total Dis Loss : 6.126263178884983e-05\n",
      "Steps : 230300, \t Total Gen Loss : 27.175514221191406, \t Total Dis Loss : 0.00015328187146224082\n",
      "Steps : 230400, \t Total Gen Loss : 32.986717224121094, \t Total Dis Loss : 0.0002677622251212597\n",
      "Steps : 230500, \t Total Gen Loss : 30.916879653930664, \t Total Dis Loss : 0.0001729789946693927\n",
      "Steps : 230600, \t Total Gen Loss : 31.658405303955078, \t Total Dis Loss : 1.8792401533573866e-05\n",
      "Time for epoch 41 is 73.84818124771118 sec\n",
      "Steps : 230700, \t Total Gen Loss : 33.451194763183594, \t Total Dis Loss : 3.703616312122904e-05\n",
      "Steps : 230800, \t Total Gen Loss : 28.870216369628906, \t Total Dis Loss : 6.404663872672245e-05\n",
      "Steps : 230900, \t Total Gen Loss : 26.454078674316406, \t Total Dis Loss : 0.0005480422405526042\n",
      "Steps : 231000, \t Total Gen Loss : 32.21752166748047, \t Total Dis Loss : 3.004703830811195e-05\n",
      "Steps : 231100, \t Total Gen Loss : 30.307329177856445, \t Total Dis Loss : 1.7797807231545448e-05\n",
      "Steps : 231200, \t Total Gen Loss : 27.640125274658203, \t Total Dis Loss : 5.5849839554866776e-05\n",
      "Steps : 231300, \t Total Gen Loss : 32.55476379394531, \t Total Dis Loss : 4.915044701192528e-05\n",
      "Steps : 231400, \t Total Gen Loss : 29.820655822753906, \t Total Dis Loss : 7.240284321596846e-05\n",
      "Steps : 231500, \t Total Gen Loss : 31.511882781982422, \t Total Dis Loss : 0.000112017965875566\n",
      "Steps : 231600, \t Total Gen Loss : 31.159929275512695, \t Total Dis Loss : 0.0005947879399172962\n",
      "Steps : 231700, \t Total Gen Loss : 30.16271209716797, \t Total Dis Loss : 2.9409720809780993e-05\n",
      "Steps : 231800, \t Total Gen Loss : 30.671138763427734, \t Total Dis Loss : 2.261594454466831e-05\n",
      "Steps : 231900, \t Total Gen Loss : 30.77680206298828, \t Total Dis Loss : 0.0002431282337056473\n",
      "Steps : 232000, \t Total Gen Loss : 27.425521850585938, \t Total Dis Loss : 0.0001569881133036688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 232100, \t Total Gen Loss : 30.20763397216797, \t Total Dis Loss : 0.00028429203666746616\n",
      "Steps : 232200, \t Total Gen Loss : 29.318748474121094, \t Total Dis Loss : 0.00023613999655935913\n",
      "Steps : 232300, \t Total Gen Loss : 30.509746551513672, \t Total Dis Loss : 3.6116282444709213e-06\n",
      "Steps : 232400, \t Total Gen Loss : 29.962221145629883, \t Total Dis Loss : 3.6947025364497676e-05\n",
      "Steps : 232500, \t Total Gen Loss : 28.31063461303711, \t Total Dis Loss : 1.1775438906624913e-05\n",
      "Steps : 232600, \t Total Gen Loss : 31.295339584350586, \t Total Dis Loss : 4.3670130253303796e-05\n",
      "Steps : 232700, \t Total Gen Loss : 30.947246551513672, \t Total Dis Loss : 6.0127858887426555e-06\n",
      "Steps : 232800, \t Total Gen Loss : 31.532875061035156, \t Total Dis Loss : 3.600782292778604e-05\n",
      "Steps : 232900, \t Total Gen Loss : 28.903961181640625, \t Total Dis Loss : 3.4797190892277285e-05\n",
      "Steps : 233000, \t Total Gen Loss : 31.97801971435547, \t Total Dis Loss : 5.101566785015166e-06\n",
      "Steps : 233100, \t Total Gen Loss : 29.16064453125, \t Total Dis Loss : 2.2416706997319125e-05\n",
      "Steps : 233200, \t Total Gen Loss : 30.94382667541504, \t Total Dis Loss : 3.559883771231398e-05\n",
      "Steps : 233300, \t Total Gen Loss : 28.20421028137207, \t Total Dis Loss : 4.3125790398335084e-05\n",
      "Steps : 233400, \t Total Gen Loss : 28.626018524169922, \t Total Dis Loss : 2.0727055016322993e-05\n",
      "Steps : 233500, \t Total Gen Loss : 31.04581642150879, \t Total Dis Loss : 2.488353129592724e-05\n",
      "Steps : 233600, \t Total Gen Loss : 28.605716705322266, \t Total Dis Loss : 3.0629566026618704e-05\n",
      "Steps : 233700, \t Total Gen Loss : 29.013442993164062, \t Total Dis Loss : 2.759145718300715e-05\n",
      "Steps : 233800, \t Total Gen Loss : 29.82692527770996, \t Total Dis Loss : 1.7003152606775984e-05\n",
      "Steps : 233900, \t Total Gen Loss : 28.130495071411133, \t Total Dis Loss : 1.4769179870199878e-05\n",
      "Steps : 234000, \t Total Gen Loss : 31.149822235107422, \t Total Dis Loss : 2.6955714929499663e-05\n",
      "Steps : 234100, \t Total Gen Loss : 31.02065658569336, \t Total Dis Loss : 0.00031941337510943413\n",
      "Steps : 234200, \t Total Gen Loss : 29.73711395263672, \t Total Dis Loss : 8.104390144580975e-05\n",
      "Steps : 234300, \t Total Gen Loss : 34.08181381225586, \t Total Dis Loss : 1.7809885321184993e-05\n",
      "Steps : 234400, \t Total Gen Loss : 26.60601806640625, \t Total Dis Loss : 8.670578245073557e-05\n",
      "Steps : 234500, \t Total Gen Loss : 26.830074310302734, \t Total Dis Loss : 4.967689164914191e-05\n",
      "Steps : 234600, \t Total Gen Loss : 32.156494140625, \t Total Dis Loss : 2.9093739613017533e-06\n",
      "Steps : 234700, \t Total Gen Loss : 29.0672607421875, \t Total Dis Loss : 3.383465082151815e-05\n",
      "Steps : 234800, \t Total Gen Loss : 27.743892669677734, \t Total Dis Loss : 0.00010243397991871461\n",
      "Steps : 234900, \t Total Gen Loss : 28.28194236755371, \t Total Dis Loss : 1.5043662642710842e-05\n",
      "Steps : 235000, \t Total Gen Loss : 27.304895401000977, \t Total Dis Loss : 1.4487648513750173e-05\n",
      "Steps : 235100, \t Total Gen Loss : 33.91018295288086, \t Total Dis Loss : 1.3671335182152689e-05\n",
      "Steps : 235200, \t Total Gen Loss : 31.03052520751953, \t Total Dis Loss : 1.1721574992407113e-05\n",
      "Steps : 235300, \t Total Gen Loss : 28.38477325439453, \t Total Dis Loss : 2.4606144506833516e-05\n",
      "Steps : 235400, \t Total Gen Loss : 27.259292602539062, \t Total Dis Loss : 3.053705586353317e-05\n",
      "Steps : 235500, \t Total Gen Loss : 33.75322723388672, \t Total Dis Loss : 5.800385679322062e-06\n",
      "Steps : 235600, \t Total Gen Loss : 27.6497745513916, \t Total Dis Loss : 0.0001579972158651799\n",
      "Steps : 235700, \t Total Gen Loss : 25.8990535736084, \t Total Dis Loss : 3.34239230141975e-05\n",
      "Steps : 235800, \t Total Gen Loss : 28.82349395751953, \t Total Dis Loss : 2.7268917619949207e-05\n",
      "Steps : 235900, \t Total Gen Loss : 25.009485244750977, \t Total Dis Loss : 2.6683157557272352e-05\n",
      "Steps : 236000, \t Total Gen Loss : 30.3255558013916, \t Total Dis Loss : 1.2215153219585773e-05\n",
      "Steps : 236100, \t Total Gen Loss : 28.02554702758789, \t Total Dis Loss : 2.8564505555550568e-05\n",
      "Steps : 236200, \t Total Gen Loss : 28.910926818847656, \t Total Dis Loss : 1.5026393157313578e-05\n",
      "Time for epoch 42 is 74.10917520523071 sec\n",
      "Steps : 236300, \t Total Gen Loss : 26.88193130493164, \t Total Dis Loss : 7.613698835484684e-05\n",
      "Steps : 236400, \t Total Gen Loss : 27.663169860839844, \t Total Dis Loss : 1.882713877421338e-05\n",
      "Steps : 236500, \t Total Gen Loss : 26.159570693969727, \t Total Dis Loss : 2.2161175365909003e-05\n",
      "Steps : 236600, \t Total Gen Loss : 28.888521194458008, \t Total Dis Loss : 1.3502159163181204e-05\n",
      "Steps : 236700, \t Total Gen Loss : 26.642108917236328, \t Total Dis Loss : 0.00013779221626464278\n",
      "Steps : 236800, \t Total Gen Loss : 26.58673858642578, \t Total Dis Loss : 0.0002225233183708042\n",
      "Steps : 236900, \t Total Gen Loss : 24.591094970703125, \t Total Dis Loss : 7.602843834320083e-05\n",
      "Steps : 237000, \t Total Gen Loss : 29.700496673583984, \t Total Dis Loss : 3.6932189686922356e-05\n",
      "Steps : 237100, \t Total Gen Loss : 25.97531509399414, \t Total Dis Loss : 0.00010619216482155025\n",
      "Steps : 237200, \t Total Gen Loss : 26.083471298217773, \t Total Dis Loss : 0.00010141827078768983\n",
      "Steps : 237300, \t Total Gen Loss : 25.37152862548828, \t Total Dis Loss : 5.859982411493547e-05\n",
      "Steps : 237400, \t Total Gen Loss : 27.79067611694336, \t Total Dis Loss : 8.196426642825827e-05\n",
      "Steps : 237500, \t Total Gen Loss : 28.157432556152344, \t Total Dis Loss : 1.6655052604619414e-05\n",
      "Steps : 237600, \t Total Gen Loss : 26.759563446044922, \t Total Dis Loss : 1.8973461919813417e-05\n",
      "Steps : 237700, \t Total Gen Loss : 29.371097564697266, \t Total Dis Loss : 1.3173816114431247e-05\n",
      "Steps : 237800, \t Total Gen Loss : 23.685768127441406, \t Total Dis Loss : 0.012169989757239819\n",
      "Steps : 237900, \t Total Gen Loss : 27.057353973388672, \t Total Dis Loss : 6.632539589190856e-05\n",
      "Steps : 238000, \t Total Gen Loss : 26.35916519165039, \t Total Dis Loss : 4.800713213626295e-05\n",
      "Steps : 238100, \t Total Gen Loss : 25.47928237915039, \t Total Dis Loss : 0.011203269474208355\n",
      "Steps : 238200, \t Total Gen Loss : 27.379344940185547, \t Total Dis Loss : 5.51574194105342e-05\n",
      "Steps : 238300, \t Total Gen Loss : 26.957481384277344, \t Total Dis Loss : 8.069910836638883e-05\n",
      "Steps : 238400, \t Total Gen Loss : 28.546875, \t Total Dis Loss : 4.220935079501942e-05\n",
      "Steps : 238500, \t Total Gen Loss : 29.07955551147461, \t Total Dis Loss : 1.5085760423971806e-05\n",
      "Steps : 238600, \t Total Gen Loss : 30.693336486816406, \t Total Dis Loss : 1.6385189155698754e-05\n",
      "Steps : 238700, \t Total Gen Loss : 29.162412643432617, \t Total Dis Loss : 1.236577099916758e-05\n",
      "Steps : 238800, \t Total Gen Loss : 26.201520919799805, \t Total Dis Loss : 1.958659959200304e-05\n",
      "Steps : 238900, \t Total Gen Loss : 28.53471565246582, \t Total Dis Loss : 7.40502309781732e-06\n",
      "Steps : 239000, \t Total Gen Loss : 27.92266082763672, \t Total Dis Loss : 8.465229257126339e-06\n",
      "Steps : 239100, \t Total Gen Loss : 26.590862274169922, \t Total Dis Loss : 7.199376341304742e-06\n",
      "Steps : 239200, \t Total Gen Loss : 29.428865432739258, \t Total Dis Loss : 8.8897086243378e-06\n",
      "Steps : 239300, \t Total Gen Loss : 26.748062133789062, \t Total Dis Loss : 6.183728692121804e-05\n",
      "Steps : 239400, \t Total Gen Loss : 28.05207061767578, \t Total Dis Loss : 0.0002810281002894044\n",
      "Steps : 239500, \t Total Gen Loss : 29.934070587158203, \t Total Dis Loss : 9.005600986711215e-06\n",
      "Steps : 239600, \t Total Gen Loss : 34.32720947265625, \t Total Dis Loss : 2.992313966387883e-06\n",
      "Steps : 239700, \t Total Gen Loss : 34.87689971923828, \t Total Dis Loss : 2.8276992907194654e-06\n",
      "Steps : 239800, \t Total Gen Loss : 32.967655181884766, \t Total Dis Loss : 2.648526105986093e-06\n",
      "Steps : 239900, \t Total Gen Loss : 34.18349838256836, \t Total Dis Loss : 2.1459145500557497e-06\n",
      "Steps : 240000, \t Total Gen Loss : 33.08189392089844, \t Total Dis Loss : 1.5083435755514074e-05\n",
      "Steps : 240100, \t Total Gen Loss : 32.57889175415039, \t Total Dis Loss : 0.00014702639600727707\n",
      "Steps : 240200, \t Total Gen Loss : 32.962486267089844, \t Total Dis Loss : 2.1784158889204264e-05\n",
      "Steps : 240300, \t Total Gen Loss : 31.764930725097656, \t Total Dis Loss : 0.00021307452698238194\n",
      "Steps : 240400, \t Total Gen Loss : 31.582046508789062, \t Total Dis Loss : 7.398028537863865e-05\n",
      "Steps : 240500, \t Total Gen Loss : 30.990581512451172, \t Total Dis Loss : 3.1083563953870907e-06\n",
      "Steps : 240600, \t Total Gen Loss : 30.8958797454834, \t Total Dis Loss : 1.113254256779328e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 240700, \t Total Gen Loss : 26.800018310546875, \t Total Dis Loss : 3.8018122722860426e-05\n",
      "Steps : 240800, \t Total Gen Loss : 30.44131088256836, \t Total Dis Loss : 2.178171052946709e-05\n",
      "Steps : 240900, \t Total Gen Loss : 28.481233596801758, \t Total Dis Loss : 7.572526556032244e-06\n",
      "Steps : 241000, \t Total Gen Loss : 30.332813262939453, \t Total Dis Loss : 3.943174669984728e-05\n",
      "Steps : 241100, \t Total Gen Loss : 27.562936782836914, \t Total Dis Loss : 4.070072100148536e-05\n",
      "Steps : 241200, \t Total Gen Loss : 27.7910099029541, \t Total Dis Loss : 8.761324352235533e-06\n",
      "Steps : 241300, \t Total Gen Loss : 29.188648223876953, \t Total Dis Loss : 3.995396036771126e-05\n",
      "Steps : 241400, \t Total Gen Loss : 30.127052307128906, \t Total Dis Loss : 6.061396561563015e-06\n",
      "Steps : 241500, \t Total Gen Loss : 29.49842071533203, \t Total Dis Loss : 6.192877481225878e-05\n",
      "Steps : 241600, \t Total Gen Loss : 28.45060157775879, \t Total Dis Loss : 2.285789196321275e-05\n",
      "Steps : 241700, \t Total Gen Loss : 26.317649841308594, \t Total Dis Loss : 0.00018275497131980956\n",
      "Steps : 241800, \t Total Gen Loss : 32.20875930786133, \t Total Dis Loss : 3.28176201946917e-06\n",
      "Time for epoch 43 is 73.78151392936707 sec\n",
      "Steps : 241900, \t Total Gen Loss : 31.423484802246094, \t Total Dis Loss : 4.9477957873023115e-06\n",
      "Steps : 242000, \t Total Gen Loss : 30.75870132446289, \t Total Dis Loss : 2.4633718567201868e-05\n",
      "Steps : 242100, \t Total Gen Loss : 31.082088470458984, \t Total Dis Loss : 2.2343483578879386e-05\n",
      "Steps : 242200, \t Total Gen Loss : 35.635684967041016, \t Total Dis Loss : 3.694254337460734e-05\n",
      "Steps : 242300, \t Total Gen Loss : 40.98551940917969, \t Total Dis Loss : 0.00015522305329795927\n",
      "Steps : 242400, \t Total Gen Loss : 31.41877555847168, \t Total Dis Loss : 0.009388814680278301\n",
      "Steps : 242500, \t Total Gen Loss : 40.01294708251953, \t Total Dis Loss : 0.00010765294427983463\n",
      "Steps : 242600, \t Total Gen Loss : 34.72324752807617, \t Total Dis Loss : 0.00029608875047415495\n",
      "Steps : 242700, \t Total Gen Loss : 30.99658203125, \t Total Dis Loss : 0.00018885298050008714\n",
      "Steps : 242800, \t Total Gen Loss : 33.41473388671875, \t Total Dis Loss : 0.0011277887970209122\n",
      "Steps : 242900, \t Total Gen Loss : 33.602272033691406, \t Total Dis Loss : 0.0026657558046281338\n",
      "Steps : 243000, \t Total Gen Loss : 37.822265625, \t Total Dis Loss : 0.001063962816260755\n",
      "Steps : 243100, \t Total Gen Loss : 34.86791229248047, \t Total Dis Loss : 0.0002390127192484215\n",
      "Steps : 243200, \t Total Gen Loss : 34.99390411376953, \t Total Dis Loss : 5.2976134611526504e-05\n",
      "Steps : 243300, \t Total Gen Loss : 32.97599411010742, \t Total Dis Loss : 5.663110641762614e-05\n",
      "Steps : 243400, \t Total Gen Loss : 33.449859619140625, \t Total Dis Loss : 0.00048594558029435575\n",
      "Steps : 243500, \t Total Gen Loss : 33.53300094604492, \t Total Dis Loss : 0.3085731863975525\n",
      "Steps : 243600, \t Total Gen Loss : 35.87347412109375, \t Total Dis Loss : 0.0006851775106042624\n",
      "Steps : 243700, \t Total Gen Loss : 35.20079040527344, \t Total Dis Loss : 0.00015764111594762653\n",
      "Steps : 243800, \t Total Gen Loss : 29.233470916748047, \t Total Dis Loss : 0.0003099686000496149\n",
      "Steps : 243900, \t Total Gen Loss : 32.55935287475586, \t Total Dis Loss : 0.0002512198407202959\n",
      "Steps : 244000, \t Total Gen Loss : 29.34711456298828, \t Total Dis Loss : 0.00037985440576449037\n",
      "Steps : 244100, \t Total Gen Loss : 31.075927734375, \t Total Dis Loss : 0.00011680342140607536\n",
      "Steps : 244200, \t Total Gen Loss : 30.638351440429688, \t Total Dis Loss : 0.0006981391925364733\n",
      "Steps : 244300, \t Total Gen Loss : 37.41021728515625, \t Total Dis Loss : 0.0001716117694741115\n",
      "Steps : 244400, \t Total Gen Loss : 38.29389953613281, \t Total Dis Loss : 2.2005053324392065e-05\n",
      "Steps : 244500, \t Total Gen Loss : 37.61060333251953, \t Total Dis Loss : 0.000998733565211296\n",
      "Steps : 244600, \t Total Gen Loss : 36.774620056152344, \t Total Dis Loss : 2.374755422351882e-05\n",
      "Steps : 244700, \t Total Gen Loss : 35.34766387939453, \t Total Dis Loss : 0.00027868006145581603\n",
      "Steps : 244800, \t Total Gen Loss : 34.81761932373047, \t Total Dis Loss : 0.00021907278278376907\n",
      "Steps : 244900, \t Total Gen Loss : 30.176177978515625, \t Total Dis Loss : 0.0034612452145665884\n",
      "Steps : 245000, \t Total Gen Loss : 34.575191497802734, \t Total Dis Loss : 2.0779561964445747e-05\n",
      "Steps : 245100, \t Total Gen Loss : 34.60600280761719, \t Total Dis Loss : 4.512606392381713e-05\n",
      "Steps : 245200, \t Total Gen Loss : 31.230636596679688, \t Total Dis Loss : 0.00020727395894937217\n",
      "Steps : 245300, \t Total Gen Loss : 37.280540466308594, \t Total Dis Loss : 0.0004760504816658795\n",
      "Steps : 245400, \t Total Gen Loss : 32.31733322143555, \t Total Dis Loss : 0.004378041252493858\n",
      "Steps : 245500, \t Total Gen Loss : 40.097496032714844, \t Total Dis Loss : 0.00014206439664121717\n",
      "Steps : 245600, \t Total Gen Loss : 40.08598327636719, \t Total Dis Loss : 0.00014927027223166078\n",
      "Steps : 245700, \t Total Gen Loss : 34.73883056640625, \t Total Dis Loss : 4.37797061749734e-05\n",
      "Steps : 245800, \t Total Gen Loss : 35.169677734375, \t Total Dis Loss : 0.0005842326791025698\n",
      "Steps : 245900, \t Total Gen Loss : 34.64910888671875, \t Total Dis Loss : 4.843629722017795e-05\n",
      "Steps : 246000, \t Total Gen Loss : 34.7802734375, \t Total Dis Loss : 4.124228871660307e-05\n",
      "Steps : 246100, \t Total Gen Loss : 34.291961669921875, \t Total Dis Loss : 2.3184154997579753e-05\n",
      "Steps : 246200, \t Total Gen Loss : 35.763328552246094, \t Total Dis Loss : 3.6367833672557026e-05\n",
      "Steps : 246300, \t Total Gen Loss : 33.44071960449219, \t Total Dis Loss : 1.8941722373710945e-05\n",
      "Steps : 246400, \t Total Gen Loss : 32.58329391479492, \t Total Dis Loss : 1.8927083146991208e-05\n",
      "Steps : 246500, \t Total Gen Loss : 31.371173858642578, \t Total Dis Loss : 6.600944470847026e-05\n",
      "Steps : 246600, \t Total Gen Loss : 32.61653137207031, \t Total Dis Loss : 0.00019098396296612918\n",
      "Steps : 246700, \t Total Gen Loss : 33.3996467590332, \t Total Dis Loss : 0.00020541390404105186\n",
      "Steps : 246800, \t Total Gen Loss : 30.289796829223633, \t Total Dis Loss : 2.1033254597568884e-05\n",
      "Steps : 246900, \t Total Gen Loss : 31.61166000366211, \t Total Dis Loss : 8.695204451214522e-05\n",
      "Steps : 247000, \t Total Gen Loss : 36.11594772338867, \t Total Dis Loss : 8.087383321253583e-05\n",
      "Steps : 247100, \t Total Gen Loss : 36.713924407958984, \t Total Dis Loss : 2.520333873690106e-05\n",
      "Steps : 247200, \t Total Gen Loss : 39.4496955871582, \t Total Dis Loss : 1.2083724868716672e-05\n",
      "Steps : 247300, \t Total Gen Loss : 37.47990417480469, \t Total Dis Loss : 8.025523129617795e-06\n",
      "Steps : 247400, \t Total Gen Loss : 33.19922637939453, \t Total Dis Loss : 1.2740745660266839e-05\n",
      "Steps : 247500, \t Total Gen Loss : 31.717283248901367, \t Total Dis Loss : 2.0932364350301214e-05\n",
      "Time for epoch 44 is 73.85016751289368 sec\n",
      "Steps : 247600, \t Total Gen Loss : 33.541099548339844, \t Total Dis Loss : 4.8600781155982986e-05\n",
      "Steps : 247700, \t Total Gen Loss : 31.20927619934082, \t Total Dis Loss : 2.849618613254279e-05\n",
      "Steps : 247800, \t Total Gen Loss : 30.12204360961914, \t Total Dis Loss : 2.526493881305214e-05\n",
      "Steps : 247900, \t Total Gen Loss : 35.337066650390625, \t Total Dis Loss : 5.4937127060838975e-06\n",
      "Steps : 248000, \t Total Gen Loss : 33.011905670166016, \t Total Dis Loss : 1.8886155885411426e-05\n",
      "Steps : 248100, \t Total Gen Loss : 34.158119201660156, \t Total Dis Loss : 1.3452769962896127e-05\n",
      "Steps : 248200, \t Total Gen Loss : 38.115928649902344, \t Total Dis Loss : 3.450523945502937e-05\n",
      "Steps : 248300, \t Total Gen Loss : 32.316192626953125, \t Total Dis Loss : 2.02262272068765e-05\n",
      "Steps : 248400, \t Total Gen Loss : 33.00723648071289, \t Total Dis Loss : 2.20046094909776e-05\n",
      "Steps : 248500, \t Total Gen Loss : 31.73231315612793, \t Total Dis Loss : 1.5479168723686598e-05\n",
      "Steps : 248600, \t Total Gen Loss : 30.75105094909668, \t Total Dis Loss : 2.183402466471307e-05\n",
      "Steps : 248700, \t Total Gen Loss : 37.68516540527344, \t Total Dis Loss : 4.5990978833287954e-05\n",
      "Steps : 248800, \t Total Gen Loss : 38.49919128417969, \t Total Dis Loss : 0.0013811272801831365\n",
      "Steps : 248900, \t Total Gen Loss : 31.533435821533203, \t Total Dis Loss : 4.2861123802140355e-05\n",
      "Steps : 249000, \t Total Gen Loss : 34.23765563964844, \t Total Dis Loss : 3.972203558078036e-05\n",
      "Steps : 249100, \t Total Gen Loss : 33.45066833496094, \t Total Dis Loss : 4.313049794291146e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 249200, \t Total Gen Loss : 34.72792053222656, \t Total Dis Loss : 1.0099094652105123e-05\n",
      "Steps : 249300, \t Total Gen Loss : 31.509855270385742, \t Total Dis Loss : 0.00015970479580573738\n",
      "Steps : 249400, \t Total Gen Loss : 31.940963745117188, \t Total Dis Loss : 1.9900193365174346e-05\n",
      "Steps : 249500, \t Total Gen Loss : 29.238670349121094, \t Total Dis Loss : 2.9983142667333595e-05\n",
      "Steps : 249600, \t Total Gen Loss : 33.07850646972656, \t Total Dis Loss : 0.0002170743973692879\n",
      "Steps : 249700, \t Total Gen Loss : 38.59690856933594, \t Total Dis Loss : 0.31159722805023193\n",
      "Steps : 249800, \t Total Gen Loss : 38.87080001831055, \t Total Dis Loss : 2.7266065444564447e-05\n",
      "Steps : 249900, \t Total Gen Loss : 41.43938446044922, \t Total Dis Loss : 6.510077218990773e-05\n",
      "Steps : 250000, \t Total Gen Loss : 40.976593017578125, \t Total Dis Loss : 2.350164868403226e-05\n",
      "Steps : 250100, \t Total Gen Loss : 33.81903076171875, \t Total Dis Loss : 0.0001232744543813169\n",
      "Steps : 250200, \t Total Gen Loss : 30.011608123779297, \t Total Dis Loss : 5.783979577245191e-05\n",
      "Steps : 250300, \t Total Gen Loss : 31.360036849975586, \t Total Dis Loss : 7.150594319682568e-05\n",
      "Steps : 250400, \t Total Gen Loss : 31.663209915161133, \t Total Dis Loss : 7.634310168214142e-05\n",
      "Steps : 250500, \t Total Gen Loss : 35.93476104736328, \t Total Dis Loss : 0.0002753656299319118\n",
      "Steps : 250600, \t Total Gen Loss : 34.81324768066406, \t Total Dis Loss : 0.00045381166273728013\n",
      "Steps : 250700, \t Total Gen Loss : 33.373077392578125, \t Total Dis Loss : 0.000130233631352894\n",
      "Steps : 250800, \t Total Gen Loss : 37.75630569458008, \t Total Dis Loss : 0.0003818916156888008\n",
      "Steps : 250900, \t Total Gen Loss : 33.690879821777344, \t Total Dis Loss : 0.0007822666666470468\n",
      "Steps : 251000, \t Total Gen Loss : 32.222694396972656, \t Total Dis Loss : 0.0007856041193008423\n",
      "Steps : 251100, \t Total Gen Loss : 37.05223846435547, \t Total Dis Loss : 0.007115783169865608\n",
      "Steps : 251200, \t Total Gen Loss : 32.578102111816406, \t Total Dis Loss : 0.0001522695238236338\n",
      "Steps : 251300, \t Total Gen Loss : 30.461910247802734, \t Total Dis Loss : 0.14093820750713348\n",
      "Steps : 251400, \t Total Gen Loss : 35.15869140625, \t Total Dis Loss : 0.0007453518337570131\n",
      "Steps : 251500, \t Total Gen Loss : 33.08191680908203, \t Total Dis Loss : 0.00018911922234110534\n",
      "Steps : 251600, \t Total Gen Loss : 31.73801040649414, \t Total Dis Loss : 0.00018437020480632782\n",
      "Steps : 251700, \t Total Gen Loss : 30.681413650512695, \t Total Dis Loss : 8.26589748612605e-05\n",
      "Steps : 251800, \t Total Gen Loss : 31.344158172607422, \t Total Dis Loss : 0.00017814966849982738\n",
      "Steps : 251900, \t Total Gen Loss : 30.201541900634766, \t Total Dis Loss : 0.0003771604096982628\n",
      "Steps : 252000, \t Total Gen Loss : 33.38291931152344, \t Total Dis Loss : 0.00024823585408739746\n",
      "Steps : 252100, \t Total Gen Loss : 29.88555908203125, \t Total Dis Loss : 0.0001423460926162079\n",
      "Steps : 252200, \t Total Gen Loss : 33.93815612792969, \t Total Dis Loss : 0.0005505093722604215\n",
      "Steps : 252300, \t Total Gen Loss : 33.889644622802734, \t Total Dis Loss : 0.00019428781524766237\n",
      "Steps : 252400, \t Total Gen Loss : 34.28881072998047, \t Total Dis Loss : 0.00023571860219817609\n",
      "Steps : 252500, \t Total Gen Loss : 29.998188018798828, \t Total Dis Loss : 0.0003351782215759158\n",
      "Steps : 252600, \t Total Gen Loss : 29.767704010009766, \t Total Dis Loss : 0.00019105224055238068\n",
      "Steps : 252700, \t Total Gen Loss : 34.27231979370117, \t Total Dis Loss : 3.9752470911480486e-05\n",
      "Steps : 252800, \t Total Gen Loss : 29.534894943237305, \t Total Dis Loss : 0.002141105942428112\n",
      "Steps : 252900, \t Total Gen Loss : 29.61617660522461, \t Total Dis Loss : 0.00026546113076619804\n",
      "Steps : 253000, \t Total Gen Loss : 28.66432762145996, \t Total Dis Loss : 0.00010008696699514985\n",
      "Steps : 253100, \t Total Gen Loss : 31.724639892578125, \t Total Dis Loss : 0.00024227987159974873\n",
      "Time for epoch 45 is 73.90634393692017 sec\n",
      "Steps : 253200, \t Total Gen Loss : 31.77780532836914, \t Total Dis Loss : 0.0001127161958720535\n",
      "Steps : 253300, \t Total Gen Loss : 31.184789657592773, \t Total Dis Loss : 0.00021310349984560162\n",
      "Steps : 253400, \t Total Gen Loss : 29.957468032836914, \t Total Dis Loss : 0.0003122976631857455\n",
      "Steps : 253500, \t Total Gen Loss : 35.22920227050781, \t Total Dis Loss : 8.819192589726299e-05\n",
      "Steps : 253600, \t Total Gen Loss : 30.31679344177246, \t Total Dis Loss : 9.822838910622522e-05\n",
      "Steps : 253700, \t Total Gen Loss : 32.44474792480469, \t Total Dis Loss : 8.648102084407583e-05\n",
      "Steps : 253800, \t Total Gen Loss : 33.616790771484375, \t Total Dis Loss : 9.846978355199099e-05\n",
      "Steps : 253900, \t Total Gen Loss : 33.04824447631836, \t Total Dis Loss : 0.00013690038758795708\n",
      "Steps : 254000, \t Total Gen Loss : 27.866329193115234, \t Total Dis Loss : 8.410782902501523e-05\n",
      "Steps : 254100, \t Total Gen Loss : 34.97999572753906, \t Total Dis Loss : 7.49705868656747e-05\n",
      "Steps : 254200, \t Total Gen Loss : 32.97834777832031, \t Total Dis Loss : 6.559785106219351e-05\n",
      "Steps : 254300, \t Total Gen Loss : 30.29946517944336, \t Total Dis Loss : 8.481595432385802e-05\n",
      "Steps : 254400, \t Total Gen Loss : 32.736297607421875, \t Total Dis Loss : 6.045236295904033e-05\n",
      "Steps : 254500, \t Total Gen Loss : 28.89341163635254, \t Total Dis Loss : 0.0001006096790661104\n",
      "Steps : 254600, \t Total Gen Loss : 35.3299674987793, \t Total Dis Loss : 0.0006557770539075136\n",
      "Steps : 254700, \t Total Gen Loss : 31.686546325683594, \t Total Dis Loss : 0.00017463254334870726\n",
      "Steps : 254800, \t Total Gen Loss : 34.690635681152344, \t Total Dis Loss : 0.0004273498198017478\n",
      "Steps : 254900, \t Total Gen Loss : 30.22416877746582, \t Total Dis Loss : 0.001828473061323166\n",
      "Steps : 255000, \t Total Gen Loss : 31.291507720947266, \t Total Dis Loss : 0.00032313651172444224\n",
      "Steps : 255100, \t Total Gen Loss : 33.544090270996094, \t Total Dis Loss : 0.0003250087611377239\n",
      "Steps : 255200, \t Total Gen Loss : 33.436527252197266, \t Total Dis Loss : 0.0001978351647267118\n",
      "Steps : 255300, \t Total Gen Loss : 31.239654541015625, \t Total Dis Loss : 0.0006206443649716675\n",
      "Steps : 255400, \t Total Gen Loss : 32.412567138671875, \t Total Dis Loss : 0.00047232082579284906\n",
      "Steps : 255500, \t Total Gen Loss : 32.38502502441406, \t Total Dis Loss : 0.000500284309964627\n",
      "Steps : 255600, \t Total Gen Loss : 39.10282897949219, \t Total Dis Loss : 0.00044249536585994065\n",
      "Steps : 255700, \t Total Gen Loss : 38.537696838378906, \t Total Dis Loss : 5.1294759032316506e-05\n",
      "Steps : 255800, \t Total Gen Loss : 33.429874420166016, \t Total Dis Loss : 6.018859858158976e-05\n",
      "Steps : 255900, \t Total Gen Loss : 30.329784393310547, \t Total Dis Loss : 0.00018400857516098768\n",
      "Steps : 256000, \t Total Gen Loss : 31.498184204101562, \t Total Dis Loss : 9.628963016439229e-05\n",
      "Steps : 256100, \t Total Gen Loss : 29.597183227539062, \t Total Dis Loss : 0.00027954764664173126\n",
      "Steps : 256200, \t Total Gen Loss : 29.156482696533203, \t Total Dis Loss : 0.00015275295299943537\n",
      "Steps : 256300, \t Total Gen Loss : 29.512226104736328, \t Total Dis Loss : 0.00011583033483475447\n",
      "Steps : 256400, \t Total Gen Loss : 32.490821838378906, \t Total Dis Loss : 0.0008671189425513148\n",
      "Steps : 256500, \t Total Gen Loss : 28.404739379882812, \t Total Dis Loss : 9.653463348513469e-05\n",
      "Steps : 256600, \t Total Gen Loss : 30.288583755493164, \t Total Dis Loss : 5.249962123343721e-05\n",
      "Steps : 256700, \t Total Gen Loss : 30.923709869384766, \t Total Dis Loss : 0.00013887287059333175\n",
      "Steps : 256800, \t Total Gen Loss : 32.78272247314453, \t Total Dis Loss : 0.00017034595657605678\n",
      "Steps : 256900, \t Total Gen Loss : 33.209014892578125, \t Total Dis Loss : 8.874537161318585e-05\n",
      "Steps : 257000, \t Total Gen Loss : 32.14881896972656, \t Total Dis Loss : 7.538998033851385e-05\n",
      "Steps : 257100, \t Total Gen Loss : 35.45199966430664, \t Total Dis Loss : 7.680054113734514e-05\n",
      "Steps : 257200, \t Total Gen Loss : 34.64897918701172, \t Total Dis Loss : 7.579333760077134e-05\n",
      "Steps : 257300, \t Total Gen Loss : 33.75226593017578, \t Total Dis Loss : 7.144183473428711e-05\n",
      "Steps : 257400, \t Total Gen Loss : 33.51935577392578, \t Total Dis Loss : 0.00028143415693193674\n",
      "Steps : 257500, \t Total Gen Loss : 35.597251892089844, \t Total Dis Loss : 0.00019744197197724134\n",
      "Steps : 257600, \t Total Gen Loss : 34.84589385986328, \t Total Dis Loss : 0.00010425224900245667\n",
      "Steps : 257700, \t Total Gen Loss : 35.36096954345703, \t Total Dis Loss : 6.0031627072021365e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 257800, \t Total Gen Loss : 29.728656768798828, \t Total Dis Loss : 0.0009086556965485215\n",
      "Steps : 257900, \t Total Gen Loss : 35.095672607421875, \t Total Dis Loss : 9.175114246318117e-05\n",
      "Steps : 258000, \t Total Gen Loss : 38.40056610107422, \t Total Dis Loss : 0.0001762907049851492\n",
      "Steps : 258100, \t Total Gen Loss : 24.310237884521484, \t Total Dis Loss : 1.308231234550476\n",
      "Steps : 258200, \t Total Gen Loss : 30.412643432617188, \t Total Dis Loss : 0.028402619063854218\n",
      "Steps : 258300, \t Total Gen Loss : 30.784217834472656, \t Total Dis Loss : 0.01734481379389763\n",
      "Steps : 258400, \t Total Gen Loss : 28.437217712402344, \t Total Dis Loss : 0.029580995440483093\n",
      "Steps : 258500, \t Total Gen Loss : 33.9869499206543, \t Total Dis Loss : 6.936161662451923e-05\n",
      "Steps : 258600, \t Total Gen Loss : 34.09639358520508, \t Total Dis Loss : 0.009794809855520725\n",
      "Steps : 258700, \t Total Gen Loss : 29.418140411376953, \t Total Dis Loss : 0.00015515570703428239\n",
      "Time for epoch 46 is 73.60323977470398 sec\n",
      "Steps : 258800, \t Total Gen Loss : 30.286191940307617, \t Total Dis Loss : 0.00024278870841953903\n",
      "Steps : 258900, \t Total Gen Loss : 34.99953842163086, \t Total Dis Loss : 0.00040653962059877813\n",
      "Steps : 259000, \t Total Gen Loss : 30.66778564453125, \t Total Dis Loss : 0.00014472714974544942\n",
      "Steps : 259100, \t Total Gen Loss : 29.543529510498047, \t Total Dis Loss : 0.00010821304749697447\n",
      "Steps : 259200, \t Total Gen Loss : 31.573759078979492, \t Total Dis Loss : 0.00010452509013703093\n",
      "Steps : 259300, \t Total Gen Loss : 29.181753158569336, \t Total Dis Loss : 7.412009290419519e-05\n",
      "Steps : 259400, \t Total Gen Loss : 35.526527404785156, \t Total Dis Loss : 0.0001539798395242542\n",
      "Steps : 259500, \t Total Gen Loss : 33.15778732299805, \t Total Dis Loss : 8.112951036309823e-05\n",
      "Steps : 259600, \t Total Gen Loss : 30.406105041503906, \t Total Dis Loss : 8.038823580136523e-05\n",
      "Steps : 259700, \t Total Gen Loss : 29.071683883666992, \t Total Dis Loss : 0.00036468266625888646\n",
      "Steps : 259800, \t Total Gen Loss : 32.699012756347656, \t Total Dis Loss : 0.00010933294106507674\n",
      "Steps : 259900, \t Total Gen Loss : 30.56460952758789, \t Total Dis Loss : 0.00019064341904595494\n",
      "Steps : 260000, \t Total Gen Loss : 29.834760665893555, \t Total Dis Loss : 9.129996033152565e-05\n",
      "Steps : 260100, \t Total Gen Loss : 30.07796287536621, \t Total Dis Loss : 8.28117408673279e-05\n",
      "Steps : 260200, \t Total Gen Loss : 26.16942024230957, \t Total Dis Loss : 7.160325185395777e-05\n",
      "Steps : 260300, \t Total Gen Loss : 30.398563385009766, \t Total Dis Loss : 4.9841801228467375e-05\n",
      "Steps : 260400, \t Total Gen Loss : 32.789039611816406, \t Total Dis Loss : 4.7504581743851304e-05\n",
      "Steps : 260500, \t Total Gen Loss : 32.30517578125, \t Total Dis Loss : 0.00011037804506486282\n",
      "Steps : 260600, \t Total Gen Loss : 29.181589126586914, \t Total Dis Loss : 0.0006036778213456273\n",
      "Steps : 260700, \t Total Gen Loss : 30.38507843017578, \t Total Dis Loss : 0.0001870948326541111\n",
      "Steps : 260800, \t Total Gen Loss : 29.966323852539062, \t Total Dis Loss : 0.00011852754687424749\n",
      "Steps : 260900, \t Total Gen Loss : 28.674428939819336, \t Total Dis Loss : 0.0002787018020171672\n",
      "Steps : 261000, \t Total Gen Loss : 30.73413848876953, \t Total Dis Loss : 0.00011715254368027672\n",
      "Steps : 261100, \t Total Gen Loss : 29.7003173828125, \t Total Dis Loss : 8.936050289776176e-05\n",
      "Steps : 261200, \t Total Gen Loss : 31.300148010253906, \t Total Dis Loss : 0.00034120018244720995\n",
      "Steps : 261300, \t Total Gen Loss : 33.49867630004883, \t Total Dis Loss : 0.0005876263021491468\n",
      "Steps : 261400, \t Total Gen Loss : 30.00531768798828, \t Total Dis Loss : 0.00030069792410358787\n",
      "Steps : 261500, \t Total Gen Loss : 37.351341247558594, \t Total Dis Loss : 7.340007414313732e-06\n",
      "Steps : 261600, \t Total Gen Loss : 28.865489959716797, \t Total Dis Loss : 0.00036129605723544955\n",
      "Steps : 261700, \t Total Gen Loss : 34.34166717529297, \t Total Dis Loss : 1.278638683288591e-05\n",
      "Steps : 261800, \t Total Gen Loss : 36.403778076171875, \t Total Dis Loss : 0.000124732730910182\n",
      "Steps : 261900, \t Total Gen Loss : 34.26846694946289, \t Total Dis Loss : 2.237485568912234e-05\n",
      "Steps : 262000, \t Total Gen Loss : 31.54724884033203, \t Total Dis Loss : 2.1662315702997148e-05\n",
      "Steps : 262100, \t Total Gen Loss : 34.30888366699219, \t Total Dis Loss : 5.809118738397956e-05\n",
      "Steps : 262200, \t Total Gen Loss : 32.61732482910156, \t Total Dis Loss : 5.5509946832899004e-05\n",
      "Steps : 262300, \t Total Gen Loss : 35.609130859375, \t Total Dis Loss : 6.723885599058121e-05\n",
      "Steps : 262400, \t Total Gen Loss : 33.07175827026367, \t Total Dis Loss : 0.00013356322597246617\n",
      "Steps : 262500, \t Total Gen Loss : 33.94060134887695, \t Total Dis Loss : 3.7437792343553156e-05\n",
      "Steps : 262600, \t Total Gen Loss : 33.78282928466797, \t Total Dis Loss : 4.4909720600117e-05\n",
      "Steps : 262700, \t Total Gen Loss : 33.283714294433594, \t Total Dis Loss : 0.0002912510826718062\n",
      "Steps : 262800, \t Total Gen Loss : 30.064250946044922, \t Total Dis Loss : 0.0013947243569418788\n",
      "Steps : 262900, \t Total Gen Loss : 33.25810241699219, \t Total Dis Loss : 0.0009911962551996112\n",
      "Steps : 263000, \t Total Gen Loss : 35.20818328857422, \t Total Dis Loss : 0.0012396093225106597\n",
      "Steps : 263100, \t Total Gen Loss : 30.017288208007812, \t Total Dis Loss : 5.553327719098888e-05\n",
      "Steps : 263200, \t Total Gen Loss : 30.892398834228516, \t Total Dis Loss : 7.971319428179413e-05\n",
      "Steps : 263300, \t Total Gen Loss : 32.69068908691406, \t Total Dis Loss : 0.00010692477371776477\n",
      "Steps : 263400, \t Total Gen Loss : 28.02215576171875, \t Total Dis Loss : 0.0006991224363446236\n",
      "Steps : 263500, \t Total Gen Loss : 30.731611251831055, \t Total Dis Loss : 0.000351463386323303\n",
      "Steps : 263600, \t Total Gen Loss : 30.49167823791504, \t Total Dis Loss : 8.17644177004695e-05\n",
      "Steps : 263700, \t Total Gen Loss : 35.46711730957031, \t Total Dis Loss : 0.0002895151264965534\n",
      "Steps : 263800, \t Total Gen Loss : 29.297607421875, \t Total Dis Loss : 0.00015038816491141915\n",
      "Steps : 263900, \t Total Gen Loss : 31.117111206054688, \t Total Dis Loss : 0.00022539521160069853\n",
      "Steps : 264000, \t Total Gen Loss : 30.592369079589844, \t Total Dis Loss : 0.00019659058307297528\n",
      "Steps : 264100, \t Total Gen Loss : 33.10871124267578, \t Total Dis Loss : 0.8165482878684998\n",
      "Steps : 264200, \t Total Gen Loss : 29.624954223632812, \t Total Dis Loss : 0.00020136093371547759\n",
      "Steps : 264300, \t Total Gen Loss : 30.304019927978516, \t Total Dis Loss : 0.00040475319838151336\n",
      "Time for epoch 47 is 73.67174220085144 sec\n",
      "Steps : 264400, \t Total Gen Loss : 27.121625900268555, \t Total Dis Loss : 0.0009940466843545437\n",
      "Steps : 264500, \t Total Gen Loss : 33.77191162109375, \t Total Dis Loss : 0.001281966920942068\n",
      "Steps : 264600, \t Total Gen Loss : 33.40228271484375, \t Total Dis Loss : 0.0003723051049746573\n",
      "Steps : 264700, \t Total Gen Loss : 30.70871353149414, \t Total Dis Loss : 0.00015272882592398673\n",
      "Steps : 264800, \t Total Gen Loss : 34.23664093017578, \t Total Dis Loss : 0.00023001522640697658\n",
      "Steps : 264900, \t Total Gen Loss : 34.50445556640625, \t Total Dis Loss : 0.00033209420507773757\n",
      "Steps : 265000, \t Total Gen Loss : 38.459922790527344, \t Total Dis Loss : 0.00012732816685456783\n",
      "Steps : 265100, \t Total Gen Loss : 33.95111846923828, \t Total Dis Loss : 2.249228418804705e-05\n",
      "Steps : 265200, \t Total Gen Loss : 32.34441375732422, \t Total Dis Loss : 0.00014530314365401864\n",
      "Steps : 265300, \t Total Gen Loss : 28.972036361694336, \t Total Dis Loss : 4.36410955444444e-05\n",
      "Steps : 265400, \t Total Gen Loss : 33.36918640136719, \t Total Dis Loss : 3.583737270673737e-05\n",
      "Steps : 265500, \t Total Gen Loss : 32.882347106933594, \t Total Dis Loss : 3.1280145776690915e-05\n",
      "Steps : 265600, \t Total Gen Loss : 35.694610595703125, \t Total Dis Loss : 2.022816988755949e-05\n",
      "Steps : 265700, \t Total Gen Loss : 35.251705169677734, \t Total Dis Loss : 4.417440504767001e-05\n",
      "Steps : 265800, \t Total Gen Loss : 30.76188087463379, \t Total Dis Loss : 4.3867999920621514e-05\n",
      "Steps : 265900, \t Total Gen Loss : 31.367389678955078, \t Total Dis Loss : 4.794724372914061e-05\n",
      "Steps : 266000, \t Total Gen Loss : 30.804977416992188, \t Total Dis Loss : 1.81746254384052e-05\n",
      "Steps : 266100, \t Total Gen Loss : 38.3309326171875, \t Total Dis Loss : 7.511332660214975e-05\n",
      "Steps : 266200, \t Total Gen Loss : 33.93401336669922, \t Total Dis Loss : 0.00029577146051451564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 266300, \t Total Gen Loss : 34.30554962158203, \t Total Dis Loss : 0.00015430079656653106\n",
      "Steps : 266400, \t Total Gen Loss : 33.5150146484375, \t Total Dis Loss : 0.0016754004172980785\n",
      "Steps : 266500, \t Total Gen Loss : 31.37626838684082, \t Total Dis Loss : 0.01502983458340168\n",
      "Steps : 266600, \t Total Gen Loss : 37.71845245361328, \t Total Dis Loss : 4.862221248913556e-05\n",
      "Steps : 266700, \t Total Gen Loss : 36.42731475830078, \t Total Dis Loss : 0.0001125101771322079\n",
      "Steps : 266800, \t Total Gen Loss : 38.38877868652344, \t Total Dis Loss : 0.00012963208428118378\n",
      "Steps : 266900, \t Total Gen Loss : 36.314517974853516, \t Total Dis Loss : 0.0025828424841165543\n",
      "Steps : 267000, \t Total Gen Loss : 35.66424560546875, \t Total Dis Loss : 0.0007964515243656933\n",
      "Steps : 267100, \t Total Gen Loss : 33.31059265136719, \t Total Dis Loss : 0.00024649486294947565\n",
      "Steps : 267200, \t Total Gen Loss : 32.2615966796875, \t Total Dis Loss : 0.0005391400773078203\n",
      "Steps : 267300, \t Total Gen Loss : 35.309715270996094, \t Total Dis Loss : 0.00026899640215560794\n",
      "Steps : 267400, \t Total Gen Loss : 37.03413391113281, \t Total Dis Loss : 0.0009494406986050308\n",
      "Steps : 267500, \t Total Gen Loss : 31.658462524414062, \t Total Dis Loss : 0.0009517463622614741\n",
      "Steps : 267600, \t Total Gen Loss : 32.86399841308594, \t Total Dis Loss : 0.01853102073073387\n",
      "Steps : 267700, \t Total Gen Loss : 35.53990173339844, \t Total Dis Loss : 0.00025546897086314857\n",
      "Steps : 267800, \t Total Gen Loss : 36.19719696044922, \t Total Dis Loss : 0.0004067675909027457\n",
      "Steps : 267900, \t Total Gen Loss : 33.92201232910156, \t Total Dis Loss : 0.00023943504493217915\n",
      "Steps : 268000, \t Total Gen Loss : 35.292945861816406, \t Total Dis Loss : 0.00016008812235668302\n",
      "Steps : 268100, \t Total Gen Loss : 34.09138488769531, \t Total Dis Loss : 0.0001510797446826473\n",
      "Steps : 268200, \t Total Gen Loss : 31.153675079345703, \t Total Dis Loss : 0.00042886639130301774\n",
      "Steps : 268300, \t Total Gen Loss : 37.17859649658203, \t Total Dis Loss : 0.0002978283737320453\n",
      "Steps : 268400, \t Total Gen Loss : 39.98280715942383, \t Total Dis Loss : 0.00012762371625285596\n",
      "Steps : 268500, \t Total Gen Loss : 32.011024475097656, \t Total Dis Loss : 0.0006644303211942315\n",
      "Steps : 268600, \t Total Gen Loss : 31.66887664794922, \t Total Dis Loss : 0.00019465122022666037\n",
      "Steps : 268700, \t Total Gen Loss : 35.86235046386719, \t Total Dis Loss : 0.0003941459290217608\n",
      "Steps : 268800, \t Total Gen Loss : 31.386030197143555, \t Total Dis Loss : 0.00018826997256837785\n",
      "Steps : 268900, \t Total Gen Loss : 35.60509490966797, \t Total Dis Loss : 0.00016253188368864357\n",
      "Steps : 269000, \t Total Gen Loss : 35.01435089111328, \t Total Dis Loss : 0.00012814979709219187\n",
      "Steps : 269100, \t Total Gen Loss : 36.96441650390625, \t Total Dis Loss : 0.00011807957343989983\n",
      "Steps : 269200, \t Total Gen Loss : 34.200843811035156, \t Total Dis Loss : 9.74879803834483e-05\n",
      "Steps : 269300, \t Total Gen Loss : 34.84480285644531, \t Total Dis Loss : 0.00011367935803718865\n",
      "Steps : 269400, \t Total Gen Loss : 32.94580841064453, \t Total Dis Loss : 9.353867790196091e-05\n",
      "Steps : 269500, \t Total Gen Loss : 36.34563446044922, \t Total Dis Loss : 4.6157070755725726e-05\n",
      "Steps : 269600, \t Total Gen Loss : 32.262264251708984, \t Total Dis Loss : 0.0002397916541667655\n",
      "Steps : 269700, \t Total Gen Loss : 37.76579666137695, \t Total Dis Loss : 0.0001503013336332515\n",
      "Steps : 269800, \t Total Gen Loss : 36.565521240234375, \t Total Dis Loss : 0.0005400002701207995\n",
      "Steps : 269900, \t Total Gen Loss : 35.40326690673828, \t Total Dis Loss : 0.0002938341349363327\n",
      "Steps : 270000, \t Total Gen Loss : 30.889808654785156, \t Total Dis Loss : 0.0013412822736427188\n",
      "Time for epoch 48 is 73.78758120536804 sec\n",
      "Steps : 270100, \t Total Gen Loss : 33.541595458984375, \t Total Dis Loss : 0.00021335348719730973\n",
      "Steps : 270200, \t Total Gen Loss : 35.42283630371094, \t Total Dis Loss : 0.003433818928897381\n",
      "Steps : 270300, \t Total Gen Loss : 35.80475616455078, \t Total Dis Loss : 0.0004455632879398763\n",
      "Steps : 270400, \t Total Gen Loss : 30.37082862854004, \t Total Dis Loss : 0.004548631142824888\n",
      "Steps : 270500, \t Total Gen Loss : 33.60379409790039, \t Total Dis Loss : 0.00018543271289672703\n",
      "Steps : 270600, \t Total Gen Loss : 31.131351470947266, \t Total Dis Loss : 0.0004431768029462546\n",
      "Steps : 270700, \t Total Gen Loss : 35.60565185546875, \t Total Dis Loss : 4.648714457289316e-05\n",
      "Steps : 270800, \t Total Gen Loss : 30.546749114990234, \t Total Dis Loss : 0.00027878308901563287\n",
      "Steps : 270900, \t Total Gen Loss : 28.639705657958984, \t Total Dis Loss : 0.0004706394101958722\n",
      "Steps : 271000, \t Total Gen Loss : 29.180164337158203, \t Total Dis Loss : 9.721630340209231e-05\n",
      "Steps : 271100, \t Total Gen Loss : 29.405838012695312, \t Total Dis Loss : 0.00020640372531488538\n",
      "Steps : 271200, \t Total Gen Loss : 30.01055335998535, \t Total Dis Loss : 0.00011213414836674929\n",
      "Steps : 271300, \t Total Gen Loss : 30.011436462402344, \t Total Dis Loss : 0.00013428559759631753\n",
      "Steps : 271400, \t Total Gen Loss : 34.22211456298828, \t Total Dis Loss : 0.0003521142352838069\n",
      "Steps : 271500, \t Total Gen Loss : 32.65559005737305, \t Total Dis Loss : 0.0002548843913245946\n",
      "Steps : 271600, \t Total Gen Loss : 30.97955322265625, \t Total Dis Loss : 0.0004009807307738811\n",
      "Steps : 271700, \t Total Gen Loss : 29.625812530517578, \t Total Dis Loss : 0.00029511304455809295\n",
      "Steps : 271800, \t Total Gen Loss : 33.78595733642578, \t Total Dis Loss : 0.00015904763131402433\n",
      "Steps : 271900, \t Total Gen Loss : 32.07541275024414, \t Total Dis Loss : 0.00016897314344532788\n",
      "Steps : 272000, \t Total Gen Loss : 29.927356719970703, \t Total Dis Loss : 0.0008568307384848595\n",
      "Steps : 272100, \t Total Gen Loss : 30.529190063476562, \t Total Dis Loss : 0.00020858249627053738\n",
      "Steps : 272200, \t Total Gen Loss : 29.505985260009766, \t Total Dis Loss : 0.0005046488367952406\n",
      "Steps : 272300, \t Total Gen Loss : 29.505531311035156, \t Total Dis Loss : 0.0003804230655077845\n",
      "Steps : 272400, \t Total Gen Loss : 29.07269287109375, \t Total Dis Loss : 0.00044672065996564925\n",
      "Steps : 272500, \t Total Gen Loss : 29.89499282836914, \t Total Dis Loss : 0.00016681008855812252\n",
      "Steps : 272600, \t Total Gen Loss : 29.558860778808594, \t Total Dis Loss : 0.00018481796723790467\n",
      "Steps : 272700, \t Total Gen Loss : 29.782085418701172, \t Total Dis Loss : 0.00019670111942104995\n",
      "Steps : 272800, \t Total Gen Loss : 27.60274887084961, \t Total Dis Loss : 0.00041278882417827845\n",
      "Steps : 272900, \t Total Gen Loss : 30.10610008239746, \t Total Dis Loss : 0.00015594647265970707\n",
      "Steps : 273000, \t Total Gen Loss : 30.598773956298828, \t Total Dis Loss : 0.00017882657994050533\n",
      "Steps : 273100, \t Total Gen Loss : 29.28152847290039, \t Total Dis Loss : 7.019089389359578e-05\n",
      "Steps : 273200, \t Total Gen Loss : 35.727684020996094, \t Total Dis Loss : 0.0003853800008073449\n",
      "Steps : 273300, \t Total Gen Loss : 29.94915008544922, \t Total Dis Loss : 0.0010433847783133388\n",
      "Steps : 273400, \t Total Gen Loss : 43.290218353271484, \t Total Dis Loss : 0.0005669513484463096\n",
      "Steps : 273500, \t Total Gen Loss : 39.226593017578125, \t Total Dis Loss : 0.0011055890936404467\n",
      "Steps : 273600, \t Total Gen Loss : 39.91053771972656, \t Total Dis Loss : 0.0002152968372683972\n",
      "Steps : 273700, \t Total Gen Loss : 37.027442932128906, \t Total Dis Loss : 6.848932389402762e-05\n",
      "Steps : 273800, \t Total Gen Loss : 33.6928596496582, \t Total Dis Loss : 0.00018050374637823552\n",
      "Steps : 273900, \t Total Gen Loss : 34.52545928955078, \t Total Dis Loss : 0.00010420552280265838\n",
      "Steps : 274000, \t Total Gen Loss : 32.86480712890625, \t Total Dis Loss : 6.004214446875267e-05\n",
      "Steps : 274100, \t Total Gen Loss : 36.866355895996094, \t Total Dis Loss : 4.127461579628289e-05\n",
      "Steps : 274200, \t Total Gen Loss : 33.331443786621094, \t Total Dis Loss : 0.0003159613406751305\n",
      "Steps : 274300, \t Total Gen Loss : 26.778362274169922, \t Total Dis Loss : 0.002446031430736184\n",
      "Steps : 274400, \t Total Gen Loss : 28.653289794921875, \t Total Dis Loss : 0.00033360248198732734\n",
      "Steps : 274500, \t Total Gen Loss : 29.968015670776367, \t Total Dis Loss : 0.00041676548426039517\n",
      "Steps : 274600, \t Total Gen Loss : 27.412134170532227, \t Total Dis Loss : 0.000412744062487036\n",
      "Steps : 274700, \t Total Gen Loss : 31.021512985229492, \t Total Dis Loss : 0.0008068355382420123\n",
      "Steps : 274800, \t Total Gen Loss : 29.14899444580078, \t Total Dis Loss : 0.00037174709723331034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 274900, \t Total Gen Loss : 29.230815887451172, \t Total Dis Loss : 0.0001628096360946074\n",
      "Steps : 275000, \t Total Gen Loss : 26.05615234375, \t Total Dis Loss : 0.0025641932152211666\n",
      "Steps : 275100, \t Total Gen Loss : 29.23089599609375, \t Total Dis Loss : 8.235639688791707e-05\n",
      "Steps : 275200, \t Total Gen Loss : 30.777969360351562, \t Total Dis Loss : 0.00012630803394131362\n",
      "Steps : 275300, \t Total Gen Loss : 30.61851692199707, \t Total Dis Loss : 0.0002893604396376759\n",
      "Steps : 275400, \t Total Gen Loss : 33.0623779296875, \t Total Dis Loss : 0.00012264192628208548\n",
      "Steps : 275500, \t Total Gen Loss : 31.74482536315918, \t Total Dis Loss : 7.24744240869768e-05\n",
      "Steps : 275600, \t Total Gen Loss : 30.366783142089844, \t Total Dis Loss : 0.0001795777934603393\n",
      "Time for epoch 49 is 73.77779006958008 sec\n",
      "Steps : 275700, \t Total Gen Loss : 31.839033126831055, \t Total Dis Loss : 6.981891056057066e-05\n",
      "Steps : 275800, \t Total Gen Loss : 30.90747833251953, \t Total Dis Loss : 0.00014224223559722304\n",
      "Steps : 275900, \t Total Gen Loss : 33.26031494140625, \t Total Dis Loss : 6.758102972526103e-05\n",
      "Steps : 276000, \t Total Gen Loss : 29.291515350341797, \t Total Dis Loss : 9.265481639886275e-05\n",
      "Steps : 276100, \t Total Gen Loss : 31.81607437133789, \t Total Dis Loss : 7.825384091120213e-05\n",
      "Steps : 276200, \t Total Gen Loss : 29.871604919433594, \t Total Dis Loss : 3.861220830003731e-05\n",
      "Steps : 276300, \t Total Gen Loss : 27.464136123657227, \t Total Dis Loss : 6.635524914599955e-05\n",
      "Steps : 276400, \t Total Gen Loss : 30.728328704833984, \t Total Dis Loss : 0.00010920560453087091\n",
      "Steps : 276500, \t Total Gen Loss : 32.393428802490234, \t Total Dis Loss : 4.824463030672632e-05\n",
      "Steps : 276600, \t Total Gen Loss : 31.700990676879883, \t Total Dis Loss : 0.00017439945077057928\n",
      "Steps : 276700, \t Total Gen Loss : 31.489471435546875, \t Total Dis Loss : 5.25191608176101e-05\n",
      "Steps : 276800, \t Total Gen Loss : 28.418365478515625, \t Total Dis Loss : 9.349688480142504e-05\n",
      "Steps : 276900, \t Total Gen Loss : 26.690048217773438, \t Total Dis Loss : 0.00013902537466492504\n",
      "Steps : 277000, \t Total Gen Loss : 35.256134033203125, \t Total Dis Loss : 0.0003752953780349344\n",
      "Steps : 277100, \t Total Gen Loss : 30.710662841796875, \t Total Dis Loss : 0.00015418346447404474\n",
      "Steps : 277200, \t Total Gen Loss : 32.031822204589844, \t Total Dis Loss : 0.0001149060481111519\n",
      "Steps : 277300, \t Total Gen Loss : 34.46143341064453, \t Total Dis Loss : 5.543007864616811e-05\n",
      "Steps : 277400, \t Total Gen Loss : 29.18366050720215, \t Total Dis Loss : 8.46747134346515e-05\n",
      "Steps : 277500, \t Total Gen Loss : 29.953243255615234, \t Total Dis Loss : 7.324200851144269e-05\n",
      "Steps : 277600, \t Total Gen Loss : 34.25578308105469, \t Total Dis Loss : 6.985728396102786e-05\n",
      "Steps : 277700, \t Total Gen Loss : 29.353862762451172, \t Total Dis Loss : 3.151618511765264e-05\n",
      "Steps : 277800, \t Total Gen Loss : 30.78297233581543, \t Total Dis Loss : 9.610613051336259e-05\n",
      "Steps : 277900, \t Total Gen Loss : 33.855873107910156, \t Total Dis Loss : 3.130553523078561e-05\n",
      "Steps : 278000, \t Total Gen Loss : 34.3049430847168, \t Total Dis Loss : 6.562767521245405e-05\n",
      "Steps : 278100, \t Total Gen Loss : 33.229713439941406, \t Total Dis Loss : 4.39159048255533e-05\n",
      "Steps : 278200, \t Total Gen Loss : 32.86356735229492, \t Total Dis Loss : 0.00036663803621195257\n",
      "Steps : 278300, \t Total Gen Loss : 32.425498962402344, \t Total Dis Loss : 0.0001062678056769073\n",
      "Steps : 278400, \t Total Gen Loss : 33.08530807495117, \t Total Dis Loss : 6.557159940712154e-05\n",
      "Steps : 278500, \t Total Gen Loss : 32.022056579589844, \t Total Dis Loss : 5.324263474904001e-05\n",
      "Steps : 278600, \t Total Gen Loss : 30.96075439453125, \t Total Dis Loss : 2.5550703867338598e-05\n",
      "Steps : 278700, \t Total Gen Loss : 27.385770797729492, \t Total Dis Loss : 8.448638982372358e-05\n",
      "Steps : 278800, \t Total Gen Loss : 32.715335845947266, \t Total Dis Loss : 4.73604304715991e-05\n",
      "Steps : 278900, \t Total Gen Loss : 31.543291091918945, \t Total Dis Loss : 3.518588709994219e-05\n",
      "Steps : 279000, \t Total Gen Loss : 31.59161949157715, \t Total Dis Loss : 2.0382758521009237e-05\n",
      "Steps : 279100, \t Total Gen Loss : 30.561763763427734, \t Total Dis Loss : 3.645633842097595e-05\n",
      "Steps : 279200, \t Total Gen Loss : 30.91596221923828, \t Total Dis Loss : 3.19760620186571e-05\n",
      "Steps : 279300, \t Total Gen Loss : 38.46604919433594, \t Total Dis Loss : 9.29745419853134e-06\n",
      "Steps : 279400, \t Total Gen Loss : 36.033843994140625, \t Total Dis Loss : 1.9935441741836257e-05\n",
      "Steps : 279500, \t Total Gen Loss : 38.401214599609375, \t Total Dis Loss : 0.00044482137309387326\n",
      "Steps : 279600, \t Total Gen Loss : 35.59507751464844, \t Total Dis Loss : 0.00011368745617801324\n",
      "Steps : 279700, \t Total Gen Loss : 33.06232452392578, \t Total Dis Loss : 0.0001801012404030189\n",
      "Steps : 279800, \t Total Gen Loss : 39.19129943847656, \t Total Dis Loss : 3.963431663578376e-05\n",
      "Steps : 279900, \t Total Gen Loss : 35.45810317993164, \t Total Dis Loss : 5.084916665509809e-06\n",
      "Steps : 280000, \t Total Gen Loss : 36.70623016357422, \t Total Dis Loss : 0.00033200974576175213\n",
      "Steps : 280100, \t Total Gen Loss : 34.24772262573242, \t Total Dis Loss : 6.247009878279641e-06\n",
      "Steps : 280200, \t Total Gen Loss : 34.59341812133789, \t Total Dis Loss : 5.944005351921078e-06\n",
      "Steps : 280300, \t Total Gen Loss : 35.12633514404297, \t Total Dis Loss : 3.5221705729782116e-06\n",
      "Steps : 280400, \t Total Gen Loss : 36.21366882324219, \t Total Dis Loss : 7.290294888662174e-06\n",
      "Steps : 280500, \t Total Gen Loss : 36.445838928222656, \t Total Dis Loss : 9.62375634117052e-06\n",
      "Steps : 280600, \t Total Gen Loss : 39.63645553588867, \t Total Dis Loss : 7.914619345683604e-06\n",
      "Steps : 280700, \t Total Gen Loss : 34.40130615234375, \t Total Dis Loss : 7.287149492185563e-05\n",
      "Steps : 280800, \t Total Gen Loss : 29.162551879882812, \t Total Dis Loss : 0.0012522494653239846\n",
      "Steps : 280900, \t Total Gen Loss : 37.57973861694336, \t Total Dis Loss : 7.419201574521139e-05\n",
      "Steps : 281000, \t Total Gen Loss : 33.918785095214844, \t Total Dis Loss : 2.8131868020864204e-05\n",
      "Steps : 281100, \t Total Gen Loss : 31.542236328125, \t Total Dis Loss : 0.000195094253285788\n",
      "Steps : 281200, \t Total Gen Loss : 33.339874267578125, \t Total Dis Loss : 0.00016714759112801403\n",
      "Time for epoch 50 is 73.9299430847168 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f12e8474590>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float64)) + ((1 - set_lambda) * tf.cast(lat, tf.float64))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36751053 0.19274305 0.41317991 ... 0.42745759 0.28111064 0.28543558]\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(an_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3475933920211997 0.3165175918105142\n",
      "0.13183036051482339 0.1303499470535049\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_zero = 0\n",
    "zero_one  = 0\n",
    "one_zero  = 0\n",
    "one_one   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_matrix=[[zero_zero,zero_one],[one_one,one_zero]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15000):\n",
    "    if int(bol_test_labels[i]) == 0 and gt_labels[i] == 0:\n",
    "        zero_zero += 1\n",
    "    elif int(bol_test_labels[i]) == 0 and gt_labels[i] == 1:\n",
    "        zero_one += 1\n",
    "    elif int(bol_test_labels[i]) == 1 and gt_labels[i] == 1:\n",
    "        one_one += 1\n",
    "    else: \n",
    "        one_zero += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_matrix=[[zero_zero,zero_one],[one_one,one_zero]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2618, 3382], [5618, 3382]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (detection_matrix[0][0]+detection_matrix[1][0])*100/15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 54.906666666666666\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 및 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-GANomaly모델을 사용하여 개구리 감지 모델을 만들어 보았습니다. 한시간 정도를 학습시켰더니 55% 정도의 정확도 밖에 얻지 못하였지만, GAN모델을 시간을 투자하는 만큼 성능을 얻어 내기에 시간을 더 투자한다면 더 좋은 성능을 얻어낼수 있을 것으로 보여집니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
