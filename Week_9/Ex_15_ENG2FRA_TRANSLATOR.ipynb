{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 Level로 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 실습때, ENG2FRA_TRANSLATOR를 문자 수준의 번역기로 구현해보는 작업을 해보았습니다.  \n",
    "\n",
    "이번에는 단어 수준의 번역기를 만들어보는 작업을 해보도록 하겠습니다.  \n",
    "\n",
    "프랑스어와 영어의 병렬코퍼스는 다음의 사이트에서 받았습니다.  \n",
    "\n",
    "https://www.manythings.org/anki/fra-eng.zip  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 전처리하기(영어, 프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53947</th>\n",
       "      <td>Don't tell Tom anything.</td>\n",
       "      <td>Ne dites rien à Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137092</th>\n",
       "      <td>Tom and Mary returned to their seats.</td>\n",
       "      <td>Tom et Mary retournèrent à leurs places.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116974</th>\n",
       "      <td>I want to know why this happened.</td>\n",
       "      <td>Je veux savoir pourquoi ceci a eu lieu.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76480</th>\n",
       "      <td>I did none of those things.</td>\n",
       "      <td>Je n'ai fait aucune de ces choses.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156314</th>\n",
       "      <td>I want you to know that I'll work very hard.</td>\n",
       "      <td>Je veux que vous sachiez que je travaillerez v...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng  \\\n",
       "53947                       Don't tell Tom anything.   \n",
       "137092         Tom and Mary returned to their seats.   \n",
       "116974             I want to know why this happened.   \n",
       "76480                    I did none of those things.   \n",
       "156314  I want you to know that I'll work very hard.   \n",
       "\n",
       "                                                      fra  \\\n",
       "53947                                Ne dites rien à Tom.   \n",
       "137092           Tom et Mary retournèrent à leurs places.   \n",
       "116974            Je veux savoir pourquoi ceci a eu lieu.   \n",
       "76480                  Je n'ai fait aucune de ces choses.   \n",
       "156314  Je veux que vous sachiez que je travaillerez v...   \n",
       "\n",
       "                                                       cc  \n",
       "53947   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "137092  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "116974  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "76480   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "156314  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "dictionary = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "# \\t에 따라 라벨링하여 분류하기\n",
    "print('전체 샘플의 수 :',len(dictionary))\n",
    "dictionary.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이번 실습에서는 33000개의 샘플중 30000개를 train set으로, 3000개를 test set으로 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24816</th>\n",
       "      <td>I was busy cooking.</td>\n",
       "      <td>J'étais occupée à faire la cuisine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>Is Tom right?</td>\n",
       "      <td>Est-ce que Tom a raison ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17690</th>\n",
       "      <td>You must tell me.</td>\n",
       "      <td>Il vous faut me le dire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>I think so, too.</td>\n",
       "      <td>Je le pense aussi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23738</th>\n",
       "      <td>He sat next to her.</td>\n",
       "      <td>Il s'est assis à son côté.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                                  fra\n",
       "24816  I was busy cooking.  J'étais occupée à faire la cuisine.\n",
       "4087         Is Tom right?            Est-ce que Tom a raison ?\n",
       "17690    You must tell me.             Il vous faut me le dire.\n",
       "11355     I think so, too.                   Je le pense aussi.\n",
       "23738  He sat next to her.           Il s'est assis à son côté."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = dictionary[['eng', 'fra']][:33000] # 33000개 샘플 사용\n",
    "dictionary.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 지시사항을 지켜주기 위해 함수를 만들어 작업을 하였습니다.  \n",
    "\n",
    "1. 구두점(Punctuation)을 단어와 분리해주세요.\n",
    "2. 소문자로 바꿔주세요.  \n",
    "3. 띄어쓰기 단위로 토큰를 수행하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def encoder_preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # 해당 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 경우, 시작점과 끝점을 알려주는 것이 중요함으로 문장 앞 뒤에 단어들을 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # 해당 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'            #문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary.eng = dictionary.eng.apply(lambda x : encoder_preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.fra = dictionary.fra.apply(lambda x : decoder_preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10072          [are, you, excited, ?]\n",
       "23282     [don, t, do, that, here, .]\n",
       "32155    [this, guy, is, a, loser, .]\n",
       "2485               [i, m, engaged, .]\n",
       "27884     [you, were, bad, at, it, .]\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [go, .]\n",
       "1                            [hi, .]\n",
       "2                            [hi, .]\n",
       "3                           [run, !]\n",
       "4                           [run, !]\n",
       "                    ...             \n",
       "32995    [what, was, their, goal, ?]\n",
       "32996    [what, were, you, doing, ?]\n",
       "32997    [what, would, tom, need, ?]\n",
       "32998    [what, would, you, like, ?]\n",
       "32999    [what, would, you, like, ?]\n",
       "Name: eng, Length: 33000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29947    [<start>, j, adore, cette, entreprise, ., <end>]\n",
       "23653      [<start>, c, est, un, homme, simple, ., <end>]\n",
       "18168              [<start>, sais, tu, lire, a, ?, <end>]\n",
       "4538                 [<start>, tom, adorait, a, ., <end>]\n",
       "20486           [<start>, faisons, un, g, teau, ., <end>]\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.fra.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [<start>, va, !, <end>]\n",
       "1                               [<start>, salut, !, <end>]\n",
       "2                               [<start>, salut, ., <end>]\n",
       "3                               [<start>, cours, !, <end>]\n",
       "4                              [<start>, courez, !, <end>]\n",
       "                               ...                        \n",
       "32995           [<start>, quel, tait, leur, but, ?, <end>]\n",
       "32996    [<start>, qu, tais, tu, en, train, de, faire, ...\n",
       "32997    [<start>, de, quoi, tom, aurait, il, besoin, ?...\n",
       "32998                [<start>, qu, aimerais, tu, ?, <end>]\n",
       "32999              [<start>, qu, aimeriez, vous, ?, <end>]\n",
       "Name: fra, Length: 33000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.fra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 1], [1133, 1], [1133, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer()                         # 단어 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(dictionary.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(dictionary.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 92, 12, 2], [1, 1069, 12, 2], [1, 1069, 3, 2]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer()                         # 단어 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(dictionary.fra)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(dictionary.fra)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4663\n",
      "프랑스어 단어장의 크기 : 7327\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(dictionary))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<start>'\n",
    "eos_token = '<end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 인풋 데이터와 타겟 데이터에서 학습을 위해 각각 종료 토큰과 시작 토큰을 제거해주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 92, 12], [1, 1069, 12], [1, 1069, 3]]\n",
      "[[92, 12, 2], [1069, 12, 2], [1069, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30  1  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 17)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 17)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스 임베딩 레이어를 사용하여 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 128\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    596864      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    937856      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 128)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 128)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 131584      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7327)   945183      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,743,071\n",
      "Trainable params: 2,743,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "filename = 'checkpoint-epoch-{}-batch-{}-trial-001.h5'.format(EPOCHS, BATCH_SIZE)\n",
    "checkpoint = ModelCheckpoint(filename,             # file명을 지정합니다\n",
    "                             monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=True,  # 가장 best 값만 저장합니다\n",
    "                             mode='auto'           # auto는 알아서 best를 찾습니다. min/max\n",
    "                            )\n",
    "earlystopping = EarlyStopping(monitor='val_loss',  # 모니터 기준 설정 (val loss) \n",
    "                              patience=20,         # 20회 Epoch동안 개선되지 않는다면 종료\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "# 콜백 정의\n",
    "reduceLR = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # 검증 손실을 기준으로 callback이 호출됩니다\n",
    "    factor=0.5,          # callback 호출시 학습률을 1/2로 줄입니다\n",
    "    patience=10,         # epoch 10 동안 개선되지 않으면 callback이 호출됩니다\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 1.8705\n",
      "Epoch 00001: val_loss improved from inf to 1.78417, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 1.8682 - val_loss: 1.7842 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 1.3510\n",
      "Epoch 00002: val_loss improved from 1.78417 to 1.57393, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 1.3510 - val_loss: 1.5739 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 1.1905\n",
      "Epoch 00003: val_loss improved from 1.57393 to 1.46059, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 1.1903 - val_loss: 1.4606 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 1.0910\n",
      "Epoch 00004: val_loss improved from 1.46059 to 1.38128, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 1.0910 - val_loss: 1.3813 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 1.0150\n",
      "Epoch 00005: val_loss improved from 1.38128 to 1.32498, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 1.0149 - val_loss: 1.3250 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.9540\n",
      "Epoch 00006: val_loss improved from 1.32498 to 1.27951, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.9538 - val_loss: 1.2795 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.9026\n",
      "Epoch 00007: val_loss improved from 1.27951 to 1.25637, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.9025 - val_loss: 1.2564 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.8603\n",
      "Epoch 00008: val_loss improved from 1.25637 to 1.21875, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.8604 - val_loss: 1.2188 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.8242\n",
      "Epoch 00009: val_loss improved from 1.21875 to 1.20198, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.8237 - val_loss: 1.2020 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.7897\n",
      "Epoch 00010: val_loss improved from 1.20198 to 1.17936, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.7897 - val_loss: 1.1794 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.7605\n",
      "Epoch 00011: val_loss improved from 1.17936 to 1.16498, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.7607 - val_loss: 1.1650 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.7354\n",
      "Epoch 00012: val_loss improved from 1.16498 to 1.14869, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.7354 - val_loss: 1.1487 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7134\n",
      "Epoch 00013: val_loss improved from 1.14869 to 1.14283, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.7134 - val_loss: 1.1428 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.6921\n",
      "Epoch 00014: val_loss improved from 1.14283 to 1.12986, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.6919 - val_loss: 1.1299 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.6698\n",
      "Epoch 00015: val_loss improved from 1.12986 to 1.12604, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.6699 - val_loss: 1.1260 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.6493\n",
      "Epoch 00016: val_loss improved from 1.12604 to 1.11401, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.6493 - val_loss: 1.1140 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00017: val_loss improved from 1.11401 to 1.11398, saving model to checkpoint-epoch-100-batch-64-trial-001.h5\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.6315 - val_loss: 1.1140 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.6166\n",
      "Epoch 00018: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.6169 - val_loss: 1.1151 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.6052\n",
      "Epoch 00019: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.6053 - val_loss: 1.1197 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.5946\n",
      "Epoch 00020: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5946 - val_loss: 1.1214 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.5851\n",
      "Epoch 00021: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5850 - val_loss: 1.1215 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.5757\n",
      "Epoch 00022: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5758 - val_loss: 1.1252 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.5668\n",
      "Epoch 00023: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.5667 - val_loss: 1.1218 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.5581\n",
      "Epoch 00024: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5582 - val_loss: 1.1290 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.5505\n",
      "Epoch 00025: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5505 - val_loss: 1.1271 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.5433\n",
      "Epoch 00026: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5433 - val_loss: 1.1332 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.5194\n",
      "Epoch 00027: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5192 - val_loss: 1.1305 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.5127\n",
      "Epoch 00028: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5128 - val_loss: 1.1336 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.5082\n",
      "Epoch 00029: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5082 - val_loss: 1.1409 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.5035\n",
      "Epoch 00030: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.5035 - val_loss: 1.1406 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.4987\n",
      "Epoch 00031: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4987 - val_loss: 1.1408 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.4925\n",
      "Epoch 00032: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.4925 - val_loss: 1.1431 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.4859\n",
      "Epoch 00033: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.4859 - val_loss: 1.1457 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.4798\n",
      "Epoch 00034: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4799 - val_loss: 1.1451 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.4756\n",
      "Epoch 00035: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4755 - val_loss: 1.1460 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.4716\n",
      "Epoch 00036: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4716 - val_loss: 1.1519 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.4595\n",
      "Epoch 00037: val_loss did not improve from 1.11398\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4596 - val_loss: 1.1506 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f49163d3a50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint, earlystopping, reduceLR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 단계에서 모델이 한번에 예측 문장을 발표하는것이 아니기때문에 모델을 재조정해주어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         596864    \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 128), (None, 128) 131584    \n",
      "=================================================================\n",
      "Total params: 728,448\n",
      "Trainable params: 728,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더 출력층 재설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    937856      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      embedding_1[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7327)   945183      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,014,623\n",
      "Trainable params: 2,014,623\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(encoder_preprocess_sentence(decoded_sentence)) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "            break\n",
    "            \n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "            \n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력결과 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: ['call', 'us', '.']\n",
      "정답 문장: ['<start>', 'appelez', 'nous', '!', '<end>']\n",
      "번역기가 번역한 문장:  appelle nous !\n",
      "-----------------------------------\n",
      "입력 문장: ['show', 'me', '.']\n",
      "정답 문장: ['<start>', 'montre', 'moi', '!', '<end>']\n",
      "번역기가 번역한 문장:  montre moi !\n",
      "-----------------------------------\n",
      "입력 문장: ['we', 'know', 'him', '.']\n",
      "정답 문장: ['<start>', 'on', 'le', 'conna', 't', '.', '<end>']\n",
      "번역기가 번역한 문장:  nous le savons .\n",
      "-----------------------------------\n",
      "입력 문장: ['she', 'helps', 'us', '.']\n",
      "정답 문장: ['<start>', 'elle', 'nous', 'aide', '.', '<end>']\n",
      "번역기가 번역한 문장:  elle nous aide .\n",
      "-----------------------------------\n",
      "입력 문장: ['do', 'what', 'i', 'say', '.']\n",
      "정답 문장: ['<start>', 'fais', 'ce', 'que', 'je', 'dis', '.', '<end>']\n",
      "번역기가 번역한 문장:  fais ce que je dis .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [100,200,3000,4321,5050]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', dictionary.eng[seq_index])\n",
    "    print('정답 문장:', dictionary.fra[seq_index]) \n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "깔끔하게 번역이 된 것 같습니다..!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 및 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프랑스어 와 영어의 병렬 코퍼스를 사용하여 ENG2FRA 번역기를 만들어 보았습니다.  \n",
    "모델을 학습시키는 과정에서 메모리의 부족현상이 일어나는 바람에 여러가지 loss의 종류에 대해 다시 복습하는 좋은 기회가 되었던것 같습니다.  \n",
    "마지막 부분에서 번역기가 번역한 'nous le savons'라는 문장은 구글번역기를 통해 다시 영어로 번역해 보았더니 we know it이라는 뜻이 었습니다. 또 여러가지 문장들의 해석 결과를 보니 기초적인 문법의 형태는 잘 갖추었으나 미묘하게 목적어가 이상하게 번역되는(?) 그런 결과가 발생하는 것 같았습니다.  \n",
    "하이퍼 파라미터와 학습 샘플의 수를 잘 조절하여 학습시킨다면 더 좋은 결과를 낼 수 있을것 같습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
