{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멋진 인공지능 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "https://www.kaggle.com/paultimothymooney/poetry/data  \n",
    "사이트에서 Song Lyrics 데이터를 다운로드합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기\n",
    "\n",
    "glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['baby It was all a dream', 'I used to read Word Up magazine', 'Salt n Pepa and Heavy D up in the limousine']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\", encoding='utf-8-sig') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " ['baby It was all a dream', 'I used to read Word Up magazine', 'Salt n Pepa and Heavy D up in the limousine', 'Hangin pictures on my wall', 'Every Saturday Rap Attack Mr Magic Marley Marl', 'I let my tape rock til my tape popped', 'Smokin weed and Bambu sippin on Private Stock', 'Way back when I had the red and black lumberjack', 'With the hat to match', 'Remember Rappin Duke duhha duhha']\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\ufeff문제 참조\n",
    "https://redcarrot.tistory.com/216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/aiffel/aiffel/lyricist/data/lyrics/notorious_big.txt']\n"
     ]
    }
   ],
   "source": [
    "print(txt_list[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 패키지 import 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "# 대망의 텐서플로우!\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess_sentence()를 사용하여 지나치게 긴문장 정제하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # 해당 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'          #문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 이후 토큰의 개수가 15개를 넘어가는 문장들을 제외하고 corpus에 저장하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 168590\n",
      "Examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> baby it was all a dream <end>',\n",
       " '<start> i used to read word up magazine <end>',\n",
       " '<start> salt n pepa and heavy d up in the limousine <end>',\n",
       " '<start> hangin pictures on my wall <end>',\n",
       " '<start> every saturday rap attack mr magic marley marl <end>',\n",
       " '<start> i let my tape rock til my tape popped <end>',\n",
       " '<start> smokin weed and bambu sippin on private stock <end>',\n",
       " '<start> way back when i had the red and black lumberjack <end>',\n",
       " '<start> with the hat to match <end>',\n",
       " '<start> remember rappin duke duhha duhha <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) > 15: continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(\"데이터 크기:\", len(corpus))\n",
    "print(\"Examples:\")\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 잘 정제되었는지 다음을 통해 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> hit me , baby , one more time oh , baby , baby , the reason i breathe is you <end>',\n",
       " '<start> why do tears come at night ? and they say she s so lucky , she s a star <end>',\n",
       " '<start> la , la , la , la la , la , la , la oh baby , baby , have you seen amy tonight ? <end>',\n",
       " '<start> la , la , la , la la , la , la , la so tell me if you see her <end>',\n",
       " '<start> oh baby , baby , baby . . . la , la , la , la la , la , la , la <end>',\n",
       " '<start> la , la , la , la la , la , la , la love me , hate me <end>',\n",
       " '<start> la , la , la , la la , la , la , la yeah love me , hate me <end>',\n",
       " '<start> we re just so pretty ! hey , don t you know that it s always the same ? <end>',\n",
       " '<start> we re just so pretty ! tell me , is it true that these men are from mars ? <end>',\n",
       " '<start> boys , then she s in control can t live with em , can t live without em pink <end>']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_corpus = []\n",
    "\n",
    "for i in range(0,len(corpus)):\n",
    "    if len(corpus[i].split())> 20:\n",
    "        long_corpus.append(corpus[i])\n",
    "\n",
    "long_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필터링을 하고 났는데도 여러가지 쓸모없는 반복이나 특수문자로 인하여 토큰의 갯수가 20이 넘는 코퍼스가 보이는 것같습니다!\n",
    "다시 corpus_new에 토큰의 갯수가 15미만인 문장들만 담아주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_new = []\n",
    "\n",
    "for i in range(0,len(corpus)):\n",
    "    if len(corpus[i].split()) > 15 : continue\n",
    "    \n",
    "    corpus_new.append(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> baby it was all a dream <end>',\n",
       " '<start> i used to read word up magazine <end>',\n",
       " '<start> salt n pepa and heavy d up in the limousine <end>',\n",
       " '<start> hangin pictures on my wall <end>',\n",
       " '<start> every saturday rap attack mr magic marley marl <end>',\n",
       " '<start> i let my tape rock til my tape popped <end>',\n",
       " '<start> smokin weed and bambu sippin on private stock <end>',\n",
       " '<start> way back when i had the red and black lumberjack <end>',\n",
       " '<start> with the hat to match <end>',\n",
       " '<start> remember rappin duke duhha duhha <end>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_new[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156226\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   52   11 ...    0    0    0]\n",
      " [   2    4  285 ...    0    0    0]\n",
      " [   2 2881  481 ...    0    0    0]\n",
      " ...\n",
      " [   2    4   61 ...    0    0    0]\n",
      " [   2  112  632 ...    0    0    0]\n",
      " [   2    8   50 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f6f44699610>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   52,   11, ...,    0,    0,    0],\n",
       "       [   2,    4,  285, ...,    0,    0,    0],\n",
       "       [   2, 2881,  481, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,    4,   61, ...,    0,    0,    0],\n",
       "       [   2,  112,  632, ...,    0,    0,    0],\n",
       "       [   2,    8,   50, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스 문장 및 타켓 문장 만들어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  52  11  53  25   9 360   3   0   0   0   0   0   0]\n",
      "[ 52  11  53  25   9 360   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터와 평가 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "                                         src_input, tgt_input, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124980, 14)\n",
      "Target Train: (124980, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124980\n"
     ]
    }
   ],
   "source": [
    "print(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 750\n",
    "hidden_size = 1500\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 2.4894805e-04, -1.5867874e-04, -1.4352816e-05, ...,\n",
       "         -2.3635796e-04,  4.1457886e-04, -2.2177149e-04],\n",
       "        [ 3.5217236e-04, -5.3204270e-04, -1.1311997e-04, ...,\n",
       "         -3.6291976e-04,  5.1643816e-04, -7.5931312e-05],\n",
       "        ...,\n",
       "        [ 1.4043397e-03, -1.8923432e-03,  3.2512592e-03, ...,\n",
       "         -4.7428111e-04, -1.8803276e-03, -2.2153086e-03],\n",
       "        [ 1.5105837e-03, -2.1046284e-03,  3.9165257e-03, ...,\n",
       "         -6.2279444e-04, -2.4277342e-03, -2.9824059e-03],\n",
       "        [ 1.5902165e-03, -2.3113268e-03,  4.5699361e-03, ...,\n",
       "         -7.6696638e-04, -2.9819233e-03, -3.6766960e-03]],\n",
       "\n",
       "       [[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 7.6638360e-04, -4.2430643e-04,  5.8499508e-04, ...,\n",
       "         -2.8842810e-04,  4.9833424e-04, -6.4378657e-04],\n",
       "        [ 1.2008380e-03, -8.4765709e-04,  4.4645133e-04, ...,\n",
       "         -2.3056722e-04,  7.1312895e-04, -1.0058609e-03],\n",
       "        ...,\n",
       "        [ 2.5311529e-03,  6.8428169e-04, -2.4871929e-03, ...,\n",
       "         -1.4126833e-04,  5.9906007e-03,  1.2880654e-04],\n",
       "        [ 2.8463837e-03,  1.2945170e-03, -2.5091458e-03, ...,\n",
       "         -4.7332465e-04,  6.8766172e-03,  3.4337447e-04],\n",
       "        [ 3.0633383e-03,  1.7927553e-03, -2.5512795e-03, ...,\n",
       "         -9.0155343e-04,  7.6773255e-03,  5.7454256e-04]],\n",
       "\n",
       "       [[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 2.9722799e-04, -5.2171404e-04,  1.7805926e-04, ...,\n",
       "         -3.9948747e-05,  5.5253975e-05,  8.9317909e-05],\n",
       "        [-2.0746565e-04, -2.6051138e-04, -5.4624479e-04, ...,\n",
       "          2.2004388e-04, -2.7725488e-04, -6.6157307e-05],\n",
       "        ...,\n",
       "        [ 2.7188812e-03,  5.0774589e-04, -1.1060247e-03, ...,\n",
       "         -5.6845730e-04,  4.6513174e-03, -2.5905398e-04],\n",
       "        [ 3.0404150e-03,  9.0533501e-04, -1.1783759e-03, ...,\n",
       "         -1.1539843e-03,  5.6338389e-03, -7.5140226e-05],\n",
       "        [ 3.2493472e-03,  1.2468513e-03, -1.3000143e-03, ...,\n",
       "         -1.7299099e-03,  6.5231798e-03,  1.2673983e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 4.1898899e-04, -2.8075886e-04,  4.7625427e-04, ...,\n",
       "         -1.9981900e-04,  3.7834674e-04, -5.1446713e-04],\n",
       "        [ 1.2190400e-03, -4.6218155e-04,  3.9667127e-04, ...,\n",
       "         -7.6153534e-05,  7.8385434e-04, -7.0843741e-04],\n",
       "        ...,\n",
       "        [ 4.0161484e-03,  1.8088015e-03, -4.9768673e-04, ...,\n",
       "         -1.2433078e-03,  7.4075814e-03,  3.4964198e-04],\n",
       "        [ 3.9346889e-03,  2.0491988e-03, -8.4181980e-04, ...,\n",
       "         -1.7365101e-03,  8.1263119e-03,  6.3349959e-04],\n",
       "        [ 3.8168903e-03,  2.2358622e-03, -1.1746564e-03, ...,\n",
       "         -2.1851566e-03,  8.7553514e-03,  8.9028943e-04]],\n",
       "\n",
       "       [[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 4.6392635e-04, -1.4044912e-04,  2.8194700e-04, ...,\n",
       "          6.3872911e-05,  7.1008137e-04, -1.3715552e-05],\n",
       "        [ 4.2460303e-04, -3.9345279e-04,  3.4336615e-04, ...,\n",
       "         -1.1189826e-05,  6.4866646e-04,  1.6431606e-04],\n",
       "        ...,\n",
       "        [ 3.2359001e-03,  2.1291024e-03, -1.6557319e-03, ...,\n",
       "         -2.7371934e-03,  8.2234079e-03,  1.1888496e-03],\n",
       "        [ 3.2501244e-03,  2.3175394e-03, -1.8946161e-03, ...,\n",
       "         -3.0956818e-03,  8.8487770e-03,  1.3697726e-03],\n",
       "        [ 3.2244448e-03,  2.4574478e-03, -2.1103141e-03, ...,\n",
       "         -3.3961358e-03,  9.3832137e-03,  1.5330455e-03]],\n",
       "\n",
       "       [[ 2.1661741e-04, -1.4487586e-04,  2.3380650e-04, ...,\n",
       "         -1.7036783e-04,  2.9531401e-04, -1.3240316e-04],\n",
       "        [ 3.1247194e-04,  1.6639166e-04,  4.8500943e-04, ...,\n",
       "         -1.3839682e-04,  6.0528429e-04, -1.6602656e-04],\n",
       "        [ 1.9972476e-04,  2.9622734e-04,  2.7908344e-04, ...,\n",
       "         -1.9499146e-04,  1.1590929e-04, -3.1455132e-04],\n",
       "        ...,\n",
       "        [ 1.0165467e-03,  9.6311036e-05, -2.1293844e-04, ...,\n",
       "          1.5361822e-04,  2.8739793e-03,  6.6527550e-04],\n",
       "        [ 1.5653552e-03,  5.5661198e-04, -2.8787687e-04, ...,\n",
       "         -2.0749863e-04,  4.0403949e-03,  6.4057775e-04],\n",
       "        [ 2.0551204e-03,  9.9319569e-04, -4.3922543e-04, ...,\n",
       "         -6.5944542e-04,  5.1798718e-03,  6.7592954e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  9000750   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  13506000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  18006000  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  18013501  \n",
      "=================================================================\n",
      "Total params: 58,526,251\n",
      "Trainable params: 58,526,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-4bf22ce965fb>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n",
    "#GPU를 못잡길래 확인해보았습니다..!\n",
    "#여러분들도 학습속도가 너무 느리다싶으면 확인해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "488/488 [==============================] - 124s 255ms/step - loss: 3.3070 - val_loss: 2.9582\n",
      "Epoch 2/10\n",
      "488/488 [==============================] - 124s 254ms/step - loss: 2.8238 - val_loss: 2.7471\n",
      "Epoch 3/10\n",
      "488/488 [==============================] - 124s 255ms/step - loss: 2.5924 - val_loss: 2.6057\n",
      "Epoch 4/10\n",
      "488/488 [==============================] - 127s 260ms/step - loss: 2.3831 - val_loss: 2.4929\n",
      "Epoch 5/10\n",
      "488/488 [==============================] - 128s 262ms/step - loss: 2.1877 - val_loss: 2.4064\n",
      "Epoch 6/10\n",
      "488/488 [==============================] - 129s 264ms/step - loss: 2.0027 - val_loss: 2.3362\n",
      "Epoch 7/10\n",
      "488/488 [==============================] - 128s 262ms/step - loss: 1.8268 - val_loss: 2.2811\n",
      "Epoch 8/10\n",
      "488/488 [==============================] - 125s 256ms/step - loss: 1.6589 - val_loss: 2.2358\n",
      "Epoch 9/10\n",
      "488/488 [==============================] - 125s 255ms/step - loss: 1.5036 - val_loss: 2.2050\n",
      "Epoch 10/10\n",
      "488/488 [==============================] - 125s 255ms/step - loss: 1.3645 - val_loss: 2.1881\n"
     ]
    }
   ],
   "source": [
    "histroy = model.fit(train_dataset, validation_data = test_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제로 가사 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   \n",
    "        # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i wish that i could talk to you <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i wish \", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM을 사용하여 인공지능 작사가 모델을 만들어 보았습니다.\n",
    "모델에서 여러가지 하이퍼파라미터를 바꾸어가며 학습을 시켰고,\n",
    "(embedding_size = 750, hidden_size = 1500)을 적용했을때 좋은 결과를 얻을수 있었습니다.  \n",
    "최종 결과에서 val_loss: 2.1881를 얻었습니다.  \n",
    "\n",
    "또 만들어낸 모델로 문장을 생성하였을때,  \n",
    "'i love' => 'i love you , i love you'  \n",
    "'i wish' => 'i wish that i could talk to you'  \n",
    "같은 매끄러운 문장들을 얻을 수 있었습니다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
